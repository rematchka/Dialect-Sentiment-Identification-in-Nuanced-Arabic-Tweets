{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aa5b66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import AutoModel\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "# Any results you write to the current directory are saved as output.\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "from transformers import BertTokenizer,BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from argparse import ArgumentParser\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from ignite.engine.engine import Engine, State, Events\n",
    "from ignite.handlers import EarlyStopping\n",
    "from ignite.contrib.handlers import TensorboardLogger, ProgressBar\n",
    "from ignite.utils import convert_tensor\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')\n",
    "import gc\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "\n",
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Pytorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Utils\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Sklearn Imports\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW\n",
    "import random\n",
    "import os\n",
    "from urllib import request\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, BertTokenizer\n",
    "from transformers.data.processors import SingleSentenceClassificationProcessor\n",
    "from transformers import Trainer , TrainingArguments\n",
    "from transformers.trainer_utils import EvaluationStrategy\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee83285d",
   "metadata": {},
   "source": [
    "# Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a5a84e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataset class for prompted BERT fine tuning\n",
    "# The dataset takes in query and passage, and then construct a training sample as:  <prompt> + [MASK] + <text>, returning the position of the mask\n",
    "# It also performs padding and stores the labels\n",
    "class BERTPromptDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, labels, tokenizer, prompt=\"ما هو شعور الكاتب؟\", max_length=512):\n",
    "        self.text=dataset['text']\n",
    "        self.labels=labels\n",
    "        # Construct the input sentence\n",
    "#         input_sentences = [\"{} {} {}\".format(prompt, tokenizer.mask_token, text) for _, (text) in dataset.iterrows()]\n",
    "        \n",
    "#         # Encode and store\n",
    "#         encodings_dict = tokenizer.batch_encode_plus(input_sentences, truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "#         self.input_ids = encodings_dict['input_ids']\n",
    "#         self.attn_masks = encodings_dict['attention_mask']\n",
    "#         self.labels = labels\n",
    "\n",
    "#         # Calculate the position of the mask using self.input_ids\n",
    "#         mask_id = tokenizer.encode(tokenizer.mask_token)[1] # 103\n",
    "#         self.mask_pos = [sent_ids.index(mask_id) for sent_ids in self.input_ids]\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return_dict = {\"text\": self.text[idx],                       \n",
    "                       \"labels\": torch.tensor(self.labels[idx]).float()} \n",
    "        return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae89b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Class that expects a prompted input from the BERTPromptDataset\n",
    "# # Takes the input, forward propagates it through BERT and concatenates the output at the [MASK] and [CLS] token to get a representation of the text\n",
    "# # This is then passed to a linear head to perform binary classification for how relevant it is\n",
    "# class BERTPrompt(torch.nn.Module):\n",
    "#     def __init__(self, bert, tokenizer):\n",
    "#         super().__init__()\n",
    "#         self.bert = bert.cuda()\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.linear = torch.nn.Linear(768*2, 3).cuda()\n",
    "#         self.act = torch.nn.Softmax()\n",
    "        \n",
    "#     # input_dict is obtained through indexing the dataset e.g. dataset[0:10]\n",
    "#     def forward(self, input_ids,attention_mask,mask_pos ):\n",
    "#         output = self.bert(input_ids,attention_mask)[0] # output is of shape [10, 256, 768]\n",
    "#         cls_out = output[:, 0, :]\n",
    "#         mask_out = output[torch.arange(cls_out.shape[0]), mask_pos, :] # indexing like [[0, 1], [13, 14]] will select items [[0, 13], [1, 14]]\n",
    "        \n",
    "#         representation = torch.cat([cls_out, mask_out], dim=1)\n",
    "#         logit = self.linear(representation)\n",
    "#         return logit\n",
    "    \n",
    "# #     # return the logit with sigmoid activation applied\n",
    "# #     def predict(self, input_dict):\n",
    "# #         return self.forward(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48d98db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedLayerPooling(nn.Module):\n",
    "    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights=None):\n",
    "        super(WeightedLayerPooling, self).__init__()\n",
    "        self.layer_start = layer_start\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.layer_weights = layer_weights if layer_weights is not None \\\n",
    "            else nn.Parameter(\n",
    "            torch.tensor([1] * (num_hidden_layers + 1 - layer_start), dtype=torch.float)\n",
    "        )\n",
    "\n",
    "    def forward(self, all_hidden_states):\n",
    "        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n",
    "        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n",
    "        weighted_average = (weight_factor * all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n",
    "        return weighted_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a89a8926",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        \n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.kaiming_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim \n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        \n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "            \n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        \n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / (torch.sum(a, 1, keepdim=True) + 1e-10)\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b49ce0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Plan:\n",
    "# take in the length of the prompt\n",
    "# create an embedding layer for that many tokens\n",
    "# create an LSTM and MLP to process the prompt \n",
    "# How to feed the embedding to BERT\n",
    "    # Get the embedding layer using https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#transformers.BertModel.get_input_embeddings \n",
    "    # Then directly pass embedding to BERT using self.bert(input_embeds=, attention_mask=)\n",
    "# add prompting logic to the tokenizer function\n",
    "    # it needs to first pass all tokens through LSTM and MLP\n",
    "    # then add them into the sentence as needed\n",
    "class BERTPrompt(torch.nn.Module):\n",
    "    def __init__(self, bert, tokenizer, max_length=512, prompt_length=5):\n",
    "        super().__init__()\n",
    "        self.hidden_size = 768\n",
    "        self.max_length = max_length\n",
    "        self.prompt_length = prompt_length\n",
    "        self.config = AutoConfig.from_pretrained('aubmindlab/araelectra-base-discriminator')\n",
    "        self.weighted_pooler = WeightedLayerPooling(num_hidden_layers=self.config.num_hidden_layers, layer_start=4)\n",
    "        \n",
    "        self.bert = bert.cuda()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.bert_embedding = self.bert.get_input_embeddings()\n",
    "        self.att=Attention(768, 512).to('cuda')\n",
    "        self.linear = torch.nn.Linear(self.hidden_size, 3).cuda()\n",
    "        # self.act = torch.nn.Sigmoid() \n",
    "        \n",
    "        # p tuning modules\n",
    "        self.prompt_embedding = torch.nn.Embedding(prompt_length, self.hidden_size).cuda()\n",
    "        self.lstm_head = torch.nn.LSTM(input_size=self.hidden_size, hidden_size=int(self.hidden_size/2),\n",
    "            num_layers=2, bidirectional=True, batch_first=True).cuda() # takes (batch_size, sequence length, hidden_size)\n",
    "        self.mlp_head = torch.nn.Sequential(torch.nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                                            torch.nn.ReLU(),\n",
    "                                            torch.nn.Linear(self.hidden_size, self.hidden_size)).cuda()\n",
    "        \n",
    "    # calls the tokenize function to get input embeddings, then passes them through bert\n",
    "    def forward(self, input_sents):\n",
    "        embeds = torch.zeros(len(input_sents), self.max_length, self.hidden_size).cuda()\n",
    "        att_mask = torch.zeros(len(input_sents), self.max_length).cuda().long()\n",
    "        for i, sent in enumerate(input_sents):\n",
    "            embeds[i, :, :], att_mask[i, :] = self.tokenize(sent)\n",
    "        \n",
    "        output = self.bert(inputs_embeds=embeds, attention_mask=att_mask)\n",
    "        all_hidden_states = torch.stack(output.hidden_states)\n",
    "        weighted_pooling_embeddings = self.weighted_pooler(all_hidden_states)\n",
    "        output=self.att(weighted_pooling_embeddings)\n",
    "        # cls_out = output[:, 0, :]\n",
    "        logits = self.linear(output)\n",
    "        return logits\n",
    "    \n",
    "    \n",
    "    # a tokenize function that embeds the sentence adds the prompt to the end and returns token embeddings along with attention mask\n",
    "    def tokenize(self, input_sent,prompt='ما هو شعور الكاتب؟'):\n",
    "        # generate prompt tokens from embedding\n",
    "        self.prompt_length=len(tokenizer.encode(prompt))\n",
    "        prompt_tokens = self.prompt_embedding(torch.arange(self.prompt_length).cuda())\n",
    "        prompt_tokens = torch.unsqueeze(prompt_tokens, 0) # add a batch dimension\n",
    "        prompt_tokens, _ = self.lstm_head(prompt_tokens)\n",
    "        prompt_tokens = self.mlp_head(prompt_tokens)[0]\n",
    "        \n",
    "        # Encode the input_sentence\n",
    "        encoding_dict = self.tokenizer.encode_plus(input_sent, truncation=True, max_length=self.max_length, padding=\"max_length\")        \n",
    "#         input_ids = encoding_dict[\"input_ids\"]\n",
    "#         mask_id = tokenizer.encode(tokenizer.mask_token)[1] # 103\n",
    "        sep_pos = len(input_sent)\n",
    "        \n",
    "        \n",
    "#         sep_pos = encoding_dict[\"input_ids\"].index(102) # the location of the [SEP] token is at the end of the input\n",
    "        token_embeds = self.bert_embedding(torch.tensor(encoding_dict[\"input_ids\"]).cuda())\n",
    "        \n",
    "        # Add the prompt tokens to the end and modify the att_mask to include the prompt\n",
    "        start_prompt_pos = min(sep_pos, self.max_length - self.prompt_length - 1) # if the sentence is truncutated we must further truncate it to fit in the prompt\n",
    "        end_prompt_pos = start_prompt_pos + self.prompt_length + 1\n",
    "        token_embeds[start_prompt_pos:end_prompt_pos, :] = torch.cat([prompt_tokens, token_embeds[sep_pos:sep_pos+1, :]], dim=0)\n",
    "        att_mask = encoding_dict[\"attention_mask\"]\n",
    "        att_mask[start_prompt_pos:end_prompt_pos] = [1]*(self.prompt_length+1)\n",
    "        att_mask = torch.tensor(att_mask).cuda()\n",
    "        \n",
    "        return token_embeds, att_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8e9f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Sequence\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\" Focal Loss, as described in https://arxiv.org/abs/1708.02002.\n",
    "    It is essentially an enhancement to cross entropy loss and is\n",
    "    useful for classification tasks when there is a large class imbalance.\n",
    "    x is expected to contain raw, unnormalized scores for each class.\n",
    "    y is expected to contain class labels.\n",
    "    Shape:\n",
    "        - x: (batch_size, C) or (batch_size, C, d1, d2, ..., dK), K > 0.\n",
    "        - y: (batch_size,) or (batch_size, d1, d2, ..., dK), K > 0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 alpha: Optional[Tensor] = None,\n",
    "                 gamma: float = 0.,\n",
    "                 reduction: str = 'mean',\n",
    "                 ignore_index: int = -100):\n",
    "        \"\"\"Constructor.\n",
    "        Args:\n",
    "            alpha (Tensor, optional): Weights for each class. Defaults to None.\n",
    "            gamma (float, optional): A constant, as described in the paper.\n",
    "                Defaults to 0.\n",
    "            reduction (str, optional): 'mean', 'sum' or 'none'.\n",
    "                Defaults to 'mean'.\n",
    "            ignore_index (int, optional): class label to ignore.\n",
    "                Defaults to -100.\n",
    "        \"\"\"\n",
    "        if reduction not in ('mean', 'sum', 'none'):\n",
    "            raise ValueError(\n",
    "                'Reduction must be one of: \"mean\", \"sum\", \"none\".')\n",
    "\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ignore_index = ignore_index\n",
    "        self.reduction = reduction\n",
    "\n",
    "        self.nll_loss = nn.NLLLoss(\n",
    "            weight=alpha, reduction='none', ignore_index=ignore_index)\n",
    "\n",
    "    def __repr__(self):\n",
    "        arg_keys = ['alpha', 'gamma', 'ignore_index', 'reduction']\n",
    "        arg_vals = [self.__dict__[k] for k in arg_keys]\n",
    "        arg_strs = [f'{k}={v}' for k, v in zip(arg_keys, arg_vals)]\n",
    "        arg_str = ', '.join(arg_strs)\n",
    "        return f'{type(self).__name__}({arg_str})'\n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor) -> Tensor:\n",
    "        if x.ndim > 2:\n",
    "            # (N, C, d1, d2, ..., dK) --> (N * d1 * ... * dK, C)\n",
    "            c = x.shape[1]\n",
    "            x = x.permute(0, *range(2, x.ndim), 1).reshape(-1, c)\n",
    "            # (N, d1, d2, ..., dK) --> (N * d1 * ... * dK,)\n",
    "            y = y.view(-1)\n",
    "\n",
    "        unignored_mask = y != self.ignore_index\n",
    "        y = y[unignored_mask]\n",
    "        if len(y) == 0:\n",
    "            return torch.tensor(0.)\n",
    "        x = x[unignored_mask]\n",
    "\n",
    "        # compute weighted cross entropy term: -alpha * log(pt)\n",
    "        # (alpha is already part of self.nll_loss)\n",
    "        log_p = F.log_softmax(x, dim=-1)\n",
    "        ce = self.nll_loss(log_p, y)\n",
    "\n",
    "        # get true class column from each row\n",
    "        all_rows = torch.arange(len(x))\n",
    "        log_pt = log_p[all_rows, y]\n",
    "\n",
    "        # compute focal term: (1 - pt)^gamma\n",
    "        pt = log_pt.exp()\n",
    "        focal_term = (1 - pt)**self.gamma\n",
    "\n",
    "        # the full loss: -alpha * ((1 - pt)^gamma) * log(pt)\n",
    "        loss = focal_term * ce\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def focal_loss(alpha: Optional[Sequence] = None,\n",
    "               gamma: float = 0.,\n",
    "               reduction: str = 'mean',\n",
    "               ignore_index: int = -100,\n",
    "               device='cuda',\n",
    "               dtype=torch.float32) -> FocalLoss:\n",
    "    \"\"\"Factory function for FocalLoss.\n",
    "    Args:\n",
    "        alpha (Sequence, optional): Weights for each class. Will be converted\n",
    "            to a Tensor if not None. Defaults to None.\n",
    "        gamma (float, optional): A constant, as described in the paper.\n",
    "            Defaults to 0.\n",
    "        reduction (str, optional): 'mean', 'sum' or 'none'.\n",
    "            Defaults to 'mean'.\n",
    "        ignore_index (int, optional): class label to ignore.\n",
    "            Defaults to -100.\n",
    "        device (str, optional): Device to move alpha to. Defaults to 'cpu'.\n",
    "        dtype (torch.dtype, optional): dtype to cast alpha to.\n",
    "            Defaults to torch.float32.\n",
    "    Returns:\n",
    "        A FocalLoss object\n",
    "    \"\"\"\n",
    "    if alpha is not None:\n",
    "        if not isinstance(alpha, Tensor):\n",
    "            alpha = torch.tensor(alpha)\n",
    "        alpha = alpha.to(device=device, dtype=dtype)\n",
    "\n",
    "    fl = FocalLoss(\n",
    "        alpha=alpha,\n",
    "        gamma=gamma,\n",
    "        reduction=reduction,\n",
    "        ignore_index=ignore_index)\n",
    "    return fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd1a573b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1_Loss(nn.Module):\n",
    "    '''Calculate F1 score. Can work with gpu tensors\n",
    "    \n",
    "    The original implmentation is written by Michal Haltuf on Kaggle.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        `ndim` == 1. epsilon <= val <= 1\n",
    "    \n",
    "    Reference\n",
    "    ---------\n",
    "    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\n",
    "    - http://www.ryanzhang.info/python/writing-your-own-loss-function-module-for-pytorch/\n",
    "    '''\n",
    "    def __init__(self, epsilon=1e-7):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def forward(self, y_pred, y_true,):\n",
    "        # assert y_pred.ndim == 2\n",
    "        # assert y_true.ndim == 1\n",
    "        # print(y_pred.shape)\n",
    "        # print(y_true.shape)\n",
    "        # y_pred[y_pred<0.5]=0\n",
    "        # y_pred[y_pred>=0.5]=0\n",
    "\n",
    "\n",
    "        \n",
    "        y_true_one_hot = F.one_hot(y_true.to(torch.int64), 3).to(torch.float32)\n",
    "        # y_pred_one_hot = F.one_hot(y_pred.to(torch.int64), 2).to(torch.float32)\n",
    "        \n",
    "        tp = (y_true_one_hot * y_pred).sum(dim=0).to(torch.float32)\n",
    "        tn = ((1 - y_true_one_hot) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "        fp = ((1 - y_true_one_hot) * y_pred).sum(dim=0).to(torch.float32)\n",
    "        fn = (y_true_one_hot * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "\n",
    "        precision = tp / (tp + fp + self.epsilon)\n",
    "        recall = tp / (tp + fn + self.epsilon)\n",
    "\n",
    "        f1 = 2* (precision*recall) / (precision + recall + self.epsilon)\n",
    "        f1 = f1.clamp(min=self.epsilon, max=1-self.epsilon)\n",
    "        f1=f1.detach()\n",
    "        # print(f1.shape)\n",
    "        # y_pred=y_pred.reshape((y_pred.shape[0], 1))\n",
    "        # y_true=y_true.reshape((y_true.shape[0], 1))\n",
    "\n",
    "        # p1=y_true*(math.log(sigmoid(y_pred)))*(1-f1)[1]\n",
    "        # p0=(1-y_true)*math.log(1-sigmoid(y_pred))*(1-f1)[0]\n",
    "\n",
    "\n",
    "        # y_true_one_hot = F.one_hot(y_true.to(torch.int64), 2)\n",
    "        # print(y_pred)\n",
    "        # print(y_true_one_hot)\n",
    "        CE =torch.nn.CrossEntropyLoss(weight=( 1 - f1))(y_pred, y_true)\n",
    "        # loss = ( 1 - f1)  * CE\n",
    "        return  CE.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d34272c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(outputs1,  targets):\n",
    "\n",
    "    criterion = F1_Loss()\n",
    "    criteria_2=FocalLoss()\n",
    "    loss = 0.5*criterion(outputs1, targets)  + 0.5*criteria_2(outputs1, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "050781e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model, optimizer, scheduler, device, num_epochs, train_loader, valid_loader):\n",
    "    # To automatically log gradients\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
    "    \n",
    "    start = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_epoch_loss = 0\n",
    "    history = defaultdict(list)\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1): \n",
    "        gc.collect()\n",
    "        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, \n",
    "                                           dataloader=train_loader, \n",
    "                                           device=CONFIG['device'], epoch=epoch)\n",
    "        \n",
    "        val_epoch_loss,f1_score = valid_one_epoch(model, valid_loader, device=CONFIG['device'], \n",
    "                                         epoch=epoch)\n",
    "    \n",
    "        history['Train Loss'].append(train_epoch_loss)\n",
    "        history['Valid Loss'].append(val_epoch_loss)\n",
    "        \n",
    "       \n",
    "        \n",
    "        # deep copy the model\n",
    "        if f1_score >= best_epoch_loss:\n",
    "            print(f\"Validation Loss Improved ({best_epoch_loss} ---> {f1_score})\")\n",
    "            best_epoch_loss = f1_score\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            PATH = \"/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/marbert_prompt/Loss_Arch_modified_with sampler_qarib.bin\"\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            # Save a model file from the current directory\n",
    "            print(\"Model Saved\")\n",
    "            \n",
    "        print()\n",
    "    \n",
    "    end = time.time()\n",
    "    time_elapsed = end - start\n",
    "    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
    "    print(\"Best Loss: {:.4f}\".format(best_epoch_loss))\n",
    "    \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14c9ffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_scheduler(optimizer):\n",
    "    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CONFIG['T_max'], \n",
    "                                                   eta_min=CONFIG['min_lr'])\n",
    "    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n",
    "        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=CONFIG['T_0'], \n",
    "                                                             eta_min=CONFIG['min_lr'])\n",
    "    elif CONFIG['scheduler'] == None:\n",
    "        return None\n",
    "        \n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97cd81df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"\n",
    "    Samples elements randomly from a given list of indices for imbalanced dataset\n",
    "    Arguments:\n",
    "        indices (list, optional): a list of indices\n",
    "        num_samples (int, optional): number of samples to draw\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, indices=None, num_samples=None):\n",
    "        # if indices is not provided,\n",
    "        # all elements in the dataset will be considered\n",
    "        self.indices = list(range(len(dataset['label']))) \\\n",
    "            if indices is None else indices\n",
    "\n",
    "        # if num_samples is not provided,\n",
    "        # draw `len(indices)` samples in each iteration\n",
    "        self.num_samples = len(self.indices) \\\n",
    "            if num_samples is None else num_samples\n",
    "\n",
    "        # distribution of classes in the dataset\n",
    "        label_to_count = {}\n",
    "        for idx in self.indices:\n",
    "            label = self._get_label(dataset, idx)\n",
    "            if label in label_to_count:\n",
    "                label_to_count[label] += 1\n",
    "            else:\n",
    "                label_to_count[label] = 1\n",
    "\n",
    "        # weight for each sample\n",
    "        weights = [1.0 / label_to_count[self._get_label(dataset, idx)] for idx in self.indices]\n",
    "        self.weights = torch.DoubleTensor(weights)\n",
    "\n",
    "    def _get_label(self, dataset, id_):\n",
    "        return dataset['label'][id_]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b770511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_loaders(train,valid):\n",
    "   \n",
    "    \n",
    "    train_dataset = BERTPromptDataset(train[['text']], train['label'], tokenizer)\n",
    "    valid_dataset = BERTPromptDataset(valid[['text']], valid['label'], tokenizer)\n",
    "    sampler = ImbalancedDatasetSampler(train)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], \n",
    "                              num_workers=2, sampler=sampler, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], \n",
    "                              num_workers=2, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    return train_loader, valid_loader\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b121b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, dataloader, device, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    prediction=[]\n",
    "    true_prediction=[]\n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, data in bar:        \n",
    "        \n",
    "        text_ids = data['text']\n",
    "#         text_mask = data['text_mask'].to(device, dtype = torch.long)\n",
    "#         mask_pos =data['mask_pos'].to(device, dtype = torch.long)\n",
    "        targets = torch.tensor(data['labels']).to(device, dtype=torch.long)\n",
    "        \n",
    "        batch_size = len(text_ids)\n",
    "\n",
    "        outputs = model(text_ids)\n",
    "        prediction.append(F.softmax(outputs).to('cpu').numpy())\n",
    "        true_prediction.append(targets.to('cpu').numpy())\n",
    "\n",
    "        # outputs = outputs.argmax(dim=1)\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        \n",
    "        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n",
    "                        LR=optimizer.param_groups[0]['lr']) \n",
    "    \n",
    "    gc.collect()\n",
    "    prediction = np.concatenate(prediction)\n",
    "    true_prediction = np.concatenate(true_prediction)\n",
    "    prediction=np.argmax(np.array(prediction),axis=1)\n",
    "    print(print_statistics(np.array(true_prediction),prediction))\n",
    "    print(f1_score(np.array(true_prediction),prediction, average='macro'))\n",
    "\n",
    "    \n",
    "    return epoch_loss, f1_score(np.array(true_prediction),prediction, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf3a5464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, data in bar:\n",
    "        \n",
    "        text_ids = data['text']\n",
    "        targets = torch.tensor(data['labels']).to(device, dtype=torch.long)\n",
    "        \n",
    "        batch_size = len(text_ids)\n",
    "        # print(targets)\n",
    "\n",
    "        outputs = model(text_ids)\n",
    "#         print(outputs.shape)\n",
    "#         print(outputs)\n",
    "        \n",
    "        # print(outputs.shape)\n",
    "\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        loss = loss / CONFIG['n_accumulate']\n",
    "        loss.backward()\n",
    "    \n",
    "        if (step + 1) % CONFIG['n_accumulate'] == 0:\n",
    "            optimizer.step()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "                \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        \n",
    "        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n",
    "                        LR=optimizer.param_groups[0]['lr'])\n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1ac4c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score,f1_score,accuracy_score,recall_score,precision_score,classification_report\n",
    "def print_statistics(y, y_pred):\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision =precision_score(y, y_pred, average='weighted')\n",
    "    recall = recall_score(y, y_pred, average='weighted')\n",
    "    f_score = f1_score(y, y_pred, average='weighted')\n",
    "    print('Accuracy: %.3f\\nPrecision: %.3f\\nRecall: %.3f\\nF_score: %.3f\\n'\n",
    "          % (accuracy, precision, recall, f_score))\n",
    "    print(classification_report(y, y_pred))\n",
    "    return accuracy, precision, recall, f_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe5f6ff",
   "metadata": {},
   "source": [
    "# Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afeda8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Dataset/NADI2022_Subtask2_TRAIN.tsv', sep='\\t', lineterminator='\\n')\n",
    "valid = pd.read_csv('/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Dataset/NADI2022_Subtask2_DEV.tsv', sep='\\t', lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42d2786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\"seed\": 42,\n",
    "          \"epochs\": 15,\n",
    "          \"train_batch_size\": 8,\n",
    "          \"valid_batch_size\": 64,\n",
    "          \"max_length\": 512,\n",
    "          \"learning_rate\": 2e-5,\n",
    "          \"scheduler\": 'CosineAnnealingLR',\n",
    "          \"min_lr\": 1e-8,\n",
    "          \"T_max\": 500,\n",
    "          \"weight_decay\": 1e-8,\n",
    "          \"n_fold\": 5,\n",
    "          \"n_accumulate\": 1,\n",
    "          \"num_classes\": 18,\n",
    "          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d07aa098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "train['#3_label'] = le.fit_transform(train['#3_label'].values)\n",
    "valid['#3_label'] = le.transform(valid['#3_label'].values)\n",
    "train.rename(columns = {'#2_content':'text', '#3_label':'label'}, inplace = True)\n",
    "valid.rename(columns = {'#2_content':'text', '#3_label':'label'}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ae768a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer= AutoTokenizer.from_pretrained('aubmindlab/araelectra-base-discriminator')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c30a30ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at aubmindlab/araelectra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "arabert_model=AutoModel.from_pretrained('aubmindlab/araelectra-base-discriminator', output_hidden_states=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be0ec741",
   "metadata": {},
   "outputs": [],
   "source": [
    "pBERT = BERTPrompt(arabert_model, tokenizer, prompt_length=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e657ed3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU: NVIDIA GeForce RTX 3080 Ti\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 187/187 [00:37<00:00,  5.05it/s, Epoch=1, LR=1.39e-5, Train_Loss=1.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                             | 0/8 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|██████████████████████████████████████████████| 8/8 [00:03<00:00,  2.13it/s, Epoch=1, LR=1.39e-5, Valid_Loss=0.924]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.618\n",
      "Precision: 0.620\n",
      "Recall: 0.618\n",
      "F_score: 0.607\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.69      0.66       190\n",
      "           1       0.63      0.35      0.45       113\n",
      "           2       0.60      0.71      0.65       197\n",
      "\n",
      "    accuracy                           0.62       500\n",
      "   macro avg       0.62      0.58      0.59       500\n",
      "weighted avg       0.62      0.62      0.61       500\n",
      "\n",
      "(0.618, 0.6197266331922714, 0.618, 0.6074281728450698)\n",
      "0.5850655393869689\n",
      "Validation Loss Improved (0 ---> 0.5850655393869689)\n",
      "Model Saved\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                           | 0/187 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|██████████████████████████████████████████| 187/187 [00:36<00:00,  5.09it/s, Epoch=2, LR=2.98e-6, Train_Loss=0.766]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 187/187 [00:36<00:00,  5.10it/s, Epoch=2, LR=2.98e-6, Train_Loss=0.766]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                             | 0/8 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|██████████████████████████████████████████████| 8/8 [00:03<00:00,  2.19it/s, Epoch=2, LR=2.98e-6, Valid_Loss=0.955]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.608\n",
      "Precision: 0.638\n",
      "Recall: 0.608\n",
      "F_score: 0.612\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.69      0.66       190\n",
      "           1       0.44      0.60      0.51       113\n",
      "           2       0.75      0.53      0.62       197\n",
      "\n",
      "    accuracy                           0.61       500\n",
      "   macro avg       0.61      0.61      0.60       500\n",
      "weighted avg       0.64      0.61      0.61       500\n",
      "\n",
      "(0.608, 0.637873590177938, 0.608, 0.6118092421973472)\n",
      "0.5978584670609276\n",
      "Validation Loss Improved (0.5850655393869689 ---> 0.5978584670609276)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/187 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|██████████████████████████████████████████| 187/187 [00:36<00:00,  5.09it/s, Epoch=3, LR=7.35e-7, Train_Loss=0.682]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 187/187 [00:36<00:00,  5.09it/s, Epoch=3, LR=7.35e-7, Train_Loss=0.682]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                             | 0/8 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|██████████████████████████████████████████████| 8/8 [00:03<00:00,  2.33it/s, Epoch=3, LR=7.35e-7, Valid_Loss=0.931]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████████████████████████████████████████| 8/8 [00:03<00:00,  2.21it/s, Epoch=3, LR=7.35e-7, Valid_Loss=0.931]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.606\n",
      "Precision: 0.621\n",
      "Recall: 0.606\n",
      "F_score: 0.610\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.63      0.64       190\n",
      "           1       0.46      0.60      0.52       113\n",
      "           2       0.68      0.59      0.63       197\n",
      "\n",
      "    accuracy                           0.61       500\n",
      "   macro avg       0.60      0.61      0.60       500\n",
      "weighted avg       0.62      0.61      0.61       500\n",
      "\n",
      "(0.606, 0.6209469457826537, 0.606, 0.6099273413607661)\n",
      "0.5976723378352752\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|██████████████████████████████████████████| 187/187 [00:36<00:00,  5.09it/s, Epoch=4, LR=9.88e-6, Train_Loss=0.686]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 187/187 [00:36<00:00,  5.08it/s, Epoch=4, LR=9.88e-6, Train_Loss=0.686]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                             | 0/8 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 8/8 [00:03<00:00,  2.18it/s, Epoch=4, LR=9.88e-6, Valid_Loss=0.985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.620\n",
      "Precision: 0.632\n",
      "Recall: 0.620\n",
      "F_score: 0.623\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.59      0.63       190\n",
      "           1       0.47      0.59      0.53       113\n",
      "           2       0.68      0.66      0.67       197\n",
      "\n",
      "    accuracy                           0.62       500\n",
      "   macro avg       0.61      0.62      0.61       500\n",
      "weighted avg       0.63      0.62      0.62       500\n",
      "\n",
      "(0.62, 0.6320793349454156, 0.62, 0.6233386416835299)\n",
      "0.6095673731591748\n",
      "Validation Loss Improved (0.5978584670609276 ---> 0.6095673731591748)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/187 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|██████████████████████████████████████████| 187/187 [00:36<00:00,  5.10it/s, Epoch=5, LR=1.92e-5, Train_Loss=0.606]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 187/187 [00:36<00:00,  5.07it/s, Epoch=5, LR=1.92e-5, Train_Loss=0.606]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                             | 0/8 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████████| 8/8 [00:03<00:00,  2.19it/s, Epoch=5, LR=1.92e-5, Valid_Loss=1.02]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.582\n",
      "Precision: 0.603\n",
      "Recall: 0.582\n",
      "F_score: 0.588\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.64      0.65       190\n",
      "           1       0.40      0.54      0.46       113\n",
      "           2       0.68      0.55      0.61       197\n",
      "\n",
      "    accuracy                           0.58       500\n",
      "   macro avg       0.58      0.58      0.57       500\n",
      "weighted avg       0.60      0.58      0.59       500\n",
      "\n",
      "(0.582, 0.6032431131019037, 0.582, 0.587722842931722)\n",
      "0.5703073402666448\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/187 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|██████████████████████████████████████████| 187/187 [00:36<00:00,  5.09it/s, Epoch=6, LR=1.72e-5, Train_Loss=0.462]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 187/187 [00:36<00:00,  5.07it/s, Epoch=6, LR=1.72e-5, Train_Loss=0.462]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                             | 0/8 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████████| 8/8 [00:03<00:00,  2.18it/s, Epoch=6, LR=1.72e-5, Valid_Loss=1.41]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.626\n",
      "Precision: 0.620\n",
      "Recall: 0.626\n",
      "F_score: 0.618\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67       190\n",
      "           1       0.53      0.37      0.44       113\n",
      "           2       0.62      0.73      0.67       197\n",
      "\n",
      "    accuracy                           0.63       500\n",
      "   macro avg       0.61      0.59      0.59       500\n",
      "weighted avg       0.62      0.63      0.62       500\n",
      "\n",
      "(0.626, 0.6197622883445668, 0.626, 0.6179964953271028)\n",
      "0.5929394162977538\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/187 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|██████████████████████████████████████████| 187/187 [00:37<00:00,  5.05it/s, Epoch=7, LR=6.38e-6, Train_Loss=0.311]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 187/187 [00:37<00:00,  5.04it/s, Epoch=7, LR=6.38e-6, Train_Loss=0.311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                             | 0/8 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████████████| 8/8 [00:03<00:00,  2.17it/s, Epoch=7, LR=6.38e-6, Valid_Loss=1.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.614\n",
      "Precision: 0.654\n",
      "Recall: 0.614\n",
      "F_score: 0.624\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.62      0.66       190\n",
      "           1       0.41      0.65      0.50       113\n",
      "           2       0.73      0.59      0.65       197\n",
      "\n",
      "    accuracy                           0.61       500\n",
      "   macro avg       0.62      0.62      0.61       500\n",
      "weighted avg       0.65      0.61      0.62       500\n",
      "\n",
      "(0.614, 0.6539764653589267, 0.614, 0.6239236729654472)\n",
      "0.6072840960169992\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/187 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|██████████████████████████████████████████| 187/187 [00:36<00:00,  5.06it/s, Epoch=8, LR=1.32e-8, Train_Loss=0.261]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 187/187 [00:36<00:00,  5.06it/s, Epoch=8, LR=1.32e-8, Train_Loss=0.261]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                             | 0/8 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      " 12%|█████▉                                         | 1/8 [00:00<00:03,  2.14it/s, Epoch=8, LR=1.32e-8, Valid_Loss=1.13]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "    \n",
    "# Create Dataloaders\n",
    "train_loader, valid_loader = prepare_loaders(train,valid)\n",
    "\n",
    "pBERT.to(CONFIG['device'])\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Define Optimizer and Scheduler\n",
    "optimizer = AdamW(pBERT.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
    "scheduler = fetch_scheduler(optimizer)\n",
    "\n",
    "model, history = run_training(pBERT, optimizer, scheduler,\n",
    "                              device=CONFIG['device'],\n",
    "                              num_epochs=CONFIG['epochs'],\n",
    "                              train_loader=train_loader, valid_loader=valid_loader )\n",
    "\n",
    "\n",
    "del model, history, train_loader, valid_loader\n",
    "_ = gc.collect()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79070c48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0be322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9d1dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b34f7f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6207e903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e32b265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dcf104",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4760b23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5004bff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f3ae25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a14011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28d9aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86f6c33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f075632",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536dc8d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b8d176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d361ffa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aecc39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc6a3ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a97de8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4224a8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
