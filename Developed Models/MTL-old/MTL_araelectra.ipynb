{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vMmciFBRyRFe",
    "outputId": "c994053a-36ca-497f-9d9c-221dc821546c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 4.7 MB/s \n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.97\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.21.2-py3-none-any.whl (4.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.7 MB 4.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 9.8 MB/s \n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
      "\u001b[K     |████████████████████████████████| 120 kB 61.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.21.2\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pytorch-ignite\n",
      "  Downloading pytorch_ignite-0.4.9-py3-none-any.whl (259 kB)\n",
      "\u001b[K     |████████████████████████████████| 259 kB 4.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.12.1+cu113)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (21.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pytorch-ignite) (3.0.9)\n",
      "Installing collected packages: pytorch-ignite\n",
      "Successfully installed pytorch-ignite-0.4.9\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n",
      "\u001b[K     |████████████████████████████████| 365 kB 4.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n",
      "\u001b[K     |████████████████████████████████| 115 kB 70.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.7.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[K     |████████████████████████████████| 212 kB 71.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.9.1)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.8.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "\u001b[K     |████████████████████████████████| 127 kB 73.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Installing collected packages: urllib3, xxhash, responses, multiprocess, datasets\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.24.3\n",
      "    Uninstalling urllib3-1.24.3:\n",
      "      Successfully uninstalled urllib3-1.24.3\n",
      "Successfully installed datasets-2.4.0 multiprocess-0.70.13 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install sentencepiece\n",
    "# !pip  install transformers\n",
    "# !pip install pytorch-ignite\n",
    "# !pip install datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AtS4CD_krp3d"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import AutoModel\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "# Any results you write to the current directory are saved as output.\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "from transformers import BertTokenizer,BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from argparse import ArgumentParser\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from ignite.engine.engine import Engine, State, Events\n",
    "from ignite.handlers import EarlyStopping\n",
    "from ignite.contrib.handlers import TensorboardLogger, ProgressBar\n",
    "from ignite.utils import convert_tensor\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')\n",
    "import gc\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "\n",
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Pytorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Utils\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Sklearn Imports\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW\n",
    "import random\n",
    "import os\n",
    "from urllib import request\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, BertTokenizer\n",
    "from transformers.data.processors import SingleSentenceClassificationProcessor\n",
    "from transformers import Trainer , TrainingArguments\n",
    "from transformers.trainer_utils import EvaluationStrategy\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARjNBDtgwfVJ"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lLRNRjusoELI"
   },
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length):\n",
    "        self.df = df\n",
    "        self.max_len = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text = df['text'].values\n",
    "        self.label=df['label'].values\n",
    "        self.task=df['task'].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.text[index]\n",
    "        # summary = self.summary[index]\n",
    "        inputs_text = self.tokenizer.encode_plus(\n",
    "                                text,\n",
    "                                truncation=True,\n",
    "                                add_special_tokens=True,\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length'\n",
    "                            )\n",
    "        \n",
    "                            \n",
    "        target = self.label[index]\n",
    "        \n",
    "        text_ids = inputs_text['input_ids']\n",
    "        text_mask = inputs_text['attention_mask']\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        return {\n",
    "            \n",
    "            'text_ids': torch.tensor(text_ids, dtype=torch.long),\n",
    "            'text_mask': torch.tensor(text_mask, dtype=torch.long),\n",
    "            'task':torch.tensor(self.task[index], dtype=torch.long),\n",
    "            'target': torch.tensor(target, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0ZOanFa2wz3L"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# import Dataset\n",
    "# import os\n",
    "# import numpy as np\n",
    "# from emoji import UNICODE_EMOJI\n",
    "# import TweetNormalizer\n",
    "# import re\n",
    "# import text_normalization\n",
    "\n",
    "\n",
    "# dic = {\n",
    "#       \"egypt\": 'المصرية',\n",
    "# \t  \"nile\": 'المصرية',\n",
    "# \t  \"msa\": \"اللغة العربية الفصحى\",\n",
    "# \t  \"magreb\": \"المغربية\",\n",
    "# \t  \"gulf\": \"الخليجية\",\n",
    "# \t  \"levant\": \"الشامية\"\n",
    "# }\n",
    "\n",
    "# def is_emoji(s):\n",
    "#     return s in UNICODE_EMOJI\n",
    "\n",
    "# # add space near your emoji\n",
    "# def add_space(text):\n",
    "#     return ''.join(' ' + char if is_emoji(char) else char for char in text).strip()\n",
    "\n",
    "# def preprocess(text, lang='ar'):\n",
    "#     if lang == 'ar':\n",
    "#         sent = add_space(text)\n",
    "#         sent = re.sub(r'(?:@[\\w_]+)', \"user\", sent)\n",
    "#         sent = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', \"url\", sent)\n",
    "#         sent = sent.replace('_', ' ')\n",
    "#         sent = sent.replace('#', ' ')\n",
    "#     else:\n",
    "#         sent = add_space(text)\n",
    "#         sent = re.sub(r'(?:@[\\w_]+)', \"@user\", sent)\n",
    "#         sent = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', \"http\", sent)\n",
    "#         sent = sent.replace('_', ' ')\n",
    "#         sent = sent.replace('#', ' ')\n",
    "\n",
    "#     return sent\n",
    "\n",
    "# def prepare_text(df, col='tweet'):\n",
    "#     if col == 'tweet':\n",
    "#         df['dialect'] = df['dialect'].map(dic)   \n",
    "#     for i in range(df.shape[0]):\n",
    "#         df.loc[i, col] = df.loc[i, 'dialect'] + ' [SEP] ' + df.loc[i, col]\n",
    "\n",
    "\n",
    "#     return df\n",
    "\n",
    "# def augment_data(df_train,text_col,label_col):\n",
    "#     df_aug = pd.DataFrame(columns=[text_col, label_col)\n",
    "#     dic_dup = {1: 3,\n",
    "#                0: 1\n",
    "#                }\n",
    "#     for i in range(df_train.shape[0]):\n",
    "#         current = df_train.iloc[i]\n",
    "#         text = current[text_col]\n",
    "#         label_cat = current[label_col]\n",
    "\n",
    "#         aug_ratio = dic_dup[label_cat]\n",
    "#         for k in range(aug_ratio):\n",
    "#             tokens = text.split(' ')\n",
    "#             l = len(tokens)\n",
    "#             n = int(0.1 * l)\n",
    "#             indices = np.random.choice(l, n, replace=False)\n",
    "#             for j in range(len(indices)):\n",
    "#                 tokens[indices[j]] = '[MASK]'\n",
    "#             new_text = ' '.join(tokens)\n",
    "#             entry = {text_col: new_text, label_col: label_cat}\n",
    "#             df_aug = df_aug.append(entry, ignore_index=True)\n",
    "#     df_aug.drop_duplicates(subset=[text_col], keep='first', inplace=True)\n",
    "#     df = pd.concat([df_train,df_aug])\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QcOa9agTwhIg"
   },
   "outputs": [],
   "source": [
    "# '''\n",
    "# Created by: Mohamed Salem Elhady  \n",
    "# Email: mohamed.elaraby@alumni.ubc.ca\n",
    "# Text Normalization: V1 \n",
    "# '''\n",
    "# import sys\n",
    "# import re\n",
    "# import emojis\n",
    "# from emoji import UNICODE_EMOJI\n",
    "# #sys.setdefaultencoding('utf-8')\n",
    "# ##########################Clean Text Data #######################################\n",
    "# ########################Global Variable Declaration##############################\n",
    "# list_seeds = ['سبحان الله', 'الله أكبر', 'اللهم', 'بسم الله', 'يا رب', 'العضيم', 'سبحان', 'يارب', 'قران', 'quran',\n",
    "#               'حديث', 'hadith', 'صلاه_الفجر', '﴾', 'ﷺ', 'صحيح البخاري', 'صحيح مسلم', 'يآرب', 'سورة']\n",
    "# MaxWordPerTweet=7\n",
    "# #################################################################################\n",
    "# def is_emoji(s):\n",
    "#     return s in UNICODE_EMOJI\n",
    "\n",
    "# # add space near your emoji\n",
    "# def add_space(text):\n",
    "#     return ''.join(' ' + char if is_emoji(char) else char for char in text).strip()\n",
    "\n",
    "# def clean(sent):\n",
    "#     \"\"\"clean data from any English char, emoticons, underscore, and repeated > 2\n",
    "#     str -> str\"\"\"\n",
    "#     p1 = re.compile('\\W')\n",
    "#     p2 = re.compile('\\s+')\n",
    "#     sent = re.sub(r\"http\\S+\", \"\", sent)\n",
    "#     sent = ReplaceThreeOrMore(sent)\n",
    "#     sent = remove_unicode_diac(sent)\n",
    "#     sent = sent.replace('_', ' ')\n",
    "#     sent = re.sub(r'[A-Za-z0-9]', r'', sent)\n",
    "#     sent = re.sub(p1, ' ', sent)\n",
    "#     sent = re.sub(p2, ' ', sent)\n",
    "#     return sent\n",
    "\n",
    "# def tokenize_emojis(tweet):\n",
    "#     return list(emojis.get(tweet))\n",
    "\n",
    "# def replace_emoji(sent):\n",
    "#     emoji_pattern = re.compile(\"[\"\n",
    "#                                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "#                                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "#                                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "#                                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "#                                \"]+\", flags=re.UNICODE)\n",
    "#     return emoji_pattern.sub(r'[MASK]', sent)\n",
    "\n",
    "# def preprocess(tweet):\n",
    "#     tweet = add_space(tweet)\n",
    "#     emos = tokenize_emojis(tweet)\n",
    "#     sent = remove_unicode_diac(tweet)\n",
    "#     sent = re.sub(r'(?:@[\\w_]+)', \"user\", sent)\n",
    "#     sent = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', \"url\", sent)\n",
    "#     sent = sent.replace('_', ' ')\n",
    "#     sent = sent.replace('#', ' ')\n",
    "#     if len(emos) > 0:\n",
    "#         sent = sent + ' [SEP] '  + ' '.join(emos)\n",
    "#     #    #sent = sent + ' [SEP] ' + clean_unicode(tweet) + ' [SEP] ' + ' '.join(emos)\n",
    "\n",
    "#     #else:\n",
    "#     #    sent = sent + ' [SEP] ' + clean_unicode(tweet)\n",
    "#     return sent\n",
    "\n",
    "# def preprocess_last(tweet, k=0):\n",
    "#     emos = tokenize_emojis(tweet)\n",
    "#     sent = remove_unicode_diac(tweet)\n",
    "#     sent = re.sub(r'(?:@[\\w_]+)', \"\", sent)\n",
    "#     sent = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', \"\", sent)\n",
    "#     sent = sent.replace('_', ' ')\n",
    "#     sent = sent.replace('#', ' ')\n",
    "#     if k == 0:\n",
    "#         sent = sent\n",
    "#     elif k ==1 :\n",
    "#         sent = sent + ' [SEP] ' + ' '.join(emos)\n",
    "#     elif k==2 :\n",
    "#         sent = sent + ' [SEP] ' + clean_unicode(tweet) + ' [SEP] ' + ' '.join(emos)\n",
    "#     elif k == 3:\n",
    "#         sent = sent + ' [SEP] ' + clean_unicode(tweet)\n",
    "#     else:\n",
    "#         sent = replace_emoji(sent)\n",
    "#         sent = sent + ' [SEP] ' + clean_unicode(tweet) + ' [SEP] ' + ' '.join(emos)\n",
    "\n",
    "#     return sent\n",
    "\n",
    "\n",
    "# def normalize(sent):\n",
    "#     \"\"\"clean data from any English char, emoticons, underscore, and repeated > 2\n",
    "#     str -> str\"\"\"\n",
    "#     sent = re.sub(r'(?:@[\\w_]+)', \"user\", sent)\n",
    "#     sent = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', \"url\", sent)\n",
    "#     #sent = re.sub(r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", \"hashtag\", sent)\n",
    "#     sent = ReplaceThreeOrMore(sent)\n",
    "#     sent = remove_unicode_diac(sent)\n",
    "#     sent = sent.replace('_', ' ')\n",
    "#     return sent\n",
    "\n",
    "# def ReplaceThreeOrMore(s):\n",
    "#     # pattern to look for three or more repetitions of any character, including\n",
    "#     # newlines.\n",
    "#     pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL)\n",
    "#     return pattern.sub(r\"\\1\\1\", s)\n",
    "# def norm_alif(text):\n",
    "#     text = text.replace(u\"\\u0625\", u\"\\u0627\")  # HAMZA below, with LETTER ALEF\n",
    "#     #text = text.replace(u\"\\u0621\", u\"\\u0627\")  # HAMZA, with LETTER ALEF\n",
    "#     text = text.replace(u\"\\u0622\", u\"\\u0627\")  # ALEF WITH MADDA ABOVE, with LETTER ALEF\n",
    "#     text = text.replace(u\"\\u0623\", u\"\\u0627\")  # ALEF WITH HAMZA ABOVE, with LETTER ALEF\n",
    "#     return text\n",
    "# def remove_unicode_diac(text):\n",
    "#     \"\"\"Takes Arabic in utf-8 and returns same text without diac\"\"\"\n",
    "#     # Replace diacritics with nothing\n",
    "#     text = text.replace(u\"\\u064B\", \"\")  # fatHatayn\n",
    "#     text = text.replace(u\"\\u064C\", \"\")  # Dammatayn\n",
    "#     text = text.replace(u\"\\u064D\", \"\")  # kasratayn\n",
    "#     text = text.replace(u\"\\u064E\", \"\")  # fatHa\n",
    "#     text = text.replace(u\"\\u064F\", \"\")  # Damma\n",
    "#     text = text.replace(u\"\\u0650\", \"\")  # kasra\n",
    "#     text = text.replace(u\"\\u0651\", \"\")  # shaddah\n",
    "#     text = text.replace(u\"\\u0652\", \"\")  # sukuun\n",
    "#     text = text.replace(u\"\\u0670\", \"`\")  # dagger 'alif\n",
    "#     return text\n",
    "# def norm_taa(text):\n",
    "#     text=text.replace(u\"\\u0629\", u\"\\u0647\") # taa' marbuuTa, with haa'\n",
    "#     #text=text.replace(u\"\\u064A\", u\"\\u0649\") # yaa' with 'alif maqSuura\n",
    "#     return text\n",
    "# def norm_yaa(text):\n",
    "#     if len(text)!=0:\n",
    "#         if text[-1] == u\"\\u064A\":\n",
    "#             text = text[:-1] + text[-1].replace(u\"\\u064A\", u\"\\u0649\")  # yaa' with 'alif maqSuura\n",
    "#     return text\n",
    "\n",
    "# def NormForWord2Vec(text):\n",
    "#     text=norm_taa(text)\n",
    "#     text=norm_yaa(text)\n",
    "#     text=norm_alif(text)\n",
    "#     return text\n",
    "\n",
    "# def remove_nonunicode2(Tweet):\n",
    "#     ## defining set of unicode ##\n",
    "#     #u\"\"\n",
    "#     #Tweet=Tweet.decode(\"utf-8\")\n",
    "#     UniLex={ ## This is list of all arabic unicode characters in addition to space (to separate words)\n",
    "#             u\"\\u0622\",\n",
    "#             u\"\\u0626\",\n",
    "#             u\"\\u0628\",\n",
    "#             u\"\\u062a\",\n",
    "#             u\"\\u062c\",\n",
    "#             u\"\\u06af\",\n",
    "#             u\"\\u062e\",\n",
    "#             u\"\\u0630\",\n",
    "#             u\"\\u0632\",\n",
    "#             u\"\\u0634\",\n",
    "#             u\"\\u0636\",\n",
    "#             u\"\\u0638\",\n",
    "#             u\"\\u063a\",\n",
    "#             u\"\\u0640\",\n",
    "#             u\"\\u0642\",\n",
    "#             u\"\\u0644\",\n",
    "#             u\"\\u0646\",\n",
    "#             u\"\\u0648\",\n",
    "#             u\"\\u064a\",\n",
    "#             u\"\\u0670\",\n",
    "#             u\"\\u067e\",\n",
    "#             u\"\\u0686\",\n",
    "#             u\"\\u0621\",\n",
    "#             u\"\\u0623\",\n",
    "#             u\"\\u0625\",\n",
    "#             u\"\\u06a4\",\n",
    "#             u\"\\u0627\",\n",
    "#             u\"\\u0629\",\n",
    "#             u\"\\u062b\",\n",
    "#             u\"\\u062d\",\n",
    "#             u\"\\u062f\",\n",
    "#             u\"\\u0631\",\n",
    "#             u\"\\u0633\",\n",
    "#             u\"\\u0635\",\n",
    "#             u\"\\u0637\",\n",
    "#             u\"\\u0639\",\n",
    "#             u\"\\u0641\",\n",
    "#             u\"\\u0643\",\n",
    "#             u\"\\u0645\",\n",
    "#             u\"\\u0647\",\n",
    "#             u\"\\u0649\",\n",
    "#             u\"\\u0671\",\n",
    "#             ' ',\n",
    "#             '\\n'\n",
    "#           }\n",
    "#     fin_tweet=\"\"\n",
    "#     for c in Tweet:\n",
    "#         if c in UniLex:\n",
    "#            fin_tweet=fin_tweet+c\n",
    "#     return fin_tweet\n",
    "\n",
    "# ###### Heuristics Calculations ######\n",
    "# def diac_counter(text):\n",
    "#     #text=text.decode(\"utf-8\")\n",
    "#     diac = [u\"\\u064B\",u\"\\u064C\", u\"\\u064D\", u\"\\u064E\", u\"\\u064F\", u\"\\u0650\", u\"\\u0651\", u\"\\u0652\", u\"\\u0670\"]\n",
    "#     diac_count=0\n",
    "#     for d in diac:\n",
    "#         diac_count+=text.count(d)\n",
    "# #         if d in text:\n",
    "# #             print(d)\n",
    "# #             diac_count+=1\n",
    "#     return diac_count\n",
    "# def check_seed(list_seeds, text):\n",
    "#     \"\"\n",
    "#     for word in list_seeds:\n",
    "#         text = text.lower()\n",
    "#         if word.decode(\"utf-8\") in text:\n",
    "#             return True\n",
    "#     return False\n",
    "# def EnglishCount(text):\n",
    "#     printable = ['e', 'a', 'o', 't', 'i']\n",
    "#     count = 0\n",
    "#     for ch in printable:\n",
    "#         count += text.count(ch.lower())\n",
    "#     return count\n",
    "# ########################################\n",
    "\n",
    "\n",
    "\n",
    "# def eliminate_single_char_words(Tweet):\n",
    "#     parts = Tweet.split(\" \")\n",
    "#     cleaned_line_parts = []\n",
    "#     for P in parts:\n",
    "#         if len(P) != 1:\n",
    "#             cleaned_line_parts.append(P)\n",
    "#     cleaned_line = ' '.join(cleaned_line_parts)\n",
    "#     return cleaned_line\n",
    "# def clean_unicode(Tweet):\n",
    "#     tweet=normalize(Tweet.strip(\"\\n\"))\n",
    "#     if len(tweet) !=0:\n",
    "#         sentence = []\n",
    "#         for word in tweet.split(\" \"):\n",
    "#             word = remove_unicode_diac(word)\n",
    "#             word = norm_alif(word)\n",
    "#             word = norm_taa(word)\n",
    "#             word = norm_yaa(word)\n",
    "#             word = normalize(word)\n",
    "#             sentence.append(word)\n",
    "#         tweet = ' '.join(sentence)\n",
    "#         tweet =remove_nonunicode2(tweet)\n",
    "#         tweet =eliminate_single_char_words(tweet)\n",
    "#     return tweet\n",
    "\n",
    "# def clean_unicode2(Tweet):\n",
    "#     KeepUniOnly(Tweet)\n",
    "#     tweet=normalize(Tweet.strip(\"\\n\"))\n",
    "#     if len(tweet) !=0:\n",
    "#         sentence = []\n",
    "#         for word in tweet.split(\" \"):\n",
    "#             word = remove_unicode_diac(word)\n",
    "#             word = normalize(word)\n",
    "#             sentence.append(word)\n",
    "#         tweet = ' '.join(sentence)\n",
    "#         tweet =remove_nonunicode2(tweet)\n",
    "#         tweet =eliminate_single_char_words(tweet)\n",
    "#     return tweet\n",
    "\n",
    "# def NormCorpusFinal(Tweet):\n",
    "#     tweet=KeepUniOnly(Tweet)\n",
    "#     tweet=NormForWord2Vec(tweet)\n",
    "#     return tweet\n",
    "\n",
    "# def KeepUniOnly(Tweet):## this one is without normalization\n",
    "#     tweet=Tweet.replace(\"# \",\" \")\n",
    "#     tweet=tweet.replace(\"#\",\" \")\n",
    "#     tweet=tweet.replace(\"_\",\" \")\n",
    "#     tweet=tweet.replace(u\"\\u0657\",\" \")\n",
    "#     tweet=tweet.replace(\"\\n\",\" \")\n",
    "#     tweet=remove_nonunicode2(tweet)\n",
    "#     tweet=eliminate_single_char_words(tweet)\n",
    "#     tweet=ReplaceThreeOrMore(tweet)\n",
    "#     return tweet\n",
    "\n",
    "# def get_charset(rawtext):\n",
    "#     chars = sorted(list(set(rawtext)))\n",
    "#     return chars\n",
    "\n",
    "# def DialectChecker(text):\n",
    "#     ##Based on Hueristics done by Hassan\n",
    "#     if (diac_counter(text)>5 or check_seed(list_seeds,text) or EnglishCount(text)>4 or \"<URL>\"  in text\n",
    "#         or text.count('#') >2 or '\"'  in text or text.count('@') or \"\\\"RT\" in text or len(text.split(\" \")) <7):\n",
    "#         return False\n",
    "#     else:\n",
    "#         return True\n",
    "\n",
    "# ###############################################################\n",
    "# '''\n",
    "# Fread=open(\"Egypt_portion.txt\",'r')\n",
    "# Fwriter=open(\"Egypt_portion_norm.txt\",'w')\n",
    "# for line in Fread:\n",
    "#     cleaned_line=clean_unicode_for_w2v(line)\n",
    "#     Fwriter.write(str(cleaned_line))\n",
    "# Fwriter.close()\n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVcFbe1Bvcqh"
   },
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RGw6mbs0uIlN"
   },
   "outputs": [],
   "source": [
    "class F1_Loss(nn.Module):\n",
    "    '''Calculate F1 score. Can work with gpu tensors\n",
    "    \n",
    "    The original implmentation is written by Michal Haltuf on Kaggle.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        `ndim` == 1. epsilon <= val <= 1\n",
    "    \n",
    "    Reference\n",
    "    ---------\n",
    "    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\n",
    "    - http://www.ryanzhang.info/python/writing-your-own-loss-function-module-for-pytorch/\n",
    "    '''\n",
    "    def __init__(self, epsilon=1e-7):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        # assert y_pred.ndim == 2\n",
    "        # assert y_true.ndim == 1\n",
    "        # print(y_pred.shape)\n",
    "        # print(y_true.shape)\n",
    "        # y_pred[y_pred<0.5]=0\n",
    "        # y_pred[y_pred>=0.5]=0\n",
    "\n",
    "\n",
    "        c = y_pred.shape[1]\n",
    "        y_true_one_hot = F.one_hot(y_true.to(torch.int64), c).to(torch.float32)\n",
    "        # y_pred_one_hot = F.one_hot(y_pred.to(torch.int64), 2).to(torch.float32)\n",
    "        \n",
    "        tp = (y_true_one_hot * y_pred).sum(dim=0).to(torch.float32)\n",
    "        tn = ((1 - y_true_one_hot) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "        fp = ((1 - y_true_one_hot) * y_pred).sum(dim=0).to(torch.float32)\n",
    "        fn = (y_true_one_hot * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "\n",
    "        precision = tp / (tp + fp + self.epsilon)\n",
    "        recall = tp / (tp + fn + self.epsilon)\n",
    "\n",
    "        f1 = 2* (precision*recall) / (precision + recall + self.epsilon)\n",
    "        f1 = f1.clamp(min=self.epsilon, max=1-self.epsilon)\n",
    "        f1=f1.detach()\n",
    "        # print(f1.shape)\n",
    "        # y_pred=y_pred.reshape((y_pred.shape[0], 1))\n",
    "        # y_true=y_true.reshape((y_true.shape[0], 1))\n",
    "\n",
    "        # p1=y_true*(math.log(sigmoid(y_pred)))*(1-f1)[1]\n",
    "        # p0=(1-y_true)*math.log(1-sigmoid(y_pred))*(1-f1)[0]\n",
    "\n",
    "\n",
    "        # y_true_one_hot = F.one_hot(y_true.to(torch.int64), 2)\n",
    "        # print(y_pred)\n",
    "        # print(y_true_one_hot)\n",
    "        CE =torch.nn.CrossEntropyLoss(weight=( 1 - f1))(y_pred, y_true)\n",
    "        # loss = ( 1 - f1)  * CE\n",
    "        return  CE.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Pdd02bGivpr5"
   },
   "outputs": [],
   "source": [
    "class Recall_Loss(nn.Module):\n",
    "    '''Calculate Recall score. Can work with gpu tensors\n",
    "    \n",
    "    The original implmentation is written by Michal Haltuf on Kaggle.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        `ndim` == 1. epsilon <= val <= 1\n",
    "    \n",
    "    Reference\n",
    "    ---------\n",
    "    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\n",
    "    - http://www.ryanzhang.info/python/writing-your-own-loss-function-module-for-pytorch/\n",
    "    '''\n",
    "    def __init__(self, epsilon=1e-7):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def forward(self, y_pred, y_true,):\n",
    "        # assert y_pred.ndim == 2\n",
    "        # assert y_true.ndim == 1\n",
    "        # print(y_pred.shape)\n",
    "        # print(y_true.shape)\n",
    "        # y_pred[y_pred<0.5]=0\n",
    "        # y_pred[y_pred>=0.5]=0\n",
    "\n",
    "\n",
    "        \n",
    "        y_true_one_hot = F.one_hot(y_true.to(torch.int64), 18).to(torch.float32)\n",
    "        # y_pred_one_hot = F.one_hot(y_pred.to(torch.int64), 2).to(torch.float32)\n",
    "        \n",
    "        tp = (y_true_one_hot * y_pred).sum(dim=0).to(torch.float32)\n",
    "        tn = ((1 - y_true_one_hot) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "        fp = ((1 - y_true_one_hot) * y_pred).sum(dim=0).to(torch.float32)\n",
    "        fn = (y_true_one_hot * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "\n",
    "        precision = tp / (tp + fp + self.epsilon)\n",
    "        recall = tp / (tp + fn + self.epsilon)\n",
    "\n",
    "        # f1 = 2* (precision*recall) / (precision + recall + self.epsilon)\n",
    "        # f1 = f1.clamp(min=self.epsilon, max=1-self.epsilon)\n",
    "        # f1=f1.detach()\n",
    "        # print(f1.shape)\n",
    "        # y_pred=y_pred.reshape((y_pred.shape[0], 1))\n",
    "        # y_true=y_true.reshape((y_true.shape[0], 1))\n",
    "\n",
    "        # p1=y_true*(math.log(sigmoid(y_pred)))*(1-f1)[1]\n",
    "        # p0=(1-y_true)*math.log(1-sigmoid(y_pred))*(1-f1)[0]\n",
    "\n",
    "\n",
    "        # y_true_one_hot = F.one_hot(y_true.to(torch.int64), 2)\n",
    "        # print(y_pred)\n",
    "        # print(y_true_one_hot)\n",
    "        recall=recall.detach()\n",
    "        CE =torch.nn.CrossEntropyLoss(weight=( 1 - recall))(y_pred, y_true_one_hot)\n",
    "        # loss = ( 1 - f1)  * CE\n",
    "        return  CE.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zKy23cw1wHB9"
   },
   "outputs": [],
   "source": [
    "## Based on https://github.com/AbdelkaderMH/iSarcasmEval/blob/8f28f24ebfb641415a604329ed859506ae687148/focal_loss.py\n",
    "class BinaryFocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a implementation of Focal Loss with smooth label cross entropy supported which is proposed in\n",
    "    'Focal Loss for Dense Object Detection. (https://arxiv.org/abs/1708.02002)'\n",
    "        Focal_Loss= -1*alpha*(1-pt)*log(pt)\n",
    "    :param alpha: (tensor) 3D or 4D the scalar factor for this criterion\n",
    "    :param gamma: (float,double) gamma > 0 reduces the relative loss for well-classified examples (p>0.5) putting more\n",
    "                    focus on hard misclassified example\n",
    "    :param reduction: `none`|`mean`|`sum`\n",
    "    :param **kwargs\n",
    "        balance_index: (int) balance class index, should be specific when alpha is float\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=3, gamma=2, ignore_index=None, reduction='mean', **kwargs):\n",
    "        super(BinaryFocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.smooth = 1e-6  # set '1e-4' when train with FP16\n",
    "        self.ignore_index = ignore_index\n",
    "        self.reduction = reduction\n",
    "\n",
    "        assert self.reduction in ['none', 'mean', 'sum']\n",
    "\n",
    "        # if self.alpha is None:\n",
    "        #     self.alpha = torch.ones(2)\n",
    "        # elif isinstance(self.alpha, (list, np.ndarray)):\n",
    "        #     self.alpha = np.asarray(self.alpha)\n",
    "        #     self.alpha = np.reshape(self.alpha, (2))\n",
    "        #     assert self.alpha.shape[0] == 2, \\\n",
    "        #         'the `alpha` shape is not match the number of class'\n",
    "        # elif isinstance(self.alpha, (float, int)):\n",
    "        #     self.alpha = np.asarray([self.alpha, 1.0 - self.alpha], dtype=np.float).view(2)\n",
    "\n",
    "        # else:\n",
    "        #     raise TypeError('{} not supported'.format(type(self.alpha)))\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        prob = torch.sigmoid(output)\n",
    "        prob = torch.clamp(prob, self.smooth, 1.0 - self.smooth)\n",
    "\n",
    "        valid_mask = None\n",
    "        if self.ignore_index is not None:\n",
    "            valid_mask = (target != self.ignore_index).float()\n",
    "\n",
    "        pos_mask = (target == 1).float()\n",
    "        neg_mask = (target == 0).float()\n",
    "        if valid_mask is not None:\n",
    "            pos_mask = pos_mask * valid_mask\n",
    "            neg_mask = neg_mask * valid_mask\n",
    "\n",
    "        pos_weight = (pos_mask * torch.pow(1 - prob, self.gamma)).detach()\n",
    "        pos_loss = -pos_weight * torch.log(prob)  # / (torch.sum(pos_weight) + 1e-4)\n",
    "\n",
    "        neg_weight = (neg_mask * torch.pow(prob, self.gamma)).detach()\n",
    "        neg_loss = -self.alpha * neg_weight * F.logsigmoid(-output)  # / (torch.sum(neg_weight) + 1e-4)\n",
    "        loss = pos_loss + neg_loss\n",
    "        loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "class FocalLoss_Ori(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a implementation of Focal Loss with smooth label cross entropy supported which is proposed in\n",
    "    'Focal Loss for Dense Object Detection. (https://arxiv.org/abs/1708.02002)'\n",
    "    Focal_Loss= -1*alpha*((1-pt)**gamma)*log(pt)\n",
    "    Args:\n",
    "        num_class: number of classes\n",
    "        alpha: class balance factor\n",
    "        gamma:\n",
    "        ignore_index:\n",
    "        reduction:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_class, alpha=None, gamma=2, ignore_index=None, reduction='mean'):\n",
    "        super(FocalLoss_Ori, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.smooth = 1e-4\n",
    "        self.ignore_index = ignore_index\n",
    "        self.alpha = alpha\n",
    "        if alpha is None:\n",
    "            self.alpha = torch.ones(num_class, )\n",
    "        elif isinstance(alpha, (int, float)):\n",
    "            self.alpha = torch.as_tensor([alpha] * num_class)\n",
    "        elif isinstance(alpha, (list, np.ndarray)):\n",
    "            self.alpha = torch.as_tensor(alpha)\n",
    "        if self.alpha.shape[0] != num_class:\n",
    "            raise RuntimeError('the length not equal to number of class')\n",
    "\n",
    "        # if isinstance(self.alpha, (list, tuple, np.ndarray)):\n",
    "        #     assert len(self.alpha) == self.num_class\n",
    "        #     self.alpha = torch.Tensor(list(self.alpha))\n",
    "        # elif isinstance(self.alpha, (float, int)):\n",
    "        #     assert 0 < self.alpha < 1.0, 'alpha should be in `(0,1)`)'\n",
    "        #     assert balance_index > -1\n",
    "        #     alpha = torch.ones((self.num_class))\n",
    "        #     alpha *= 1 - self.alpha\n",
    "        #     alpha[balance_index] = self.alpha\n",
    "        #     self.alpha = alpha\n",
    "        # elif isinstance(self.alpha, torch.Tensor):\n",
    "        #     self.alpha = self.alpha\n",
    "        # else:\n",
    "        #     raise TypeError('Not support alpha type, expect `int|float|list|tuple|torch.Tensor`')\n",
    "\n",
    "    def forward(self, logit, target):\n",
    "        # assert isinstance(self.alpha,torch.Tensor)\\\n",
    "        N, C = logit.shape[:2]\n",
    "        alpha = self.alpha.to(logit.device)\n",
    "        prob = F.softmax(logit, dim=1)\n",
    "        if prob.dim() > 2:\n",
    "            # N,C,d1,d2 -> N,C,m (m=d1*d2*...)\n",
    "            prob = prob.view(N, C, -1)\n",
    "            prob = prob.transpose(1, 2).contiguous()  # [N,C,d1*d2..] -> [N,d1*d2..,C]\n",
    "            prob = prob.view(-1, prob.size(-1))  # [N,d1*d2..,C]-> [N*d1*d2..,C]\n",
    "        ori_shp = target.shape\n",
    "        target = target.view(-1, 1)  # [N,d1,d2,...]->[N*d1*d2*...,1]\n",
    "        valid_mask = None\n",
    "        if self.ignore_index is not None:\n",
    "            valid_mask = target != self.ignore_index\n",
    "            target = target * valid_mask\n",
    "\n",
    "        # ----------memory saving way--------\n",
    "        prob = prob.gather(1, target).view(-1) + self.smooth  # avoid nan\n",
    "        logpt = torch.log(prob)\n",
    "        # alpha_class = alpha.gather(0, target.view(-1))\n",
    "        alpha_class = alpha[target.squeeze().long()]\n",
    "        class_weight = -alpha_class * torch.pow(torch.sub(1.0, prob), self.gamma)\n",
    "        loss = class_weight * logpt\n",
    "        if valid_mask is not None:\n",
    "            loss = loss * valid_mask.squeeze()\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "            if valid_mask is not None:\n",
    "                loss = loss.sum() / valid_mask.sum()\n",
    "        elif self.reduction == 'none':\n",
    "            loss = loss.view(ori_shp)\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "def weighted_binary_cross_entropy(input, targets, pos_weight, weight=None, size_average=True, reduce=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sigmoid_x: predicted probability of size [N,C], N sample and C Class. Eg. Must be in range of [0,1], i.e. Output from Sigmoid.\n",
    "        targets: true value, one-hot-like vector of size [N,C]\n",
    "        pos_weight: Weight for postive sample\n",
    "    \"\"\"\n",
    "    sigmoid_x = torch.sigmoid(input)\n",
    "    if not (targets.size() == sigmoid_x.size()):\n",
    "        raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(targets.size(), sigmoid_x.size()))\n",
    "\n",
    "    loss = -pos_weight* targets * sigmoid_x.log() - (1-targets)*(1-sigmoid_x).log()\n",
    "\n",
    "    if weight is not None:\n",
    "        loss = loss * weight\n",
    "\n",
    "    if not reduce:\n",
    "        return loss\n",
    "    elif size_average:\n",
    "        return loss.mean()\n",
    "    else:\n",
    "        return loss.sum()\n",
    "\n",
    "class WeightedBCELoss(nn.Module):\n",
    "    def __init__(self, pos_weight= 1, weight=None, PosWeightIsDynamic= True, WeightIsDynamic= False, size_average=True, reduce=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pos_weight = Weight for postive samples. Size [1,C]\n",
    "            weight = Weight for Each class. Size [1,C]\n",
    "            PosWeightIsDynamic: If True, the pos_weight is computed on each batch. If pos_weight is None, then it remains None.\n",
    "            WeightIsDynamic: If True, the weight is computed on each batch. If weight is None, then it remains None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        #self.register_buffer('weight', weight)\n",
    "        #self.register_buffer('pos_weight', pos_weight)\n",
    "        self.size_average = size_average\n",
    "        self.reduce = reduce\n",
    "        self.PosWeightIsDynamic = PosWeightIsDynamic\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # pos_weight = Variable(self.pos_weight) if not isinstance(self.pos_weight, Variable) else self.pos_weight\n",
    "        if self.PosWeightIsDynamic:\n",
    "            positive_counts = target.sum(dim=0)\n",
    "            nBatch = len(target)\n",
    "            self.pos_weight = (nBatch - positive_counts)/(positive_counts +1e-5)\n",
    "\n",
    "\n",
    "        return weighted_binary_cross_entropy(input, target,\n",
    "                                                self.pos_weight,\n",
    "                                                weight=None,\n",
    "                                                size_average=self.size_average,\n",
    "                                                reduce=self.reduce)\n",
    "\n",
    "\n",
    "class WeightedCELoss(nn.Module):\n",
    "    def __init__(self, size_average=True, reduce=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pos_weight = Weight for postive samples. Size [1,C]\n",
    "            weight = Weight for Each class. Size [1,C]\n",
    "            PosWeightIsDynamic: If True, the pos_weight is computed on each batch. If pos_weight is None, then it remains None.\n",
    "            WeightIsDynamic: If True, the weight is computed on each batch. If weight is None, then it remains None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.size_average = size_average\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        positive_counts = target.sum(dim=0)\n",
    "        nBatch = len(target)\n",
    "        pos_weight = (nBatch - positive_counts)/(positive_counts +1e-5)\n",
    "        neg_count = nBatch - positive_counts\n",
    "        neg_weight = (nBatch - neg_count)/(neg_count +1e-5)\n",
    "\n",
    "        weight = torch.tensor([neg_weight, pos_weight], device=target.device)\n",
    "  \n",
    "\n",
    "\n",
    "        return F.cross_entropy(input, target, weight=weight)\n",
    "\n",
    "\n",
    "\n",
    "class FLMultiLoss(nn.Module):\n",
    "    def __init__(self, gamma= 2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pos_weight = Weight for postive samples. Size [1,C]\n",
    "            weight = Weight for Each class. Size [1,C]\n",
    "            PosWeightIsDynamic: If True, the pos_weight is computed on each batch. If pos_weight is None, then it remains None.\n",
    "            WeightIsDynamic: If True, the weight is computed on each batch. If weight is None, then it remains None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.gamma = gamma\n",
    "    def forward(self, input, target):\n",
    "\n",
    "\n",
    "        return focal_binary_cross_entropy(input, target, gamma=2)\n",
    "\n",
    "def EntropyLoss(input_):\n",
    "    mask = input_.ge(0.000001)\n",
    "    mask_out = torch.masked_select(input_, mask)\n",
    "    entropy = -(torch.sum(mask_out * torch.log(mask_out)))\n",
    "    return entropy / float(input_.size(0))\n",
    "\n",
    "def focal_binary_cross_entropy(logits, targets, gamma=2):\n",
    "    num_label = targets.shape[1]\n",
    "    l = logits.reshape(-1)\n",
    "    t = targets.reshape(-1)\n",
    "    p = torch.sigmoid(l)\n",
    "    p = torch.where(t >= 0.5, p, 1-p)\n",
    "    logp = - torch.log(torch.clamp(p, 1e-4, 1-1e-4))\n",
    "    loss = logp*((1-p)**gamma)\n",
    "    loss = num_label*loss.mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dG5x6Ck0nOh_"
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Sequence\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\" Focal Loss, as described in https://arxiv.org/abs/1708.02002.\n",
    "    It is essentially an enhancement to cross entropy loss and is\n",
    "    useful for classification tasks when there is a large class imbalance.\n",
    "    x is expected to contain raw, unnormalized scores for each class.\n",
    "    y is expected to contain class labels.\n",
    "    Shape:\n",
    "        - x: (batch_size, C) or (batch_size, C, d1, d2, ..., dK), K > 0.\n",
    "        - y: (batch_size,) or (batch_size, d1, d2, ..., dK), K > 0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 alpha: Optional[Tensor] = None,\n",
    "                 gamma: float = 0.,\n",
    "                 reduction: str = 'mean',\n",
    "                 ignore_index: int = -100):\n",
    "        \"\"\"Constructor.\n",
    "        Args:\n",
    "            alpha (Tensor, optional): Weights for each class. Defaults to None.\n",
    "            gamma (float, optional): A constant, as described in the paper.\n",
    "                Defaults to 0.\n",
    "            reduction (str, optional): 'mean', 'sum' or 'none'.\n",
    "                Defaults to 'mean'.\n",
    "            ignore_index (int, optional): class label to ignore.\n",
    "                Defaults to -100.\n",
    "        \"\"\"\n",
    "        if reduction not in ('mean', 'sum', 'none'):\n",
    "            raise ValueError(\n",
    "                'Reduction must be one of: \"mean\", \"sum\", \"none\".')\n",
    "\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ignore_index = ignore_index\n",
    "        self.reduction = reduction\n",
    "\n",
    "        self.nll_loss = nn.NLLLoss(\n",
    "            weight=alpha, reduction='none', ignore_index=ignore_index)\n",
    "\n",
    "    def __repr__(self):\n",
    "        arg_keys = ['alpha', 'gamma', 'ignore_index', 'reduction']\n",
    "        arg_vals = [self.__dict__[k] for k in arg_keys]\n",
    "        arg_strs = [f'{k}={v}' for k, v in zip(arg_keys, arg_vals)]\n",
    "        arg_str = ', '.join(arg_strs)\n",
    "        return f'{type(self).__name__}({arg_str})'\n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor) -> Tensor:\n",
    "        if x.ndim > 2:\n",
    "            # (N, C, d1, d2, ..., dK) --> (N * d1 * ... * dK, C)\n",
    "            c = x.shape[1]\n",
    "            x = x.permute(0, *range(2, x.ndim), 1).reshape(-1, c)\n",
    "            # (N, d1, d2, ..., dK) --> (N * d1 * ... * dK,)\n",
    "            y = y.view(-1)\n",
    "\n",
    "        unignored_mask = y != self.ignore_index\n",
    "        y = y[unignored_mask]\n",
    "        if len(y) == 0:\n",
    "            return torch.tensor(0.)\n",
    "        x = x[unignored_mask]\n",
    "\n",
    "        # compute weighted cross entropy term: -alpha * log(pt)\n",
    "        # (alpha is already part of self.nll_loss)\n",
    "        log_p = F.log_softmax(x, dim=-1)\n",
    "        ce = self.nll_loss(log_p, y)\n",
    "\n",
    "        # get true class column from each row\n",
    "        all_rows = torch.arange(len(x))\n",
    "        log_pt = log_p[all_rows, y]\n",
    "\n",
    "        # compute focal term: (1 - pt)^gamma\n",
    "        pt = log_pt.exp()\n",
    "        focal_term = (1 - pt)**self.gamma\n",
    "\n",
    "        # the full loss: -alpha * ((1 - pt)^gamma) * log(pt)\n",
    "        loss = focal_term * ce\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def focal_loss(alpha: Optional[Sequence] = None,\n",
    "               gamma: float = 0.,\n",
    "               reduction: str = 'mean',\n",
    "               ignore_index: int = -100,\n",
    "               device='cuda',\n",
    "               dtype=torch.float32) -> FocalLoss:\n",
    "    \"\"\"Factory function for FocalLoss.\n",
    "    Args:\n",
    "        alpha (Sequence, optional): Weights for each class. Will be converted\n",
    "            to a Tensor if not None. Defaults to None.\n",
    "        gamma (float, optional): A constant, as described in the paper.\n",
    "            Defaults to 0.\n",
    "        reduction (str, optional): 'mean', 'sum' or 'none'.\n",
    "            Defaults to 'mean'.\n",
    "        ignore_index (int, optional): class label to ignore.\n",
    "            Defaults to -100.\n",
    "        device (str, optional): Device to move alpha to. Defaults to 'cpu'.\n",
    "        dtype (torch.dtype, optional): dtype to cast alpha to.\n",
    "            Defaults to torch.float32.\n",
    "    Returns:\n",
    "        A FocalLoss object\n",
    "    \"\"\"\n",
    "    if alpha is not None:\n",
    "        if not isinstance(alpha, Tensor):\n",
    "            alpha = torch.tensor(alpha)\n",
    "        alpha = alpha.to(device=device, dtype=dtype)\n",
    "\n",
    "    fl = FocalLoss(\n",
    "        alpha=alpha,\n",
    "        gamma=gamma,\n",
    "        reduction=reduction,\n",
    "        ignore_index=ignore_index)\n",
    "    return fl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvByMHwKuI4d"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FW53GnvRuJ6J"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import AutoModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def init_weights(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv2d') != -1 or classname.find('ConvTranspose2d') != -1:\n",
    "        nn.init.kaiming_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "class AttentionWithContext(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AttentionWithContext, self).__init__()\n",
    "\n",
    "        self.attn = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.contx = nn.Linear(hidden_dim, 1, bias=False)\n",
    "        #self.apply(init_weights)\n",
    "    def forward(self, inp):\n",
    "        u = torch.tanh_(self.attn(inp))\n",
    "        a = F.softmax(self.contx(u))\n",
    "        s = (a * inp).sum(1)\n",
    "        return s\n",
    "\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 pretrained_path='aubmindlab/bert-base-arabert'):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "\n",
    "        \n",
    "        self.transformer = AutoModel.from_pretrained(pretrained_path, output_hidden_states=True)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None):\n",
    "        outputs = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "            , output_hidden_states=True\n",
    "        )\n",
    "        #(output_last_layer, pooled_cls, (output_layers))\n",
    "        #output[0] (8, seqlen=64, 768) cls [8, 768] ( 12 (8, seqlen=64, 768))\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def output_num(self):\n",
    "        return self.transformer.config.hidden_size\n",
    "\n",
    "class ATTClassifier(nn.Module):\n",
    "    def __init__(self, in_feature, class_num=1, dropout_prob=0.2):\n",
    "        super(ATTClassifier, self).__init__()\n",
    "        self.attention = AttentionWithContext(in_feature)\n",
    "\n",
    "        self.Classifier = nn.Sequential(\n",
    "            nn.Linear(2 * in_feature, 512),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, class_num)\n",
    "        )\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        att = self.attention(x[0]) #(X[0] (bs, seqlenght, embedD) att = \\sum_i alpha_i x[0][i]\n",
    "\n",
    "        xx = torch.cat([att, x[1]], 1)\n",
    "\n",
    "        out = self.Classifier(xx)\n",
    "        return out\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        \n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.kaiming_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim \n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        \n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "            \n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        \n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / (torch.sum(a, 1, keepdim=True) + 1e-10)\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)\n",
    "\n",
    "class WeightedLayerPooling(nn.Module):\n",
    "    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights=None):\n",
    "        super(WeightedLayerPooling, self).__init__()\n",
    "        self.layer_start = layer_start\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.layer_weights = layer_weights if layer_weights is not None \\\n",
    "            else nn.Parameter(\n",
    "            torch.tensor([1] * (num_hidden_layers + 1 - layer_start), dtype=torch.float)\n",
    "        )\n",
    "\n",
    "    def forward(self, all_hidden_states):\n",
    "        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n",
    "        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n",
    "        weighted_average = (weight_factor * all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n",
    "        return weighted_average\n",
    "class NADIModel(nn.Module):\n",
    "  def __init__(self, pretrained_path='aubmindlab/bert-base-arabert',in_feature=768, class_num=3, dropout_prob=0.2):\n",
    "        super(NADIModel, self).__init__()\n",
    "        self.base_model=TransformerLayer(pretrained_path).to('cuda')\n",
    "        self.config = AutoConfig.from_pretrained(pretrained_path)\n",
    "        self.weighted_pooler = WeightedLayerPooling(num_hidden_layers=self.config.num_hidden_layers, layer_start=4)\n",
    "        self.att1=Attention(768, 512).to('cuda')\n",
    "        self.classifier1=nn.Linear(768, 18)\n",
    "        self.att2=Attention(768, 512).to('cuda')\n",
    "        self.classifier2=nn.Linear(768, 3)\n",
    "  def forward(self, ids,mask):\n",
    "    output=self.base_model(ids,mask)\n",
    "    all_hidden_states = torch.stack(output.hidden_states)\n",
    "    weighted_pooling_embeddings = (self.weighted_pooler(all_hidden_states)) # For WeightedLayerPooling\n",
    "    # weighted_pooling_embeddings = weighted_pooling_embeddings[:, 0]\n",
    "    # print(weighted_pooling_embeddings.shape)\n",
    "    output1=self.att1(weighted_pooling_embeddings)\n",
    "    output1=self.classifier1(output1)\n",
    "    \n",
    "    output2=self.att2(weighted_pooling_embeddings)\n",
    "    output2=self.classifier2(output2)\n",
    "    # output=torch.softmax(output, dim=1)\n",
    "    return output1, output2\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYQWLGzVyqt_"
   },
   "source": [
    "# Train, Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pjYrf5IerGh7"
   },
   "outputs": [],
   "source": [
    "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"\n",
    "    Samples elements randomly from a given list of indices for imbalanced dataset\n",
    "    Arguments:\n",
    "        indices (list, optional): a list of indices\n",
    "        num_samples (int, optional): number of samples to draw\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, indices=None, num_samples=None):\n",
    "        # if indices is not provided,\n",
    "        # all elements in the dataset will be considered\n",
    "        self.indices = list(range(len(dataset['#3_label']))) \\\n",
    "            if indices is None else indices\n",
    "\n",
    "        # if num_samples is not provided,\n",
    "        # draw `len(indices)` samples in each iteration\n",
    "        self.num_samples = len(self.indices) \\\n",
    "            if num_samples is None else num_samples\n",
    "\n",
    "        # distribution of classes in the dataset\n",
    "        label_to_count = {}\n",
    "        for idx in self.indices:\n",
    "            label = self._get_label(dataset, idx)\n",
    "            if label in label_to_count:\n",
    "                label_to_count[label] += 1\n",
    "            else:\n",
    "                label_to_count[label] = 1\n",
    "\n",
    "        # weight for each sample\n",
    "        weights = [1.0 / label_to_count[self._get_label(dataset, idx)] for idx in self.indices]\n",
    "        self.weights = torch.DoubleTensor(weights)\n",
    "\n",
    "    def _get_label(self, dataset, id_):\n",
    "        return dataset['#3_label'][id_]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ZWP3677hqMr1"
   },
   "outputs": [],
   "source": [
    "def criterion(outputs1,  targets):\n",
    "\n",
    "    criterion = F1_Loss()\n",
    "    criterion_2 = FocalLoss()\n",
    "    loss = criterion_2(outputs1, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ltIB44KZyxPt"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, data in bar:\n",
    "        \n",
    "        text_ids = data['text_ids'].to(device, dtype = torch.long)\n",
    "        text_mask = data['text_mask'].to(device, dtype = torch.long)\n",
    "        targets = data['target'].to(device, dtype=torch.long)\n",
    "        task = data['task'].to(device, dtype=torch.long)\n",
    "        \n",
    "        batch_size = text_ids.size(0)\n",
    "        # print(targets)\n",
    "\n",
    "        # print(outputs.shape)\n",
    "        \n",
    "        # print(outputs.shape)\n",
    "\n",
    "        \n",
    "        output1,output2 = model(text_ids, text_mask)\n",
    "        output1_task_1=output1[task==1]\n",
    "        output2_task_2=output2[task==2]\n",
    "        # outputs = outputs.argmax(dim=1)\n",
    "        targets_1=targets[task==1]\n",
    "        targets_2=targets[task==2]\n",
    "#         print(output1)\n",
    "#         print(output2)\n",
    "#         print(output1_task_1)\n",
    "#         print(output2_task_2)\n",
    "        \n",
    "        loss = criterion(output1_task_1, targets_1) +   criterion(output2_task_2, targets_2)\n",
    "        \n",
    "        loss = loss / CONFIG['n_accumulate']\n",
    "        loss.backward()\n",
    "    \n",
    "        if (step + 1) % CONFIG['n_accumulate'] == 0:\n",
    "            optimizer.step()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "                \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        \n",
    "        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n",
    "                        LR=optimizer.param_groups[0]['lr'])\n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "U5Ro64pIy4HN"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, dataloader, device, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, data in bar:        \n",
    "        \n",
    "        text_ids = data['text_ids'].to(device, dtype = torch.long)\n",
    "        text_mask = data['text_mask'].to(device, dtype = torch.long)\n",
    "        task = data['task'].to(device, dtype=torch.long)\n",
    "        targets = data['target'].to(device, dtype=torch.long)\n",
    "        \n",
    "        batch_size = text_ids.size(0)\n",
    "\n",
    "        output1,output2 = model(text_ids, text_mask)\n",
    "        output1_task_1=output1[task==1]\n",
    "        output2_task_2=output2[task==2]\n",
    "        # outputs = outputs.argmax(dim=1)\n",
    "        targets_1=targets[task==1]\n",
    "        targets_2=targets[task==2]\n",
    "        \n",
    "        loss = criterion(output1_task_1, targets_1) +   criterion(output2_task_2, targets_2)\n",
    "        \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        \n",
    "        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n",
    "                        LR=optimizer.param_groups[0]['lr'])   \n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "HpH2EB0snbV0"
   },
   "outputs": [],
   "source": [
    "def run_training(model, optimizer, scheduler, device, num_epochs, fold,train_loader, valid_loader):\n",
    "    # To automatically log gradients\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
    "    \n",
    "    start = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_epoch_loss = np.inf\n",
    "    history = defaultdict(list)\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1): \n",
    "        gc.collect()\n",
    "        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, \n",
    "                                           dataloader=train_loader, \n",
    "                                           device=CONFIG['device'], epoch=epoch)\n",
    "        \n",
    "        val_epoch_loss = valid_one_epoch(model, valid_loader, device=CONFIG['device'], \n",
    "                                         epoch=epoch)\n",
    "    \n",
    "        history['Train Loss'].append(train_epoch_loss)\n",
    "        history['Valid Loss'].append(val_epoch_loss)\n",
    "        \n",
    "       \n",
    "        \n",
    "        # deep copy the model\n",
    "        if val_epoch_loss <= best_epoch_loss:\n",
    "            print(f\"Validation Loss Improved ({best_epoch_loss} ---> {val_epoch_loss})\")\n",
    "            best_epoch_loss = val_epoch_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            PATH = f\"/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/MTL/Loss_F1-Fold-{fold}.bin\"\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            # Save a model file from the current directory\n",
    "            print(\"Model Saved\")\n",
    "            \n",
    "        print()\n",
    "    \n",
    "    end = time.time()\n",
    "    time_elapsed = end - start\n",
    "    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
    "    print(\"Best Loss: {:.4f}\".format(best_epoch_loss))\n",
    "    \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "JxT6JHg0nogC"
   },
   "outputs": [],
   "source": [
    "def fetch_scheduler(optimizer):\n",
    "    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CONFIG['T_max'], \n",
    "                                                   eta_min=CONFIG['min_lr'])\n",
    "    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n",
    "        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=CONFIG['T_0'], \n",
    "                                                             eta_min=CONFIG['min_lr'])\n",
    "    elif CONFIG['scheduler'] == None:\n",
    "        return None\n",
    "        \n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "x-x9oUAsnqqy"
   },
   "outputs": [],
   "source": [
    "def prepare_loaders(train,fold):\n",
    "    df_train = train[train.kfold != fold].reset_index(drop=True)\n",
    "    df_valid = train[train.kfold == fold].reset_index(drop=True)\n",
    "    # print(len(df_valid))\n",
    "#     sampler = ImbalancedDatasetSampler(df_train)\n",
    "    \n",
    "    train_dataset = TrainDataset(df_train, tokenizer=tokenizer, max_length=CONFIG['max_length'])\n",
    "    valid_dataset = TrainDataset(df_valid, tokenizer=tokenizer, max_length=CONFIG['max_length'])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], \n",
    "                              num_workers=2, shuffle=True, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], \n",
    "                              num_workers=2, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-MsRHzBnZX8"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ih8htg1LoRZC",
    "outputId": "39af89bf-8ba4-4d1a-a811-dcb4e0f005bc"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "yeq4g6QBasWl"
   },
   "outputs": [],
   "source": [
    "# ,jmnbv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "5dHag8YTn7jE"
   },
   "outputs": [],
   "source": [
    "train_1 = pd.read_csv('/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Dataset/NADI2022_Subtask1_TRAIN.tsv', sep='\\t', lineterminator='\\n')\n",
    "train_2 = pd.read_csv('/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Dataset/NADI2022_Subtask2_TRAIN.tsv', sep='\\t', lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1.rename(columns = {'#2_content':'text', '#3_label':'label'}, inplace = True)\n",
    "train_1['task']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2.rename(columns = {'#2_content':'text', '#3_label':'label'}, inplace = True)\n",
    "train_2['task']=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ZLZqgBrJoyaa",
    "outputId": "9ca687cb-ddc9-43cb-a20c-fc886b98f709"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#1_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_15711</td>\n",
       "      <td>الطرحة في 61 النفر 10 جنيه</td>\n",
       "      <td>yemen</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_2257</td>\n",
       "      <td>كله ولا يطالبوا بارض فدك الله ياخدهم الله يعجل...</td>\n",
       "      <td>lebanon</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_5618</td>\n",
       "      <td>' لما اطلقتو تلك التغريدة وقتها كان في واحد سع...</td>\n",
       "      <td>algeria</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_7284</td>\n",
       "      <td>بجكولى بى خير</td>\n",
       "      <td>iraq</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_15841</td>\n",
       "      <td>يعم القمر براحه علينا</td>\n",
       "      <td>egypt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         #1_id                                               text    label  \\\n",
       "0  TRAIN_15711                         الطرحة في 61 النفر 10 جنيه    yemen   \n",
       "1   TRAIN_2257  كله ولا يطالبوا بارض فدك الله ياخدهم الله يعجل...  lebanon   \n",
       "2   TRAIN_5618  ' لما اطلقتو تلك التغريدة وقتها كان في واحد سع...  algeria   \n",
       "3   TRAIN_7284                                      بجكولى بى خير     iraq   \n",
       "4  TRAIN_15841                              يعم القمر براحه علينا    egypt   \n",
       "\n",
       "   task  \n",
       "0     1  \n",
       "1     1  \n",
       "2     1  \n",
       "3     1  \n",
       "4     1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gB9D4twgo0bI",
    "outputId": "1456047b-ac9a-455a-952a-c0b5adf9bc83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20398"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qYdSpBt2o2HR",
    "outputId": "2e975ea2-539c-4302-fb06-6b20517b9f23"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_1['label'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_1['label'] = le.fit_transform(train_1['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "train_2['label'] = le.fit_transform(train_2['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "alyqp1JApKHQ"
   },
   "outputs": [],
   "source": [
    "df_train=pd.concat([train_1,train_2])\n",
    "# df_train=train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "F8zmQRzrpDra"
   },
   "outputs": [],
   "source": [
    "num_classes=df_train['label'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "0_HBDMdYn1Mi"
   },
   "outputs": [],
   "source": [
    "CONFIG = {\"seed\": 42,\n",
    "          \"epochs\": 5,\n",
    "          \"train_batch_size\": 8,\n",
    "          \"valid_batch_size\": 64,\n",
    "          \"max_length\": 512,\n",
    "          \"learning_rate\": 2e-5,\n",
    "          \"scheduler\": 'CosineAnnealingLR',\n",
    "          \"min_lr\": 1e-8,\n",
    "          \"T_max\": 500,\n",
    "          \"weight_decay\": 1e-8,\n",
    "          \"n_fold\": 5,\n",
    "          \"n_accumulate\": 1,\n",
    "          \"num_classes\": 18,\n",
    "          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "8ecc4684681f4b3099e4fe08204145fa",
      "0c0474b5a7ea46d6807ad413535ce1e0",
      "3433fc1712f448bc9cdf268fd6ede083",
      "7c64b7181c0e4349a618b32d724b84bb",
      "8f8439d8998348578a87975550cc1bf5",
      "d59d26c0b1f24e8bb90a923396cb2165",
      "6023b4788ddf4dcba7225f7304ac9cad",
      "a7ebd3ba058f4ab5a0d90a16ed7e7d4d",
      "3167039da13a488c8b3bf01c3f0e272c",
      "abb5b8d77531458f9fc027d359428fe1",
      "50cc7d788adb41a087929f08717d0467",
      "3602f8b1a37a4786b399413393f18dd9",
      "fc606378c1814928ba931e29b5660d94",
      "02b2b89becfe4c759662eb741058b652",
      "cbefe95319af473dbbfc45610e4af174",
      "e13ca209a08d40a599745c247acbe9f5",
      "49870c1e039b4cf8af88e40869788c08",
      "68aecec4ae5843929c0577a71df4471f",
      "0f328a76c2fd44848101d7130193017c",
      "4b9bc91402d94dc4b37d01eb3f3360ea",
      "ad64a3ffc0654b81ad899d02f2f1a6c9",
      "4e8954aabb184c728eb31e7ac2f49e25",
      "d77acfdb947c44bd9a06e1743606b6d3",
      "f8915edc81c44f62824f56eca89302d0",
      "74f8af9a05f941519e17d334fc596bbd",
      "fb846a62b0e04c08bd29f6b4340dfb28",
      "386b7016fd454b798f21b249ba0fc23c",
      "22175bccca6f46e48e523bed0cd565a6",
      "7cfea8b1145c49f8801162c29df81070",
      "5824947de84b4c3f8644672fefcf54e1",
      "a2bd1ce764784a4f90685a01bd64baec",
      "cac7e5ac9abb4c469acc97476ad1dc4b",
      "8b34605fb31d4c5cb841d179ca98017a",
      "a3497a8c3bea45439534f9106b892c85",
      "2fa119dd048a4a05bb4c5c0bb3beaf2b",
      "98f5c3d77e784340937dfae1170b5f59",
      "0db69d4a110e4cdc869cc2efaa0554d1",
      "219afa17846c4559b2d0cf62ab0aefc6",
      "7a5934dd1def45e8b89e05f661c505eb",
      "589a55a17b8047749c341fff2c2eff26",
      "7207f6ef362144f99e27e6363577f441",
      "6d479fcac1ce4e35851496b962ad3c97",
      "29e70ab330bf4e7588f6d92db96f2657",
      "eea86a4731dd4979b4910e05715333d3"
     ]
    },
    "id": "89VbI4nqq17L",
    "outputId": "44c81029-ded9-419c-b96e-c8765492d8d1"
   },
   "outputs": [],
   "source": [
    "tokenizer= AutoTokenizer.from_pretrained('aubmindlab/araelectra-base-discriminator')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "E9sfjlh6qj6t"
   },
   "outputs": [],
   "source": [
    "df_train.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Dq0tSkUrntN4"
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=CONFIG['n_fold'], shuffle=True, random_state=CONFIG['seed'])\n",
    "\n",
    "for fold, ( _, val_) in enumerate(skf.split(X=df_train, y=df_train['label'])):\n",
    "    df_train.loc[val_ , \"kfold\"] = int(fold)\n",
    "    \n",
    "df_train[\"kfold\"] = df_train[\"kfold\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_0l7FW06mgP",
    "outputId": "3d29f4c2-8e84-4183-915f-99dd0a8d5cf2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 2, 4, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['kfold'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "wOMg6NEVsEkU"
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# df_train['#3_label'] = le.fit_transform(df_train['#3_label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "95b87c436baf47178e72db86a4f4f75d",
      "8de50c3083944cc4a07d17572bfb1f37",
      "cf4282f8f75d4113be5240af50960478",
      "6850ed0ce99d4395ac1f914e5492a0cb",
      "1c4c4c2f8cc048b193d50360d5f332cb",
      "5e35db4fb42e4c9ba375cfc83c39932f",
      "318894e308b34176b169253d5ee154f3",
      "5fbefd38c53e482b973a739d832c7d04",
      "301968824bcf4797bf64ec53a1d7dc6f",
      "73deb6e4a86a47ccbe2ddc7dcb3e024f",
      "9f033512e0d34714b08758df977ac04d",
      "c8c0a899e9ff49d08a732f0dc94db3de",
      "d68e1669d72f44ada0ccb143ab4cb1ae",
      "1144088a6f814f6a90b11d61b8f25d82",
      "4e336f48c3a44c24b494d8fd681c36d4",
      "cff23bdd315b4655b6ed8355456c80f2",
      "7d8de37eaff54ece9765aa7f93708db7",
      "96cf6f39b3a845788f201ca57810cfed",
      "16914a1f36874c81891f43da824a2b81",
      "d93ab43bac6545c4ad82e4a44a9c2d04",
      "4a7ef870656c4eeebbf61d41e3fdc6c7",
      "9ba8376b657e498cb916f20e11d9456b"
     ]
    },
    "id": "RdoS6yH5nwWG",
    "outputId": "739e0434-fc27-4abd-ceb8-078d348a2197"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Fold: 0 ======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at aubmindlab/araelectra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU: NVIDIA GeForce RTX 3080 Ti\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 2189/2189 [06:09<00:00,  5.92it/s, Epoch=1, LR=1.37e-5, Train_Loss=2.79]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/69 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 69/69 [00:25<00:00,  2.68it/s, Epoch=1, LR=1.37e-5, Valid_Loss=2.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (inf ---> 2.1475472465497716)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████| 2189/2189 [06:10<00:00,  5.92it/s, Epoch=2, LR=2.81e-6, Train_Loss=2.44]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/69 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 69/69 [00:25<00:00,  2.70it/s, Epoch=2, LR=2.81e-6, Valid_Loss=2.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (2.1475472465497716 ---> 2.0291340442008625)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████| 2189/2189 [06:09<00:00,  5.92it/s, Epoch=3, LR=8.83e-7, Train_Loss=2.19]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/69 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 69/69 [00:25<00:00,  2.70it/s, Epoch=3, LR=8.83e-7, Valid_Loss=1.98]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (2.0291340442008625 ---> 1.9846393273301322)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████| 2189/2189 [06:09<00:00,  5.92it/s, Epoch=4, LR=1.04e-5, Train_Loss=1.96]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 69/69 [00:25<00:00,  2.70it/s, Epoch=4, LR=1.04e-5, Valid_Loss=1.99]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████| 2189/2189 [06:10<00:00,  5.91it/s, Epoch=5, LR=1.94e-5, Train_Loss=1.78]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/69 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 69/69 [00:25<00:00,  2.69it/s, Epoch=5, LR=1.94e-5, Valid_Loss=2.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete in 0h 33m 1s\n",
      "Best Loss: 1.9846\n",
      "\n",
      "====== Fold: 1 ======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at aubmindlab/araelectra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU: NVIDIA GeForce RTX 3080 Ti\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████| 2189/2189 [06:10<00:00,  5.92it/s, Epoch=1, LR=1.37e-5, Train_Loss=2.77]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/69 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 69/69 [00:25<00:00,  2.68it/s, Epoch=1, LR=1.37e-5, Valid_Loss=2.09]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (inf ---> 2.0944489362577325)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████| 2189/2189 [06:10<00:00,  5.90it/s, Epoch=2, LR=2.81e-6, Train_Loss=2.41]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/69 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 69/69 [00:25<00:00,  2.70it/s, Epoch=2, LR=2.81e-6, Valid_Loss=1.98]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (2.0944489362577325 ---> 1.9836348878738543)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████| 2189/2189 [06:10<00:00,  5.91it/s, Epoch=3, LR=8.83e-7, Train_Loss=2.14]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 69/69 [00:25<00:00,  2.69it/s, Epoch=3, LR=8.83e-7, Valid_Loss=1.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (1.9836348878738543 ---> 1.9463459830850227)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████| 2189/2189 [06:10<00:00,  5.90it/s, Epoch=4, LR=1.04e-5, Train_Loss=1.89]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/69 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 69/69 [00:25<00:00,  2.69it/s, Epoch=4, LR=1.04e-5, Valid_Loss=1.97]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████| 2189/2189 [06:11<00:00,  5.90it/s, Epoch=5, LR=1.94e-5, Train_Loss=1.67]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 69/69 [00:25<00:00,  2.70it/s, Epoch=5, LR=1.94e-5, Valid_Loss=2.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete in 0h 33m 6s\n",
      "Best Loss: 1.9463\n",
      "\n",
      "====== Fold: 2 ======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at aubmindlab/araelectra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU: NVIDIA GeForce RTX 3080 Ti\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████| 2189/2189 [06:10<00:00,  5.90it/s, Epoch=1, LR=1.37e-5, Train_Loss=2.78]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/69 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 69/69 [00:25<00:00,  2.68it/s, Epoch=1, LR=1.37e-5, Valid_Loss=2.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (inf ---> 2.1113844137213547)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████| 2189/2189 [06:10<00:00,  5.91it/s, Epoch=2, LR=2.81e-6, Train_Loss=2.42]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/69 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 69/69 [00:25<00:00,  2.70it/s, Epoch=2, LR=2.81e-6, Valid_Loss=1.99]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (2.1113844137213547 ---> 1.9922276041823435)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                          | 0/2189 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████| 2189/2189 [06:10<00:00,  5.90it/s, Epoch=3, LR=8.83e-7, Train_Loss=2.17]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/69 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 69/69 [00:25<00:00,  2.69it/s, Epoch=3, LR=8.83e-7, Valid_Loss=1.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (1.9922276041823435 ---> 1.941436971485887)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                          | 0/2189 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████| 2189/2189 [06:10<00:00,  5.90it/s, Epoch=4, LR=1.04e-5, Train_Loss=1.94]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/69 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 69/69 [00:25<00:00,  2.70it/s, Epoch=4, LR=1.04e-5, Valid_Loss=1.96]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████| 2189/2189 [06:10<00:00,  5.91it/s, Epoch=5, LR=1.94e-5, Train_Loss=1.75]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 69/69 [00:25<00:00,  2.69it/s, Epoch=5, LR=1.94e-5, Valid_Loss=1.98]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete in 0h 33m 5s\n",
      "Best Loss: 1.9414\n",
      "\n",
      "====== Fold: 3 ======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at aubmindlab/araelectra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU: NVIDIA GeForce RTX 3080 Ti\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████| 2189/2189 [06:10<00:00,  5.90it/s, Epoch=1, LR=1.37e-5, Train_Loss=2.78]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/69 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 69/69 [00:25<00:00,  2.68it/s, Epoch=1, LR=1.37e-5, Valid_Loss=2.09]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (inf ---> 2.0888384876389514)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████| 2189/2189 [06:10<00:00,  5.90it/s, Epoch=2, LR=2.81e-6, Train_Loss=2.42]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/69 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 69/69 [00:25<00:00,  2.69it/s, Epoch=2, LR=2.81e-6, Valid_Loss=1.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (2.0888384876389514 ---> 1.9737655748431338)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████| 2189/2189 [06:10<00:00,  5.91it/s, Epoch=3, LR=8.83e-7, Train_Loss=2.16]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/69 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 69/69 [00:25<00:00,  2.69it/s, Epoch=3, LR=8.83e-7, Valid_Loss=1.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (1.9737655748431338 ---> 1.9360909280942493)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████| 2189/2189 [06:10<00:00,  5.91it/s, Epoch=4, LR=1.04e-5, Train_Loss=1.94]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/69 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 69/69 [00:25<00:00,  2.70it/s, Epoch=4, LR=1.04e-5, Valid_Loss=1.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (1.9360909280942493 ---> 1.9255270653594219)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████| 2189/2189 [06:10<00:00,  5.91it/s, Epoch=5, LR=1.94e-5, Train_Loss=1.74]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/69 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 69/69 [00:25<00:00,  2.70it/s, Epoch=5, LR=1.94e-5, Valid_Loss=1.96]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete in 0h 33m 5s\n",
      "Best Loss: 1.9255\n",
      "\n",
      "====== Fold: 4 ======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at aubmindlab/araelectra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU: NVIDIA GeForce RTX 3080 Ti\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████| 2189/2189 [06:10<00:00,  5.91it/s, Epoch=1, LR=1.37e-5, Train_Loss=2.78]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/69 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 69/69 [00:25<00:00,  2.68it/s, Epoch=1, LR=1.37e-5, Valid_Loss=2.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (inf ---> 2.0748505784812434)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                          | 0/2189 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████| 2189/2189 [06:10<00:00,  5.92it/s, Epoch=2, LR=2.81e-6, Train_Loss=2.42]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/69 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 69/69 [00:25<00:00,  2.70it/s, Epoch=2, LR=2.81e-6, Valid_Loss=1.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (2.0748505784812434 ---> 1.968178841615273)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████| 2189/2189 [06:10<00:00,  5.91it/s, Epoch=3, LR=8.83e-7, Train_Loss=2.15]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/69 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 69/69 [00:25<00:00,  2.70it/s, Epoch=3, LR=8.83e-7, Valid_Loss=1.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (1.968178841615273 ---> 1.9346260390724868)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████| 2189/2189 [06:10<00:00,  5.91it/s, Epoch=4, LR=1.04e-5, Train_Loss=1.92]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/69 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 69/69 [00:25<00:00,  2.70it/s, Epoch=4, LR=1.04e-5, Valid_Loss=1.96]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|██████████████████████████████████████████| 2189/2189 [06:10<00:00,  5.92it/s, Epoch=5, LR=1.94e-5, Train_Loss=1.7]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████████████| 69/69 [00:25<00:00,  2.70it/s, Epoch=5, LR=1.94e-5, Valid_Loss=2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete in 0h 33m 3s\n",
      "Best Loss: 1.9346\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for fold in range(0, CONFIG['n_fold']):\n",
    "    print(f\"====== Fold: {fold} ======\")\n",
    "\n",
    "    \n",
    "    # Create Dataloaders\n",
    "    train_loader, valid_loader = prepare_loaders(df_train,fold=fold)\n",
    "    \n",
    "    model = NADIModel('aubmindlab/araelectra-base-discriminator')\n",
    "    model.to(CONFIG['device'])\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Define Optimizer and Scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
    "    scheduler = fetch_scheduler(optimizer)\n",
    "    \n",
    "    model, history = run_training(model, optimizer, scheduler,\n",
    "                                  device=CONFIG['device'],\n",
    "                                  num_epochs=CONFIG['epochs'],\n",
    "                                  fold=fold,train_loader=train_loader, valid_loader=valid_loader )\n",
    "    \n",
    "    \n",
    "    del model, history, train_loader, valid_loader\n",
    "    _ = gc.collect()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "bert-base-arabertv02-twitter_multiple losses_w_imablancesampler.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02b2b89becfe4c759662eb741058b652": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f328a76c2fd44848101d7130193017c",
      "max": 750551,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4b9bc91402d94dc4b37d01eb3f3360ea",
      "value": 750551
     }
    },
    "0c0474b5a7ea46d6807ad413535ce1e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d59d26c0b1f24e8bb90a923396cb2165",
      "placeholder": "​",
      "style": "IPY_MODEL_6023b4788ddf4dcba7225f7304ac9cad",
      "value": "Downloading tokenizer_config.json: 100%"
     }
    },
    "0db69d4a110e4cdc869cc2efaa0554d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_29e70ab330bf4e7588f6d92db96f2657",
      "placeholder": "​",
      "style": "IPY_MODEL_eea86a4731dd4979b4910e05715333d3",
      "value": " 112/112 [00:00&lt;00:00, 3.38kB/s]"
     }
    },
    "0f328a76c2fd44848101d7130193017c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1144088a6f814f6a90b11d61b8f25d82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_16914a1f36874c81891f43da824a2b81",
      "max": 541120363,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d93ab43bac6545c4ad82e4a44a9c2d04",
      "value": 541120363
     }
    },
    "16914a1f36874c81891f43da824a2b81": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c4c4c2f8cc048b193d50360d5f332cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "219afa17846c4559b2d0cf62ab0aefc6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "22175bccca6f46e48e523bed0cd565a6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29e70ab330bf4e7588f6d92db96f2657": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2fa119dd048a4a05bb4c5c0bb3beaf2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a5934dd1def45e8b89e05f661c505eb",
      "placeholder": "​",
      "style": "IPY_MODEL_589a55a17b8047749c341fff2c2eff26",
      "value": "Downloading special_tokens_map.json: 100%"
     }
    },
    "301968824bcf4797bf64ec53a1d7dc6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3167039da13a488c8b3bf01c3f0e272c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "318894e308b34176b169253d5ee154f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3433fc1712f448bc9cdf268fd6ede083": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a7ebd3ba058f4ab5a0d90a16ed7e7d4d",
      "max": 476,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3167039da13a488c8b3bf01c3f0e272c",
      "value": 476
     }
    },
    "3602f8b1a37a4786b399413393f18dd9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fc606378c1814928ba931e29b5660d94",
       "IPY_MODEL_02b2b89becfe4c759662eb741058b652",
       "IPY_MODEL_cbefe95319af473dbbfc45610e4af174"
      ],
      "layout": "IPY_MODEL_e13ca209a08d40a599745c247acbe9f5"
     }
    },
    "386b7016fd454b798f21b249ba0fc23c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "49870c1e039b4cf8af88e40869788c08": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a7ef870656c4eeebbf61d41e3fdc6c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b9bc91402d94dc4b37d01eb3f3360ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4e336f48c3a44c24b494d8fd681c36d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a7ef870656c4eeebbf61d41e3fdc6c7",
      "placeholder": "​",
      "style": "IPY_MODEL_9ba8376b657e498cb916f20e11d9456b",
      "value": " 516M/516M [00:09&lt;00:00, 57.1MB/s]"
     }
    },
    "4e8954aabb184c728eb31e7ac2f49e25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "50cc7d788adb41a087929f08717d0467": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5824947de84b4c3f8644672fefcf54e1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "589a55a17b8047749c341fff2c2eff26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5e35db4fb42e4c9ba375cfc83c39932f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5fbefd38c53e482b973a739d832c7d04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6023b4788ddf4dcba7225f7304ac9cad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6850ed0ce99d4395ac1f914e5492a0cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_73deb6e4a86a47ccbe2ddc7dcb3e024f",
      "placeholder": "​",
      "style": "IPY_MODEL_9f033512e0d34714b08758df977ac04d",
      "value": " 667/667 [00:00&lt;00:00, 22.4kB/s]"
     }
    },
    "68aecec4ae5843929c0577a71df4471f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6d479fcac1ce4e35851496b962ad3c97": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7207f6ef362144f99e27e6363577f441": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73deb6e4a86a47ccbe2ddc7dcb3e024f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74f8af9a05f941519e17d334fc596bbd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5824947de84b4c3f8644672fefcf54e1",
      "max": 1252935,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a2bd1ce764784a4f90685a01bd64baec",
      "value": 1252935
     }
    },
    "7a5934dd1def45e8b89e05f661c505eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7c64b7181c0e4349a618b32d724b84bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_abb5b8d77531458f9fc027d359428fe1",
      "placeholder": "​",
      "style": "IPY_MODEL_50cc7d788adb41a087929f08717d0467",
      "value": " 476/476 [00:00&lt;00:00, 9.96kB/s]"
     }
    },
    "7cfea8b1145c49f8801162c29df81070": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7d8de37eaff54ece9765aa7f93708db7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b34605fb31d4c5cb841d179ca98017a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8de50c3083944cc4a07d17572bfb1f37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5e35db4fb42e4c9ba375cfc83c39932f",
      "placeholder": "​",
      "style": "IPY_MODEL_318894e308b34176b169253d5ee154f3",
      "value": "Downloading config.json: 100%"
     }
    },
    "8ecc4684681f4b3099e4fe08204145fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0c0474b5a7ea46d6807ad413535ce1e0",
       "IPY_MODEL_3433fc1712f448bc9cdf268fd6ede083",
       "IPY_MODEL_7c64b7181c0e4349a618b32d724b84bb"
      ],
      "layout": "IPY_MODEL_8f8439d8998348578a87975550cc1bf5"
     }
    },
    "8f8439d8998348578a87975550cc1bf5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95b87c436baf47178e72db86a4f4f75d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8de50c3083944cc4a07d17572bfb1f37",
       "IPY_MODEL_cf4282f8f75d4113be5240af50960478",
       "IPY_MODEL_6850ed0ce99d4395ac1f914e5492a0cb"
      ],
      "layout": "IPY_MODEL_1c4c4c2f8cc048b193d50360d5f332cb"
     }
    },
    "96cf6f39b3a845788f201ca57810cfed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "98f5c3d77e784340937dfae1170b5f59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7207f6ef362144f99e27e6363577f441",
      "max": 112,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6d479fcac1ce4e35851496b962ad3c97",
      "value": 112
     }
    },
    "9ba8376b657e498cb916f20e11d9456b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9f033512e0d34714b08758df977ac04d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a2bd1ce764784a4f90685a01bd64baec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a3497a8c3bea45439534f9106b892c85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2fa119dd048a4a05bb4c5c0bb3beaf2b",
       "IPY_MODEL_98f5c3d77e784340937dfae1170b5f59",
       "IPY_MODEL_0db69d4a110e4cdc869cc2efaa0554d1"
      ],
      "layout": "IPY_MODEL_219afa17846c4559b2d0cf62ab0aefc6"
     }
    },
    "a7ebd3ba058f4ab5a0d90a16ed7e7d4d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "abb5b8d77531458f9fc027d359428fe1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad64a3ffc0654b81ad899d02f2f1a6c9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c8c0a899e9ff49d08a732f0dc94db3de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d68e1669d72f44ada0ccb143ab4cb1ae",
       "IPY_MODEL_1144088a6f814f6a90b11d61b8f25d82",
       "IPY_MODEL_4e336f48c3a44c24b494d8fd681c36d4"
      ],
      "layout": "IPY_MODEL_cff23bdd315b4655b6ed8355456c80f2"
     }
    },
    "cac7e5ac9abb4c469acc97476ad1dc4b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cbefe95319af473dbbfc45610e4af174": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad64a3ffc0654b81ad899d02f2f1a6c9",
      "placeholder": "​",
      "style": "IPY_MODEL_4e8954aabb184c728eb31e7ac2f49e25",
      "value": " 733k/733k [00:00&lt;00:00, 1.55MB/s]"
     }
    },
    "cf4282f8f75d4113be5240af50960478": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5fbefd38c53e482b973a739d832c7d04",
      "max": 667,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_301968824bcf4797bf64ec53a1d7dc6f",
      "value": 667
     }
    },
    "cff23bdd315b4655b6ed8355456c80f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d59d26c0b1f24e8bb90a923396cb2165": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d68e1669d72f44ada0ccb143ab4cb1ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7d8de37eaff54ece9765aa7f93708db7",
      "placeholder": "​",
      "style": "IPY_MODEL_96cf6f39b3a845788f201ca57810cfed",
      "value": "Downloading pytorch_model.bin: 100%"
     }
    },
    "d77acfdb947c44bd9a06e1743606b6d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f8915edc81c44f62824f56eca89302d0",
       "IPY_MODEL_74f8af9a05f941519e17d334fc596bbd",
       "IPY_MODEL_fb846a62b0e04c08bd29f6b4340dfb28"
      ],
      "layout": "IPY_MODEL_386b7016fd454b798f21b249ba0fc23c"
     }
    },
    "d93ab43bac6545c4ad82e4a44a9c2d04": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e13ca209a08d40a599745c247acbe9f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eea86a4731dd4979b4910e05715333d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f8915edc81c44f62824f56eca89302d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_22175bccca6f46e48e523bed0cd565a6",
      "placeholder": "​",
      "style": "IPY_MODEL_7cfea8b1145c49f8801162c29df81070",
      "value": "Downloading tokenizer.json: 100%"
     }
    },
    "fb846a62b0e04c08bd29f6b4340dfb28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cac7e5ac9abb4c469acc97476ad1dc4b",
      "placeholder": "​",
      "style": "IPY_MODEL_8b34605fb31d4c5cb841d179ca98017a",
      "value": " 1.19M/1.19M [00:00&lt;00:00, 1.53MB/s]"
     }
    },
    "fc606378c1814928ba931e29b5660d94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_49870c1e039b4cf8af88e40869788c08",
      "placeholder": "​",
      "style": "IPY_MODEL_68aecec4ae5843929c0577a71df4471f",
      "value": "Downloading vocab.txt: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
