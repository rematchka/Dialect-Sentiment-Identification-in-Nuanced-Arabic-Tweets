{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22950,"status":"ok","timestamp":1662644120357,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"},"user_tz":-120},"id":"vMmciFBRyRFe","outputId":"faab4fae-7c78-4bef-e165-1523298b5def"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.97)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.21.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.9.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pytorch-ignite in /usr/local/lib/python3.7/dist-packages (0.4.10)\n","Requirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.12.1+cu113)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (21.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pytorch-ignite) (3.0.9)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.4.0)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.9.1)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.8.1)\n","Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.12.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.1.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.8.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.2.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"]}],"source":["!pip install sentencepiece\n","!pip  install transformers\n","!pip install pytorch-ignite\n","!pip install datasets\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"AtS4CD_krp3d","executionInfo":{"status":"ok","timestamp":1662644125540,"user_tz":-120,"elapsed":5190,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}}},"outputs":[],"source":["import argparse\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import pandas as pd\n","import random\n","import numpy as np\n","import torch.nn as nn\n","import torch\n","from transformers import AutoModel\n","import torch.nn.functional as F\n","from sklearn.metrics import f1_score, accuracy_score, classification_report\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","# Any results you write to the current directory are saved as output.\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the \"../input/\" directory.\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","from transformers import BertTokenizer,BertModel\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader,Dataset\n","from torch.nn.utils.rnn import pack_padded_sequence\n","from torch.optim import AdamW\n","from tqdm import tqdm\n","from argparse import ArgumentParser\n","from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n","from ignite.metrics import Accuracy, Loss\n","from ignite.engine.engine import Engine, State, Events\n","from ignite.handlers import EarlyStopping\n","from ignite.contrib.handlers import TensorboardLogger, ProgressBar\n","from ignite.utils import convert_tensor\n","from torch.optim.lr_scheduler import ExponentialLR\n","import warnings  \n","warnings.filterwarnings('ignore')\n","import gc\n","import copy\n","import time\n","import random\n","import string\n","\n","# For data manipulation\n","import numpy as np\n","import pandas as pd\n","\n","# Pytorch Imports\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Utils\n","from tqdm import tqdm\n","from collections import defaultdict\n","\n","# Sklearn Imports\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import StratifiedKFold, KFold\n","from transformers import AutoTokenizer, AutoModel, AdamW\n","import random\n","import os\n","from urllib import request\n","import numpy as np\n","from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n","\n","from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, BertTokenizer\n","from transformers.data.processors import SingleSentenceClassificationProcessor\n","from transformers import Trainer , TrainingArguments\n","from transformers.trainer_utils import EvaluationStrategy\n","from transformers.data.processors.utils import InputFeatures\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader"]},{"cell_type":"markdown","metadata":{"id":"ARjNBDtgwfVJ"},"source":["# Preprocessing"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"lLRNRjusoELI","executionInfo":{"status":"ok","timestamp":1662644125541,"user_tz":-120,"elapsed":13,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}}},"outputs":[],"source":["class TrainDataset(Dataset):\n","    def __init__(self, df, tokenizer, max_length):\n","        self.df = df\n","        self.max_len = max_length\n","        self.tokenizer = tokenizer\n","        self.text = df['#2_content'].values\n","        self.label=df['#3_label'].values\n","        \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, index):\n","        text = self.text[index]\n","        # summary = self.summary[index]\n","        inputs_text = self.tokenizer.encode_plus(\n","                                text,\n","                                truncation=True,\n","                                add_special_tokens=True,\n","                                max_length=self.max_len,\n","                                padding='max_length'\n","                            )\n","        \n","                            \n","        target = self.label[index]\n","        \n","        text_ids = inputs_text['input_ids']\n","        text_mask = inputs_text['attention_mask']\n","        \n","       \n","        \n","        \n","        return {\n","            \n","            'text_ids': torch.tensor(text_ids, dtype=torch.long),\n","            'text_mask': torch.tensor(text_mask, dtype=torch.long),\n","            'target': torch.tensor(target, dtype=torch.float)\n","        }"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"0ZOanFa2wz3L","executionInfo":{"status":"ok","timestamp":1662644125542,"user_tz":-120,"elapsed":13,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}}},"outputs":[],"source":["# import pandas as pd\n","# from sklearn.model_selection import train_test_split\n","# from torch.utils.data import Dataset, DataLoader\n","# import Dataset\n","# import os\n","# import numpy as np\n","# from emoji import UNICODE_EMOJI\n","# import TweetNormalizer\n","# import re\n","# import text_normalization\n","\n","\n","# dic = {\n","#       \"egypt\": 'المصرية',\n","# \t  \"nile\": 'المصرية',\n","# \t  \"msa\": \"اللغة العربية الفصحى\",\n","# \t  \"magreb\": \"المغربية\",\n","# \t  \"gulf\": \"الخليجية\",\n","# \t  \"levant\": \"الشامية\"\n","# }\n","\n","# def is_emoji(s):\n","#     return s in UNICODE_EMOJI\n","\n","# # add space near your emoji\n","# def add_space(text):\n","#     return ''.join(' ' + char if is_emoji(char) else char for char in text).strip()\n","\n","# def preprocess(text, lang='ar'):\n","#     if lang == 'ar':\n","#         sent = add_space(text)\n","#         sent = re.sub(r'(?:@[\\w_]+)', \"user\", sent)\n","#         sent = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', \"url\", sent)\n","#         sent = sent.replace('_', ' ')\n","#         sent = sent.replace('#', ' ')\n","#     else:\n","#         sent = add_space(text)\n","#         sent = re.sub(r'(?:@[\\w_]+)', \"@user\", sent)\n","#         sent = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', \"http\", sent)\n","#         sent = sent.replace('_', ' ')\n","#         sent = sent.replace('#', ' ')\n","\n","#     return sent\n","\n","# def prepare_text(df, col='tweet'):\n","#     if col == 'tweet':\n","#         df['dialect'] = df['dialect'].map(dic)   \n","#     for i in range(df.shape[0]):\n","#         df.loc[i, col] = df.loc[i, 'dialect'] + ' [SEP] ' + df.loc[i, col]\n","\n","\n","#     return df\n","\n","# def augment_data(df_train,text_col,label_col):\n","#     df_aug = pd.DataFrame(columns=[text_col, label_col)\n","#     dic_dup = {1: 3,\n","#                0: 1\n","#                }\n","#     for i in range(df_train.shape[0]):\n","#         current = df_train.iloc[i]\n","#         text = current[text_col]\n","#         label_cat = current[label_col]\n","\n","#         aug_ratio = dic_dup[label_cat]\n","#         for k in range(aug_ratio):\n","#             tokens = text.split(' ')\n","#             l = len(tokens)\n","#             n = int(0.1 * l)\n","#             indices = np.random.choice(l, n, replace=False)\n","#             for j in range(len(indices)):\n","#                 tokens[indices[j]] = '[MASK]'\n","#             new_text = ' '.join(tokens)\n","#             entry = {text_col: new_text, label_col: label_cat}\n","#             df_aug = df_aug.append(entry, ignore_index=True)\n","#     df_aug.drop_duplicates(subset=[text_col], keep='first', inplace=True)\n","#     df = pd.concat([df_train,df_aug])\n","#     return df\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"QcOa9agTwhIg","executionInfo":{"status":"ok","timestamp":1662644125542,"user_tz":-120,"elapsed":13,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}}},"outputs":[],"source":["# '''\n","# Created by: Mohamed Salem Elhady  \n","# Email: mohamed.elaraby@alumni.ubc.ca\n","# Text Normalization: V1 \n","# '''\n","# import sys\n","# import re\n","# import emojis\n","# from emoji import UNICODE_EMOJI\n","# #sys.setdefaultencoding('utf-8')\n","# ##########################Clean Text Data #######################################\n","# ########################Global Variable Declaration##############################\n","# list_seeds = ['سبحان الله', 'الله أكبر', 'اللهم', 'بسم الله', 'يا رب', 'العضيم', 'سبحان', 'يارب', 'قران', 'quran',\n","#               'حديث', 'hadith', 'صلاه_الفجر', '﴾', 'ﷺ', 'صحيح البخاري', 'صحيح مسلم', 'يآرب', 'سورة']\n","# MaxWordPerTweet=7\n","# #################################################################################\n","# def is_emoji(s):\n","#     return s in UNICODE_EMOJI\n","\n","# # add space near your emoji\n","# def add_space(text):\n","#     return ''.join(' ' + char if is_emoji(char) else char for char in text).strip()\n","\n","# def clean(sent):\n","#     \"\"\"clean data from any English char, emoticons, underscore, and repeated > 2\n","#     str -> str\"\"\"\n","#     p1 = re.compile('\\W')\n","#     p2 = re.compile('\\s+')\n","#     sent = re.sub(r\"http\\S+\", \"\", sent)\n","#     sent = ReplaceThreeOrMore(sent)\n","#     sent = remove_unicode_diac(sent)\n","#     sent = sent.replace('_', ' ')\n","#     sent = re.sub(r'[A-Za-z0-9]', r'', sent)\n","#     sent = re.sub(p1, ' ', sent)\n","#     sent = re.sub(p2, ' ', sent)\n","#     return sent\n","\n","# def tokenize_emojis(tweet):\n","#     return list(emojis.get(tweet))\n","\n","# def replace_emoji(sent):\n","#     emoji_pattern = re.compile(\"[\"\n","#                                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","#                                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","#                                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","#                                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","#                                \"]+\", flags=re.UNICODE)\n","#     return emoji_pattern.sub(r'[MASK]', sent)\n","\n","# def preprocess(tweet):\n","#     tweet = add_space(tweet)\n","#     emos = tokenize_emojis(tweet)\n","#     sent = remove_unicode_diac(tweet)\n","#     sent = re.sub(r'(?:@[\\w_]+)', \"user\", sent)\n","#     sent = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', \"url\", sent)\n","#     sent = sent.replace('_', ' ')\n","#     sent = sent.replace('#', ' ')\n","#     if len(emos) > 0:\n","#         sent = sent + ' [SEP] '  + ' '.join(emos)\n","#     #    #sent = sent + ' [SEP] ' + clean_unicode(tweet) + ' [SEP] ' + ' '.join(emos)\n","\n","#     #else:\n","#     #    sent = sent + ' [SEP] ' + clean_unicode(tweet)\n","#     return sent\n","\n","# def preprocess_last(tweet, k=0):\n","#     emos = tokenize_emojis(tweet)\n","#     sent = remove_unicode_diac(tweet)\n","#     sent = re.sub(r'(?:@[\\w_]+)', \"\", sent)\n","#     sent = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', \"\", sent)\n","#     sent = sent.replace('_', ' ')\n","#     sent = sent.replace('#', ' ')\n","#     if k == 0:\n","#         sent = sent\n","#     elif k ==1 :\n","#         sent = sent + ' [SEP] ' + ' '.join(emos)\n","#     elif k==2 :\n","#         sent = sent + ' [SEP] ' + clean_unicode(tweet) + ' [SEP] ' + ' '.join(emos)\n","#     elif k == 3:\n","#         sent = sent + ' [SEP] ' + clean_unicode(tweet)\n","#     else:\n","#         sent = replace_emoji(sent)\n","#         sent = sent + ' [SEP] ' + clean_unicode(tweet) + ' [SEP] ' + ' '.join(emos)\n","\n","#     return sent\n","\n","\n","# def normalize(sent):\n","#     \"\"\"clean data from any English char, emoticons, underscore, and repeated > 2\n","#     str -> str\"\"\"\n","#     sent = re.sub(r'(?:@[\\w_]+)', \"user\", sent)\n","#     sent = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', \"url\", sent)\n","#     #sent = re.sub(r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", \"hashtag\", sent)\n","#     sent = ReplaceThreeOrMore(sent)\n","#     sent = remove_unicode_diac(sent)\n","#     sent = sent.replace('_', ' ')\n","#     return sent\n","\n","# def ReplaceThreeOrMore(s):\n","#     # pattern to look for three or more repetitions of any character, including\n","#     # newlines.\n","#     pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL)\n","#     return pattern.sub(r\"\\1\\1\", s)\n","# def norm_alif(text):\n","#     text = text.replace(u\"\\u0625\", u\"\\u0627\")  # HAMZA below, with LETTER ALEF\n","#     #text = text.replace(u\"\\u0621\", u\"\\u0627\")  # HAMZA, with LETTER ALEF\n","#     text = text.replace(u\"\\u0622\", u\"\\u0627\")  # ALEF WITH MADDA ABOVE, with LETTER ALEF\n","#     text = text.replace(u\"\\u0623\", u\"\\u0627\")  # ALEF WITH HAMZA ABOVE, with LETTER ALEF\n","#     return text\n","# def remove_unicode_diac(text):\n","#     \"\"\"Takes Arabic in utf-8 and returns same text without diac\"\"\"\n","#     # Replace diacritics with nothing\n","#     text = text.replace(u\"\\u064B\", \"\")  # fatHatayn\n","#     text = text.replace(u\"\\u064C\", \"\")  # Dammatayn\n","#     text = text.replace(u\"\\u064D\", \"\")  # kasratayn\n","#     text = text.replace(u\"\\u064E\", \"\")  # fatHa\n","#     text = text.replace(u\"\\u064F\", \"\")  # Damma\n","#     text = text.replace(u\"\\u0650\", \"\")  # kasra\n","#     text = text.replace(u\"\\u0651\", \"\")  # shaddah\n","#     text = text.replace(u\"\\u0652\", \"\")  # sukuun\n","#     text = text.replace(u\"\\u0670\", \"`\")  # dagger 'alif\n","#     return text\n","# def norm_taa(text):\n","#     text=text.replace(u\"\\u0629\", u\"\\u0647\") # taa' marbuuTa, with haa'\n","#     #text=text.replace(u\"\\u064A\", u\"\\u0649\") # yaa' with 'alif maqSuura\n","#     return text\n","# def norm_yaa(text):\n","#     if len(text)!=0:\n","#         if text[-1] == u\"\\u064A\":\n","#             text = text[:-1] + text[-1].replace(u\"\\u064A\", u\"\\u0649\")  # yaa' with 'alif maqSuura\n","#     return text\n","\n","# def NormForWord2Vec(text):\n","#     text=norm_taa(text)\n","#     text=norm_yaa(text)\n","#     text=norm_alif(text)\n","#     return text\n","\n","# def remove_nonunicode2(Tweet):\n","#     ## defining set of unicode ##\n","#     #u\"\"\n","#     #Tweet=Tweet.decode(\"utf-8\")\n","#     UniLex={ ## This is list of all arabic unicode characters in addition to space (to separate words)\n","#             u\"\\u0622\",\n","#             u\"\\u0626\",\n","#             u\"\\u0628\",\n","#             u\"\\u062a\",\n","#             u\"\\u062c\",\n","#             u\"\\u06af\",\n","#             u\"\\u062e\",\n","#             u\"\\u0630\",\n","#             u\"\\u0632\",\n","#             u\"\\u0634\",\n","#             u\"\\u0636\",\n","#             u\"\\u0638\",\n","#             u\"\\u063a\",\n","#             u\"\\u0640\",\n","#             u\"\\u0642\",\n","#             u\"\\u0644\",\n","#             u\"\\u0646\",\n","#             u\"\\u0648\",\n","#             u\"\\u064a\",\n","#             u\"\\u0670\",\n","#             u\"\\u067e\",\n","#             u\"\\u0686\",\n","#             u\"\\u0621\",\n","#             u\"\\u0623\",\n","#             u\"\\u0625\",\n","#             u\"\\u06a4\",\n","#             u\"\\u0627\",\n","#             u\"\\u0629\",\n","#             u\"\\u062b\",\n","#             u\"\\u062d\",\n","#             u\"\\u062f\",\n","#             u\"\\u0631\",\n","#             u\"\\u0633\",\n","#             u\"\\u0635\",\n","#             u\"\\u0637\",\n","#             u\"\\u0639\",\n","#             u\"\\u0641\",\n","#             u\"\\u0643\",\n","#             u\"\\u0645\",\n","#             u\"\\u0647\",\n","#             u\"\\u0649\",\n","#             u\"\\u0671\",\n","#             ' ',\n","#             '\\n'\n","#           }\n","#     fin_tweet=\"\"\n","#     for c in Tweet:\n","#         if c in UniLex:\n","#            fin_tweet=fin_tweet+c\n","#     return fin_tweet\n","\n","# ###### Heuristics Calculations ######\n","# def diac_counter(text):\n","#     #text=text.decode(\"utf-8\")\n","#     diac = [u\"\\u064B\",u\"\\u064C\", u\"\\u064D\", u\"\\u064E\", u\"\\u064F\", u\"\\u0650\", u\"\\u0651\", u\"\\u0652\", u\"\\u0670\"]\n","#     diac_count=0\n","#     for d in diac:\n","#         diac_count+=text.count(d)\n","# #         if d in text:\n","# #             print(d)\n","# #             diac_count+=1\n","#     return diac_count\n","# def check_seed(list_seeds, text):\n","#     \"\"\n","#     for word in list_seeds:\n","#         text = text.lower()\n","#         if word.decode(\"utf-8\") in text:\n","#             return True\n","#     return False\n","# def EnglishCount(text):\n","#     printable = ['e', 'a', 'o', 't', 'i']\n","#     count = 0\n","#     for ch in printable:\n","#         count += text.count(ch.lower())\n","#     return count\n","# ########################################\n","\n","\n","\n","# def eliminate_single_char_words(Tweet):\n","#     parts = Tweet.split(\" \")\n","#     cleaned_line_parts = []\n","#     for P in parts:\n","#         if len(P) != 1:\n","#             cleaned_line_parts.append(P)\n","#     cleaned_line = ' '.join(cleaned_line_parts)\n","#     return cleaned_line\n","# def clean_unicode(Tweet):\n","#     tweet=normalize(Tweet.strip(\"\\n\"))\n","#     if len(tweet) !=0:\n","#         sentence = []\n","#         for word in tweet.split(\" \"):\n","#             word = remove_unicode_diac(word)\n","#             word = norm_alif(word)\n","#             word = norm_taa(word)\n","#             word = norm_yaa(word)\n","#             word = normalize(word)\n","#             sentence.append(word)\n","#         tweet = ' '.join(sentence)\n","#         tweet =remove_nonunicode2(tweet)\n","#         tweet =eliminate_single_char_words(tweet)\n","#     return tweet\n","\n","# def clean_unicode2(Tweet):\n","#     KeepUniOnly(Tweet)\n","#     tweet=normalize(Tweet.strip(\"\\n\"))\n","#     if len(tweet) !=0:\n","#         sentence = []\n","#         for word in tweet.split(\" \"):\n","#             word = remove_unicode_diac(word)\n","#             word = normalize(word)\n","#             sentence.append(word)\n","#         tweet = ' '.join(sentence)\n","#         tweet =remove_nonunicode2(tweet)\n","#         tweet =eliminate_single_char_words(tweet)\n","#     return tweet\n","\n","# def NormCorpusFinal(Tweet):\n","#     tweet=KeepUniOnly(Tweet)\n","#     tweet=NormForWord2Vec(tweet)\n","#     return tweet\n","\n","# def KeepUniOnly(Tweet):## this one is without normalization\n","#     tweet=Tweet.replace(\"# \",\" \")\n","#     tweet=tweet.replace(\"#\",\" \")\n","#     tweet=tweet.replace(\"_\",\" \")\n","#     tweet=tweet.replace(u\"\\u0657\",\" \")\n","#     tweet=tweet.replace(\"\\n\",\" \")\n","#     tweet=remove_nonunicode2(tweet)\n","#     tweet=eliminate_single_char_words(tweet)\n","#     tweet=ReplaceThreeOrMore(tweet)\n","#     return tweet\n","\n","# def get_charset(rawtext):\n","#     chars = sorted(list(set(rawtext)))\n","#     return chars\n","\n","# def DialectChecker(text):\n","#     ##Based on Hueristics done by Hassan\n","#     if (diac_counter(text)>5 or check_seed(list_seeds,text) or EnglishCount(text)>4 or \"<URL>\"  in text\n","#         or text.count('#') >2 or '\"'  in text or text.count('@') or \"\\\"RT\" in text or len(text.split(\" \")) <7):\n","#         return False\n","#     else:\n","#         return True\n","\n","# ###############################################################\n","# '''\n","# Fread=open(\"Egypt_portion.txt\",'r')\n","# Fwriter=open(\"Egypt_portion_norm.txt\",'w')\n","# for line in Fread:\n","#     cleaned_line=clean_unicode_for_w2v(line)\n","#     Fwriter.write(str(cleaned_line))\n","# Fwriter.close()\n","# '''"]},{"cell_type":"markdown","metadata":{"id":"iVcFbe1Bvcqh"},"source":["# Losses"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"RGw6mbs0uIlN","executionInfo":{"status":"ok","timestamp":1662644125543,"user_tz":-120,"elapsed":13,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}}},"outputs":[],"source":["class F1_Loss(nn.Module):\n","    '''Calculate F1 score. Can work with gpu tensors\n","    \n","    The original implmentation is written by Michal Haltuf on Kaggle.\n","    \n","    Returns\n","    -------\n","    torch.Tensor\n","        `ndim` == 1. epsilon <= val <= 1\n","    \n","    Reference\n","    ---------\n","    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n","    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n","    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\n","    - http://www.ryanzhang.info/python/writing-your-own-loss-function-module-for-pytorch/\n","    '''\n","    def __init__(self, epsilon=1e-7):\n","        super().__init__()\n","        self.epsilon = epsilon\n","        \n","    def forward(self, y_pred, y_true,):\n","        # assert y_pred.ndim == 2\n","        # assert y_true.ndim == 1\n","        # print(y_pred.shape)\n","        # print(y_true.shape)\n","        # y_pred[y_pred<0.5]=0\n","        # y_pred[y_pred>=0.5]=0\n","\n","\n","        \n","        y_true_one_hot = F.one_hot(y_true.to(torch.int64), 18).to(torch.float32)\n","        # y_pred_one_hot = F.one_hot(y_pred.to(torch.int64), 2).to(torch.float32)\n","        \n","        tp = (y_true_one_hot * y_pred).sum(dim=0).to(torch.float32)\n","        tn = ((1 - y_true_one_hot) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n","        fp = ((1 - y_true_one_hot) * y_pred).sum(dim=0).to(torch.float32)\n","        fn = (y_true_one_hot * (1 - y_pred)).sum(dim=0).to(torch.float32)\n","\n","        precision = tp / (tp + fp + self.epsilon)\n","        recall = tp / (tp + fn + self.epsilon)\n","\n","        f1 = 2* (precision*recall) / (precision + recall + self.epsilon)\n","        f1 = f1.clamp(min=self.epsilon, max=1-self.epsilon)\n","        f1=f1.detach()\n","        # print(f1.shape)\n","        # y_pred=y_pred.reshape((y_pred.shape[0], 1))\n","        # y_true=y_true.reshape((y_true.shape[0], 1))\n","\n","        # p1=y_true*(math.log(sigmoid(y_pred)))*(1-f1)[1]\n","        # p0=(1-y_true)*math.log(1-sigmoid(y_pred))*(1-f1)[0]\n","\n","\n","        # y_true_one_hot = F.one_hot(y_true.to(torch.int64), 2)\n","        # print(y_pred)\n","        # print(y_true_one_hot)\n","        CE =torch.nn.CrossEntropyLoss(weight=( 1 - f1))(y_pred, y_true_one_hot)\n","        # loss = ( 1 - f1)  * CE\n","        return  CE.mean()"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"Pdd02bGivpr5","executionInfo":{"status":"ok","timestamp":1662644125543,"user_tz":-120,"elapsed":13,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}}},"outputs":[],"source":["class Recall_Loss(nn.Module):\n","    '''Calculate Recall score. Can work with gpu tensors\n","    \n","    The original implmentation is written by Michal Haltuf on Kaggle.\n","    \n","    Returns\n","    -------\n","    torch.Tensor\n","        `ndim` == 1. epsilon <= val <= 1\n","    \n","    Reference\n","    ---------\n","    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n","    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n","    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\n","    - http://www.ryanzhang.info/python/writing-your-own-loss-function-module-for-pytorch/\n","    '''\n","    def __init__(self, epsilon=1e-7):\n","        super().__init__()\n","        self.epsilon = epsilon\n","        \n","    def forward(self, y_pred, y_true,):\n","        # assert y_pred.ndim == 2\n","        # assert y_true.ndim == 1\n","        # print(y_pred.shape)\n","        # print(y_true.shape)\n","        # y_pred[y_pred<0.5]=0\n","        # y_pred[y_pred>=0.5]=0\n","\n","\n","        \n","        y_true_one_hot = F.one_hot(y_true.to(torch.int64), 18).to(torch.float32)\n","        # y_pred_one_hot = F.one_hot(y_pred.to(torch.int64), 2).to(torch.float32)\n","        \n","        tp = (y_true_one_hot * y_pred).sum(dim=0).to(torch.float32)\n","        tn = ((1 - y_true_one_hot) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n","        fp = ((1 - y_true_one_hot) * y_pred).sum(dim=0).to(torch.float32)\n","        fn = (y_true_one_hot * (1 - y_pred)).sum(dim=0).to(torch.float32)\n","\n","        precision = tp / (tp + fp + self.epsilon)\n","        recall = tp / (tp + fn + self.epsilon)\n","\n","        # f1 = 2* (precision*recall) / (precision + recall + self.epsilon)\n","        # f1 = f1.clamp(min=self.epsilon, max=1-self.epsilon)\n","        # f1=f1.detach()\n","        # print(f1.shape)\n","        # y_pred=y_pred.reshape((y_pred.shape[0], 1))\n","        # y_true=y_true.reshape((y_true.shape[0], 1))\n","\n","        # p1=y_true*(math.log(sigmoid(y_pred)))*(1-f1)[1]\n","        # p0=(1-y_true)*math.log(1-sigmoid(y_pred))*(1-f1)[0]\n","\n","\n","        # y_true_one_hot = F.one_hot(y_true.to(torch.int64), 2)\n","        # print(y_pred)\n","        # print(y_true_one_hot)\n","        recall=recall.detach()\n","        CE =torch.nn.CrossEntropyLoss(weight=( 1 - recall))(y_pred, y_true_one_hot)\n","        # loss = ( 1 - f1)  * CE\n","        return  CE.mean()"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"zKy23cw1wHB9","executionInfo":{"status":"ok","timestamp":1662644125544,"user_tz":-120,"elapsed":13,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}}},"outputs":[],"source":["## Based on https://github.com/AbdelkaderMH/iSarcasmEval/blob/8f28f24ebfb641415a604329ed859506ae687148/focal_loss.py\n","class BinaryFocalLoss(nn.Module):\n","    \"\"\"\n","    This is a implementation of Focal Loss with smooth label cross entropy supported which is proposed in\n","    'Focal Loss for Dense Object Detection. (https://arxiv.org/abs/1708.02002)'\n","        Focal_Loss= -1*alpha*(1-pt)*log(pt)\n","    :param alpha: (tensor) 3D or 4D the scalar factor for this criterion\n","    :param gamma: (float,double) gamma > 0 reduces the relative loss for well-classified examples (p>0.5) putting more\n","                    focus on hard misclassified example\n","    :param reduction: `none`|`mean`|`sum`\n","    :param **kwargs\n","        balance_index: (int) balance class index, should be specific when alpha is float\n","    \"\"\"\n","\n","    def __init__(self, alpha=3, gamma=2, ignore_index=None, reduction='mean', **kwargs):\n","        super(BinaryFocalLoss, self).__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","        self.smooth = 1e-6  # set '1e-4' when train with FP16\n","        self.ignore_index = ignore_index\n","        self.reduction = reduction\n","\n","        assert self.reduction in ['none', 'mean', 'sum']\n","\n","        # if self.alpha is None:\n","        #     self.alpha = torch.ones(2)\n","        # elif isinstance(self.alpha, (list, np.ndarray)):\n","        #     self.alpha = np.asarray(self.alpha)\n","        #     self.alpha = np.reshape(self.alpha, (2))\n","        #     assert self.alpha.shape[0] == 2, \\\n","        #         'the `alpha` shape is not match the number of class'\n","        # elif isinstance(self.alpha, (float, int)):\n","        #     self.alpha = np.asarray([self.alpha, 1.0 - self.alpha], dtype=np.float).view(2)\n","\n","        # else:\n","        #     raise TypeError('{} not supported'.format(type(self.alpha)))\n","\n","    def forward(self, output, target):\n","        prob = torch.sigmoid(output)\n","        prob = torch.clamp(prob, self.smooth, 1.0 - self.smooth)\n","\n","        valid_mask = None\n","        if self.ignore_index is not None:\n","            valid_mask = (target != self.ignore_index).float()\n","\n","        pos_mask = (target == 1).float()\n","        neg_mask = (target == 0).float()\n","        if valid_mask is not None:\n","            pos_mask = pos_mask * valid_mask\n","            neg_mask = neg_mask * valid_mask\n","\n","        pos_weight = (pos_mask * torch.pow(1 - prob, self.gamma)).detach()\n","        pos_loss = -pos_weight * torch.log(prob)  # / (torch.sum(pos_weight) + 1e-4)\n","\n","        neg_weight = (neg_mask * torch.pow(prob, self.gamma)).detach()\n","        neg_loss = -self.alpha * neg_weight * F.logsigmoid(-output)  # / (torch.sum(neg_weight) + 1e-4)\n","        loss = pos_loss + neg_loss\n","        loss = loss.mean()\n","        return loss\n","\n","\n","class FocalLoss_Ori(nn.Module):\n","    \"\"\"\n","    This is a implementation of Focal Loss with smooth label cross entropy supported which is proposed in\n","    'Focal Loss for Dense Object Detection. (https://arxiv.org/abs/1708.02002)'\n","    Focal_Loss= -1*alpha*((1-pt)**gamma)*log(pt)\n","    Args:\n","        num_class: number of classes\n","        alpha: class balance factor\n","        gamma:\n","        ignore_index:\n","        reduction:\n","    \"\"\"\n","\n","    def __init__(self, num_class, alpha=None, gamma=2, ignore_index=None, reduction='mean'):\n","        super(FocalLoss_Ori, self).__init__()\n","        self.num_class = num_class\n","        self.gamma = gamma\n","        self.reduction = reduction\n","        self.smooth = 1e-4\n","        self.ignore_index = ignore_index\n","        self.alpha = alpha\n","        if alpha is None:\n","            self.alpha = torch.ones(num_class, )\n","        elif isinstance(alpha, (int, float)):\n","            self.alpha = torch.as_tensor([alpha] * num_class)\n","        elif isinstance(alpha, (list, np.ndarray)):\n","            self.alpha = torch.as_tensor(alpha)\n","        if self.alpha.shape[0] != num_class:\n","            raise RuntimeError('the length not equal to number of class')\n","\n","        # if isinstance(self.alpha, (list, tuple, np.ndarray)):\n","        #     assert len(self.alpha) == self.num_class\n","        #     self.alpha = torch.Tensor(list(self.alpha))\n","        # elif isinstance(self.alpha, (float, int)):\n","        #     assert 0 < self.alpha < 1.0, 'alpha should be in `(0,1)`)'\n","        #     assert balance_index > -1\n","        #     alpha = torch.ones((self.num_class))\n","        #     alpha *= 1 - self.alpha\n","        #     alpha[balance_index] = self.alpha\n","        #     self.alpha = alpha\n","        # elif isinstance(self.alpha, torch.Tensor):\n","        #     self.alpha = self.alpha\n","        # else:\n","        #     raise TypeError('Not support alpha type, expect `int|float|list|tuple|torch.Tensor`')\n","\n","    def forward(self, logit, target):\n","        # assert isinstance(self.alpha,torch.Tensor)\\\n","        N, C = logit.shape[:2]\n","        alpha = self.alpha.to(logit.device)\n","        prob = F.softmax(logit, dim=1)\n","        if prob.dim() > 2:\n","            # N,C,d1,d2 -> N,C,m (m=d1*d2*...)\n","            prob = prob.view(N, C, -1)\n","            prob = prob.transpose(1, 2).contiguous()  # [N,C,d1*d2..] -> [N,d1*d2..,C]\n","            prob = prob.view(-1, prob.size(-1))  # [N,d1*d2..,C]-> [N*d1*d2..,C]\n","        ori_shp = target.shape\n","        target = target.view(-1, 1)  # [N,d1,d2,...]->[N*d1*d2*...,1]\n","        valid_mask = None\n","        if self.ignore_index is not None:\n","            valid_mask = target != self.ignore_index\n","            target = target * valid_mask\n","\n","        # ----------memory saving way--------\n","        prob = prob.gather(1, target).view(-1) + self.smooth  # avoid nan\n","        logpt = torch.log(prob)\n","        # alpha_class = alpha.gather(0, target.view(-1))\n","        alpha_class = alpha[target.squeeze().long()]\n","        class_weight = -alpha_class * torch.pow(torch.sub(1.0, prob), self.gamma)\n","        loss = class_weight * logpt\n","        if valid_mask is not None:\n","            loss = loss * valid_mask.squeeze()\n","\n","        if self.reduction == 'mean':\n","            loss = loss.mean()\n","            if valid_mask is not None:\n","                loss = loss.sum() / valid_mask.sum()\n","        elif self.reduction == 'none':\n","            loss = loss.view(ori_shp)\n","        return loss\n","\n","\n","\n","def weighted_binary_cross_entropy(input, targets, pos_weight, weight=None, size_average=True, reduce=True):\n","    \"\"\"\n","    Args:\n","        sigmoid_x: predicted probability of size [N,C], N sample and C Class. Eg. Must be in range of [0,1], i.e. Output from Sigmoid.\n","        targets: true value, one-hot-like vector of size [N,C]\n","        pos_weight: Weight for postive sample\n","    \"\"\"\n","    sigmoid_x = torch.sigmoid(input)\n","    if not (targets.size() == sigmoid_x.size()):\n","        raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(targets.size(), sigmoid_x.size()))\n","\n","    loss = -pos_weight* targets * sigmoid_x.log() - (1-targets)*(1-sigmoid_x).log()\n","\n","    if weight is not None:\n","        loss = loss * weight\n","\n","    if not reduce:\n","        return loss\n","    elif size_average:\n","        return loss.mean()\n","    else:\n","        return loss.sum()\n","\n","class WeightedBCELoss(nn.Module):\n","    def __init__(self, pos_weight= 1, weight=None, PosWeightIsDynamic= True, WeightIsDynamic= False, size_average=True, reduce=True):\n","        \"\"\"\n","        Args:\n","            pos_weight = Weight for postive samples. Size [1,C]\n","            weight = Weight for Each class. Size [1,C]\n","            PosWeightIsDynamic: If True, the pos_weight is computed on each batch. If pos_weight is None, then it remains None.\n","            WeightIsDynamic: If True, the weight is computed on each batch. If weight is None, then it remains None.\n","        \"\"\"\n","        super().__init__()\n","\n","        #self.register_buffer('weight', weight)\n","        #self.register_buffer('pos_weight', pos_weight)\n","        self.size_average = size_average\n","        self.reduce = reduce\n","        self.PosWeightIsDynamic = PosWeightIsDynamic\n","\n","    def forward(self, input, target):\n","        # pos_weight = Variable(self.pos_weight) if not isinstance(self.pos_weight, Variable) else self.pos_weight\n","        if self.PosWeightIsDynamic:\n","            positive_counts = target.sum(dim=0)\n","            nBatch = len(target)\n","            self.pos_weight = (nBatch - positive_counts)/(positive_counts +1e-5)\n","\n","\n","        return weighted_binary_cross_entropy(input, target,\n","                                                self.pos_weight,\n","                                                weight=None,\n","                                                size_average=self.size_average,\n","                                                reduce=self.reduce)\n","\n","\n","class WeightedCELoss(nn.Module):\n","    def __init__(self, size_average=True, reduce=True):\n","        \"\"\"\n","        Args:\n","            pos_weight = Weight for postive samples. Size [1,C]\n","            weight = Weight for Each class. Size [1,C]\n","            PosWeightIsDynamic: If True, the pos_weight is computed on each batch. If pos_weight is None, then it remains None.\n","            WeightIsDynamic: If True, the weight is computed on each batch. If weight is None, then it remains None.\n","        \"\"\"\n","        super().__init__()\n","\n","        self.size_average = size_average\n","        self.reduce = reduce\n","\n","    def forward(self, input, target):\n","        positive_counts = target.sum(dim=0)\n","        nBatch = len(target)\n","        pos_weight = (nBatch - positive_counts)/(positive_counts +1e-5)\n","        neg_count = nBatch - positive_counts\n","        neg_weight = (nBatch - neg_count)/(neg_count +1e-5)\n","\n","        weight = torch.tensor([neg_weight, pos_weight], device=target.device)\n","  \n","\n","\n","        return F.cross_entropy(input, target, weight=weight)\n","\n","\n","\n","class FLMultiLoss(nn.Module):\n","    def __init__(self, gamma= 2):\n","        \"\"\"\n","        Args:\n","            pos_weight = Weight for postive samples. Size [1,C]\n","            weight = Weight for Each class. Size [1,C]\n","            PosWeightIsDynamic: If True, the pos_weight is computed on each batch. If pos_weight is None, then it remains None.\n","            WeightIsDynamic: If True, the weight is computed on each batch. If weight is None, then it remains None.\n","        \"\"\"\n","        super().__init__()\n","\n","        self.gamma = gamma\n","    def forward(self, input, target):\n","\n","\n","        return focal_binary_cross_entropy(input, target, gamma=2)\n","\n","def EntropyLoss(input_):\n","    mask = input_.ge(0.000001)\n","    mask_out = torch.masked_select(input_, mask)\n","    entropy = -(torch.sum(mask_out * torch.log(mask_out)))\n","    return entropy / float(input_.size(0))\n","\n","def focal_binary_cross_entropy(logits, targets, gamma=2):\n","    num_label = targets.shape[1]\n","    l = logits.reshape(-1)\n","    t = targets.reshape(-1)\n","    p = torch.sigmoid(l)\n","    p = torch.where(t >= 0.5, p, 1-p)\n","    logp = - torch.log(torch.clamp(p, 1e-4, 1-1e-4))\n","    loss = logp*((1-p)**gamma)\n","    loss = num_label*loss.mean()\n","    return loss"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"dG5x6Ck0nOh_","executionInfo":{"status":"ok","timestamp":1662644125544,"user_tz":-120,"elapsed":13,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}}},"outputs":[],"source":["from typing import Optional, Sequence\n","\n","import torch\n","from torch import Tensor\n","from torch import nn\n","from torch.nn import functional as F\n","\n","\n","class FocalLoss(nn.Module):\n","    \"\"\" Focal Loss, as described in https://arxiv.org/abs/1708.02002.\n","    It is essentially an enhancement to cross entropy loss and is\n","    useful for classification tasks when there is a large class imbalance.\n","    x is expected to contain raw, unnormalized scores for each class.\n","    y is expected to contain class labels.\n","    Shape:\n","        - x: (batch_size, C) or (batch_size, C, d1, d2, ..., dK), K > 0.\n","        - y: (batch_size,) or (batch_size, d1, d2, ..., dK), K > 0.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 alpha: Optional[Tensor] = None,\n","                 gamma: float = 0.,\n","                 reduction: str = 'mean',\n","                 ignore_index: int = -100):\n","        \"\"\"Constructor.\n","        Args:\n","            alpha (Tensor, optional): Weights for each class. Defaults to None.\n","            gamma (float, optional): A constant, as described in the paper.\n","                Defaults to 0.\n","            reduction (str, optional): 'mean', 'sum' or 'none'.\n","                Defaults to 'mean'.\n","            ignore_index (int, optional): class label to ignore.\n","                Defaults to -100.\n","        \"\"\"\n","        if reduction not in ('mean', 'sum', 'none'):\n","            raise ValueError(\n","                'Reduction must be one of: \"mean\", \"sum\", \"none\".')\n","\n","        super().__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","        self.ignore_index = ignore_index\n","        self.reduction = reduction\n","\n","        self.nll_loss = nn.NLLLoss(\n","            weight=alpha, reduction='none', ignore_index=ignore_index)\n","\n","    def __repr__(self):\n","        arg_keys = ['alpha', 'gamma', 'ignore_index', 'reduction']\n","        arg_vals = [self.__dict__[k] for k in arg_keys]\n","        arg_strs = [f'{k}={v}' for k, v in zip(arg_keys, arg_vals)]\n","        arg_str = ', '.join(arg_strs)\n","        return f'{type(self).__name__}({arg_str})'\n","\n","    def forward(self, x: Tensor, y: Tensor) -> Tensor:\n","        if x.ndim > 2:\n","            # (N, C, d1, d2, ..., dK) --> (N * d1 * ... * dK, C)\n","            c = x.shape[1]\n","            x = x.permute(0, *range(2, x.ndim), 1).reshape(-1, c)\n","            # (N, d1, d2, ..., dK) --> (N * d1 * ... * dK,)\n","            y = y.view(-1)\n","\n","        unignored_mask = y != self.ignore_index\n","        y = y[unignored_mask]\n","        if len(y) == 0:\n","            return torch.tensor(0.)\n","        x = x[unignored_mask]\n","\n","        # compute weighted cross entropy term: -alpha * log(pt)\n","        # (alpha is already part of self.nll_loss)\n","        log_p = F.log_softmax(x, dim=-1)\n","        ce = self.nll_loss(log_p, y)\n","\n","        # get true class column from each row\n","        all_rows = torch.arange(len(x))\n","        log_pt = log_p[all_rows, y]\n","\n","        # compute focal term: (1 - pt)^gamma\n","        pt = log_pt.exp()\n","        focal_term = (1 - pt)**self.gamma\n","\n","        # the full loss: -alpha * ((1 - pt)^gamma) * log(pt)\n","        loss = focal_term * ce\n","\n","        if self.reduction == 'mean':\n","            loss = loss.mean()\n","        elif self.reduction == 'sum':\n","            loss = loss.sum()\n","\n","        return loss\n","\n","\n","def focal_loss(alpha: Optional[Sequence] = None,\n","               gamma: float = 0.,\n","               reduction: str = 'mean',\n","               ignore_index: int = -100,\n","               device='cuda',\n","               dtype=torch.float32) -> FocalLoss:\n","    \"\"\"Factory function for FocalLoss.\n","    Args:\n","        alpha (Sequence, optional): Weights for each class. Will be converted\n","            to a Tensor if not None. Defaults to None.\n","        gamma (float, optional): A constant, as described in the paper.\n","            Defaults to 0.\n","        reduction (str, optional): 'mean', 'sum' or 'none'.\n","            Defaults to 'mean'.\n","        ignore_index (int, optional): class label to ignore.\n","            Defaults to -100.\n","        device (str, optional): Device to move alpha to. Defaults to 'cpu'.\n","        dtype (torch.dtype, optional): dtype to cast alpha to.\n","            Defaults to torch.float32.\n","    Returns:\n","        A FocalLoss object\n","    \"\"\"\n","    if alpha is not None:\n","        if not isinstance(alpha, Tensor):\n","            alpha = torch.tensor(alpha)\n","        alpha = alpha.to(device=device, dtype=dtype)\n","\n","    fl = FocalLoss(\n","        alpha=alpha,\n","        gamma=gamma,\n","        reduction=reduction,\n","        ignore_index=ignore_index)\n","    return fl"]},{"cell_type":"markdown","metadata":{"id":"fvByMHwKuI4d"},"source":["# Models"]},{"cell_type":"code","source":["def freeze(module):\n","    \"\"\"\n","    Freezes module's parameters.\n","    \"\"\"\n","    for parameter in module.parameters():\n","        parameter.requires_grad = False\n","\n","def get_freezed_parameters(module):\n","    \"\"\"\n","    Returns names of freezed parameters of the given module.\n","    \"\"\"\n","    freezed_parameters = []\n","    for name, parameter in module.named_parameters():\n","        if not parameter.requires_grad:\n","            freezed_parameters.append(name)"],"metadata":{"id":"D66s_K6LTOWp","executionInfo":{"status":"ok","timestamp":1662644125545,"user_tz":-120,"elapsed":14,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","execution_count":35,"metadata":{"id":"FW53GnvRuJ6J","executionInfo":{"status":"ok","timestamp":1662644175806,"user_tz":-120,"elapsed":251,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}}},"outputs":[],"source":["import torch.nn as nn\n","import torch\n","from transformers import AutoModel\n","import torch.nn.functional as F\n","\n","def init_weights(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv2d') != -1 or classname.find('ConvTranspose2d') != -1:\n","        nn.init.kaiming_uniform_(m.weight)\n","        nn.init.zeros_(m.bias)\n","    elif classname.find('BatchNorm') != -1:\n","        nn.init.normal_(m.weight, 1.0, 0.02)\n","        nn.init.zeros_(m.bias)\n","    elif classname.find('Linear') != -1:\n","        nn.init.xavier_normal_(m.weight)\n","        if m.bias is not None:\n","            nn.init.zeros_(m.bias)\n","\n","\n","class AttentionWithContext(nn.Module):\n","    def __init__(self, hidden_dim):\n","        super(AttentionWithContext, self).__init__()\n","\n","        self.attn = nn.Linear(hidden_dim, hidden_dim)\n","        self.contx = nn.Linear(hidden_dim, 1, bias=False)\n","        #self.apply(init_weights)\n","    def forward(self, inp):\n","        u = torch.tanh_(self.attn(inp))\n","        a = F.softmax(self.contx(u), dim=1)\n","        s = (a * inp).sum(1)\n","        return s\n","\n","\n","class TransformerLayer(nn.Module):\n","    def __init__(self,\n","                 pretrained_path='aubmindlab/bert-base-arabert'):\n","        super(TransformerLayer, self).__init__()\n","\n","        \n","        self.transformer = AutoModel.from_pretrained(pretrained_path, output_hidden_states=True)\n","\n","\n","    def forward(self, input_ids=None, attention_mask=None):\n","        outputs = self.transformer(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask\n","        )\n","        #(output_last_layer, pooled_cls, (output_layers))\n","        #output[0] (8, seqlen=64, 768) cls [8, 768] ( 12 (8, seqlen=64, 768))\n","\n","        return outputs\n","\n","    def output_num(self):\n","        return self.transformer.config.hidden_size\n","\n","class ATTClassifier(nn.Module):\n","    def __init__(self, in_feature, class_num=1, dropout_prob=0.2):\n","        super(ATTClassifier, self).__init__()\n","        self.attention = AttentionWithContext(in_feature)\n","\n","        self.Classifier = nn.Sequential(\n","            nn.Linear(2 * in_feature, 512),\n","            nn.Dropout(dropout_prob),\n","            nn.ReLU(),\n","            nn.Linear(512, class_num)\n","        )\n","\n","        self.apply(init_weights)\n","\n","    def forward(self, x):\n","        att = self.attention(x[0]) #(X[0] (bs, seqlenght, embedD) att = \\sum_i alpha_i x[0][i]\n","\n","        xx = torch.cat([att, x[1]], 1)\n","\n","        out = self.Classifier(xx)\n","        return out\n","\n","class NADIModel(nn.Module):\n","  def __init__(self, pretrained_path='aubmindlab/bert-base-arabert',in_feature=768, class_num=18, dropout_prob=0.2):\n","        super(NADIModel, self).__init__()\n","        self.base_model = AutoModel.from_pretrained(pretrained_path, output_hidden_states=True)\n","        freeze(self.base_model.embeddings)\n","        freeze(self.base_model.encoder.layer[:2])\n","        self.classifier=ATTClassifier(self.base_model.config.hidden_size, class_num=class_num).to('cuda')\n","  def forward(self, ids,mask):\n","    output=self.base_model(ids,mask)\n","    output=self.classifier(output)\n","    # output=torch.softmax(output, dim=1)\n","    return output\n"," "]},{"cell_type":"markdown","metadata":{"id":"SYQWLGzVyqt_"},"source":["# Train, Validation"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"pjYrf5IerGh7","executionInfo":{"status":"ok","timestamp":1662644176176,"user_tz":-120,"elapsed":3,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}}},"outputs":[],"source":["class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n","    \"\"\"\n","    Samples elements randomly from a given list of indices for imbalanced dataset\n","    Arguments:\n","        indices (list, optional): a list of indices\n","        num_samples (int, optional): number of samples to draw\n","    \"\"\"\n","\n","    def __init__(self, dataset, indices=None, num_samples=None):\n","        # if indices is not provided,\n","        # all elements in the dataset will be considered\n","        self.indices = list(range(len(dataset['#3_label']))) \\\n","            if indices is None else indices\n","\n","        # if num_samples is not provided,\n","        # draw `len(indices)` samples in each iteration\n","        self.num_samples = len(self.indices) \\\n","            if num_samples is None else num_samples\n","\n","        # distribution of classes in the dataset\n","        label_to_count = {}\n","        for idx in self.indices:\n","            label = self._get_label(dataset, idx)\n","            if label in label_to_count:\n","                label_to_count[label] += 1\n","            else:\n","                label_to_count[label] = 1\n","\n","        # weight for each sample\n","        weights = [1.0 / label_to_count[self._get_label(dataset, idx)] for idx in self.indices]\n","        self.weights = torch.DoubleTensor(weights)\n","\n","    def _get_label(self, dataset, id_):\n","        return dataset['#3_label'][id_]\n","\n","    def __iter__(self):\n","        return (self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True))\n","\n","    def __len__(self):\n","        return self.num_samples"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"ZWP3677hqMr1","executionInfo":{"status":"ok","timestamp":1662644176177,"user_tz":-120,"elapsed":4,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}}},"outputs":[],"source":["def criterion(outputs1,  targets):\n","\n","    criterion = FocalLoss()\n","    loss = criterion(outputs1, targets)\n","    return loss"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"ltIB44KZyxPt","executionInfo":{"status":"ok","timestamp":1662644176178,"user_tz":-120,"elapsed":4,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}}},"outputs":[],"source":["def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n","    model.train()\n","    \n","    dataset_size = 0\n","    running_loss = 0.0\n","    \n","    \n","    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n","    for step, data in bar:\n","        \n","        text_ids = data['text_ids'].to(device, dtype = torch.long)\n","        text_mask = data['text_mask'].to(device, dtype = torch.long)\n","        targets = data['target'].to(device, dtype=torch.long)\n","        \n","        batch_size = text_ids.size(0)\n","        # print(targets)\n","\n","        outputs = model(text_ids, text_mask)\n","        # print(outputs.shape)\n","        \n","        # print(outputs.shape)\n","\n","        \n","        loss = criterion(outputs, targets)\n","        loss = loss / CONFIG['n_accumulate']\n","        loss.backward()\n","    \n","        if (step + 1) % CONFIG['n_accumulate'] == 0:\n","            optimizer.step()\n","\n","            # zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            if scheduler is not None:\n","                scheduler.step()\n","                \n","        running_loss += (loss.item() * batch_size)\n","        dataset_size += batch_size\n","        \n","        epoch_loss = running_loss / dataset_size\n","        \n","        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n","                        LR=optimizer.param_groups[0]['lr'])\n","    gc.collect()\n","    \n","    return epoch_loss"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"CKOqGTqHGTGj","executionInfo":{"status":"ok","timestamp":1662644176179,"user_tz":-120,"elapsed":5,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}}},"outputs":[],"source":["from sklearn.metrics import jaccard_score,f1_score,accuracy_score,recall_score,precision_score,classification_report\n","def print_statistics(y, y_pred):\n","    accuracy = accuracy_score(y, y_pred)\n","    precision =precision_score(y, y_pred, average='weighted')\n","    recall = recall_score(y, y_pred, average='weighted')\n","    f_score = f1_score(y, y_pred, average='weighted')\n","    print('Accuracy: %.3f\\nPrecision: %.3f\\nRecall: %.3f\\nF_score: %.3f\\n'\n","          % (accuracy, precision, recall, f_score))\n","    print(classification_report(y, y_pred))\n","    return accuracy, precision, recall, f_score"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"U5Ro64pIy4HN","executionInfo":{"status":"ok","timestamp":1662644176506,"user_tz":-120,"elapsed":332,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}}},"outputs":[],"source":["@torch.no_grad()\n","def valid_one_epoch(model, dataloader, device, epoch):\n","    model.eval()\n","    \n","    dataset_size = 0\n","    running_loss = 0.0\n","    prediction=[]\n","    true_prediction=[]\n","    \n","    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n","    for step, data in bar:        \n","        \n","        text_ids = data['text_ids'].to(device, dtype = torch.long)\n","        text_mask = data['text_mask'].to(device, dtype = torch.long)\n","        targets = data['target'].to(device, dtype=torch.long)\n","        \n","        batch_size = text_ids.size(0)\n","\n","        outputs = model(text_ids, text_mask)\n","        prediction.append(F.softmax(outputs).to('cpu').numpy())\n","        true_prediction.append(targets.to('cpu').numpy())\n","        # outputs = outputs.argmax(dim=1)\n","        \n","        loss = criterion(outputs, targets)\n","        \n","        running_loss += (loss.item() * batch_size)\n","        dataset_size += batch_size\n","        \n","        epoch_loss = running_loss / dataset_size\n","        \n","        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n","                        LR=optimizer.param_groups[0]['lr'])   \n","    \n","    gc.collect()\n","    prediction = np.concatenate(prediction)\n","    true_prediction = np.concatenate(true_prediction)\n","    prediction=np.argmax(np.array(prediction),axis=1)\n","    print(print_statistics(np.array(true_prediction),prediction))\n","    print(f1_score(np.array(true_prediction),prediction, average='macro'))\n","    \n","    return f1_score(np.array(true_prediction),prediction, average='macro')"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"HpH2EB0snbV0","executionInfo":{"status":"ok","timestamp":1662644176507,"user_tz":-120,"elapsed":4,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}}},"outputs":[],"source":["def run_training(model, optimizer, scheduler, device, num_epochs, fold):\n","    # To automatically log gradients\n","    \n","    if torch.cuda.is_available():\n","        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n","    \n","    start = time.time()\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_epoch_loss = 0\n","    history = defaultdict(list)\n","    \n","    for epoch in range(1, num_epochs + 1): \n","        gc.collect()\n","        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, \n","                                           dataloader=train_loader, \n","                                           device=CONFIG['device'], epoch=epoch)\n","        \n","        val_epoch_loss = valid_one_epoch(model, valid_loader, device=CONFIG['device'], \n","                                         epoch=epoch)\n","    \n","        history['Train Loss'].append(train_epoch_loss)\n","        history['Valid Loss'].append(val_epoch_loss)\n","        \n","       \n","        \n","        # deep copy the model\n","        if val_epoch_loss >= best_epoch_loss:\n","            print(f\"Validation Loss Improved ({best_epoch_loss} ---> {val_epoch_loss})\")\n","            best_epoch_loss = val_epoch_loss\n","            best_model_wts = copy.deepcopy(model.state_dict())\n","            PATH = f\"/content/drive/MyDrive/wanlp/NADI/Saved_Models/Camel_bert/Loss-new-Fold-{fold}.bin\"\n","            torch.save(model.state_dict(), PATH)\n","            # Save a model file from the current directory\n","            print(\"Model Saved\")\n","            \n","        print()\n","    \n","    end = time.time()\n","    time_elapsed = end - start\n","    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n","    print(\"Best Loss: {:.4f}\".format(best_epoch_loss))\n","    \n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    \n","    return model, history"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"JxT6JHg0nogC","executionInfo":{"status":"ok","timestamp":1662644176508,"user_tz":-120,"elapsed":5,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}}},"outputs":[],"source":["def fetch_scheduler(optimizer):\n","    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n","        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CONFIG['T_max'], \n","                                                   eta_min=CONFIG['min_lr'])\n","    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n","        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=CONFIG['T_0'], \n","                                                             eta_min=CONFIG['min_lr'])\n","    elif CONFIG['scheduler'] == None:\n","        return None\n","        \n","    return scheduler"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"x-x9oUAsnqqy","executionInfo":{"status":"ok","timestamp":1662644176508,"user_tz":-120,"elapsed":5,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}}},"outputs":[],"source":["def prepare_loaders(train,valid):\n","    # df_train = train[train.kfold != fold].reset_index(drop=True)\n","    # df_valid = train[train.kfold == fold].reset_index(drop=True)\n","    # sampler = ImbalancedDatasetSampler(df_train)\n","    \n","    train_dataset = TrainDataset(train, tokenizer=tokenizer, max_length=CONFIG['max_length'])\n","    valid_dataset = TrainDataset(valid, tokenizer=tokenizer, max_length=CONFIG['max_length'])\n","\n","    train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], \n","                              num_workers=2, shuffle=True, pin_memory=True, drop_last=True)\n","    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], \n","                              num_workers=2, shuffle=False, pin_memory=True)\n","    \n","    return train_loader, valid_loader"]},{"cell_type":"markdown","metadata":{"id":"T-MsRHzBnZX8"},"source":["# Train"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2888,"status":"ok","timestamp":1662644179391,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"},"user_tz":-120},"id":"ih8htg1LoRZC","outputId":"18a28834-3813-472c-d9ca-afb724b1fb61"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"5dHag8YTn7jE","executionInfo":{"status":"ok","timestamp":1662644179393,"user_tz":-120,"elapsed":34,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}}},"outputs":[],"source":["train = pd.read_csv('/content/drive/MyDrive/wanlp/Datasets/NADI2022-Train/NADI2022-Train/Subtask1/NADI2022_Subtask1_TRAIN.tsv', sep='\\t', lineterminator='\\n')\n","valid = pd.read_csv('/content/drive/MyDrive/wanlp/Datasets/NADI2022-Train/NADI2022-Train/Subtask1/NADI2022_Subtask1_DEV.tsv', sep='\\t', lineterminator='\\n')"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1662644179397,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"},"user_tz":-120},"id":"ZLZqgBrJoyaa","outputId":"42996ff4-133e-4f25-d903-1575a122c878"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["         #1_id                                         #2_content #3_label\n","0  TRAIN_15711                         الطرحة في 61 النفر 10 جنيه    yemen\n","1   TRAIN_2257  كله ولا يطالبوا بارض فدك الله ياخدهم الله يعجل...  lebanon\n","2   TRAIN_5618  ' لما اطلقتو تلك التغريدة وقتها كان في واحد سع...  algeria\n","3   TRAIN_7284                                      بجكولى بى خير     iraq\n","4  TRAIN_15841                              يعم القمر براحه علينا    egypt"],"text/html":["\n","  <div id=\"df-2174e7d5-d4c0-46cc-b5b9-19295a7c2759\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>#1_id</th>\n","      <th>#2_content</th>\n","      <th>#3_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>TRAIN_15711</td>\n","      <td>الطرحة في 61 النفر 10 جنيه</td>\n","      <td>yemen</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>TRAIN_2257</td>\n","      <td>كله ولا يطالبوا بارض فدك الله ياخدهم الله يعجل...</td>\n","      <td>lebanon</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>TRAIN_5618</td>\n","      <td>' لما اطلقتو تلك التغريدة وقتها كان في واحد سع...</td>\n","      <td>algeria</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>TRAIN_7284</td>\n","      <td>بجكولى بى خير</td>\n","      <td>iraq</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>TRAIN_15841</td>\n","      <td>يعم القمر براحه علينا</td>\n","      <td>egypt</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2174e7d5-d4c0-46cc-b5b9-19295a7c2759')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2174e7d5-d4c0-46cc-b5b9-19295a7c2759 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2174e7d5-d4c0-46cc-b5b9-19295a7c2759');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":46}],"source":["train.head()"]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1662644179399,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"},"user_tz":-120},"id":"gB9D4twgo0bI","outputId":"5020e5dd-2d27-4e07-b0a2-6a6932d26f35"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["20398"]},"metadata":{},"execution_count":47}],"source":["len(train)"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1662644179401,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"},"user_tz":-120},"id":"qYdSpBt2o2HR","outputId":"5e13cb04-29b5-4307-f1d6-6d6b45054b6d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["18"]},"metadata":{},"execution_count":48}],"source":["train['#3_label'].nunique()"]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1662644179402,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"},"user_tz":-120},"id":"LREmIaISpGJF","outputId":"63ddd95c-f008-4865-e8fa-624c8172243d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["18"]},"metadata":{},"execution_count":49}],"source":["valid['#3_label'].nunique()"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"alyqp1JApKHQ","executionInfo":{"status":"ok","timestamp":1662644179402,"user_tz":-120,"elapsed":26,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}}},"outputs":[],"source":["# df_train=pd.concat([train,valid])"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"F8zmQRzrpDra","executionInfo":{"status":"ok","timestamp":1662644179403,"user_tz":-120,"elapsed":27,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}}},"outputs":[],"source":["num_classes=train['#3_label'].nunique()"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"0_HBDMdYn1Mi","executionInfo":{"status":"ok","timestamp":1662644179403,"user_tz":-120,"elapsed":26,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}}},"outputs":[],"source":["CONFIG = {\"seed\": 42,\n","          \"epochs\": 5,\n","          \"train_batch_size\": 16,\n","          \"valid_batch_size\": 64,\n","          \"max_length\": 512,\n","          \"learning_rate\": 5e-6,\n","          \"scheduler\": 'CosineAnnealingLR',\n","          \"min_lr\": 1e-8,\n","          \"T_max\": 500,\n","          \"weight_decay\": 1e-2,\n","          \"n_fold\": 5,\n","          \"n_accumulate\": 1,\n","          \"num_classes\": 18,\n","          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n","          }"]},{"cell_type":"code","execution_count":53,"metadata":{"executionInfo":{"elapsed":370,"status":"ok","timestamp":1662644179748,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"},"user_tz":-120},"id":"89VbI4nqq17L"},"outputs":[],"source":["tokenizer= AutoTokenizer.from_pretrained('CAMeL-Lab/bert-base-arabic-camelbert-da')\n"]},{"cell_type":"code","execution_count":54,"metadata":{"id":"E9sfjlh6qj6t","executionInfo":{"status":"ok","timestamp":1662644179751,"user_tz":-120,"elapsed":14,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}}},"outputs":[],"source":["train.reset_index(inplace=True)"]},{"cell_type":"code","execution_count":55,"metadata":{"id":"Dq0tSkUrntN4","executionInfo":{"status":"ok","timestamp":1662644179751,"user_tz":-120,"elapsed":12,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}}},"outputs":[],"source":["skf = StratifiedKFold(n_splits=CONFIG['n_fold'], shuffle=True, random_state=CONFIG['seed'])\n","\n","for fold, ( _, val_) in enumerate(skf.split(X=train, y=train['#3_label'])):\n","    train.loc[val_ , \"kfold\"] = int(fold)\n","    \n","train[\"kfold\"] = train[\"kfold\"].astype(int)"]},{"cell_type":"code","execution_count":56,"metadata":{"id":"wOMg6NEVsEkU","executionInfo":{"status":"ok","timestamp":1662644179752,"user_tz":-120,"elapsed":12,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}}},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","\n","le = LabelEncoder()\n","train['#3_label'] = le.fit_transform(train['#3_label'].values)"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"UJRzCkxxHJ_3","executionInfo":{"status":"ok","timestamp":1662644179752,"user_tz":-120,"elapsed":12,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}}},"outputs":[],"source":["valid['#3_label'] = le.transform(valid['#3_label'].values)"]},{"cell_type":"code","execution_count":58,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RdoS6yH5nwWG","outputId":"93931105-ef62-4f8c-90bd-bd2c93d13e79","executionInfo":{"status":"ok","timestamp":1662653812625,"user_tz":-120,"elapsed":2271990,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}}},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-mix were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["[INFO] Using GPU: Tesla T4\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1274/1274 [29:11<00:00,  1.37s/it, Epoch=1, LR=2.13e-6, Train_Loss=2.21]\n","100%|██████████| 77/77 [02:43<00:00,  2.12s/it, Epoch=1, LR=2.13e-6, Valid_Loss=1.98]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Accuracy: 0.421\n","Precision: 0.370\n","Recall: 0.421\n","F_score: 0.342\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.42      0.50      0.46       430\n","           1       0.00      0.00      0.00        52\n","           2       0.66      0.88      0.75      1041\n","           3       0.40      0.59      0.48       664\n","           4       0.00      0.00      0.00       104\n","           5       0.27      0.81      0.41       520\n","           6       0.00      0.00      0.00       105\n","           7       0.19      0.10      0.13       157\n","           8       0.28      0.07      0.11       314\n","           9       0.75      0.01      0.03       207\n","          10       0.17      0.03      0.05       355\n","          11       0.00      0.00      0.00       104\n","          12       0.00      0.00      0.00        52\n","          13       0.00      0.00      0.00        53\n","          14       0.19      0.13      0.16       278\n","          15       0.50      0.08      0.13       173\n","          16       0.37      0.04      0.08       157\n","          17       0.00      0.00      0.00       105\n","\n","    accuracy                           0.42      4871\n","   macro avg       0.23      0.18      0.15      4871\n","weighted avg       0.37      0.42      0.34      4871\n","\n","(0.42106343666598234, 0.3704552569266943, 0.42106343666598234, 0.34201877151484966)\n","0.15423573778957173\n","Validation Loss Improved (0 ---> 0.15423573778957173)\n","Model Saved\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1274/1274 [29:22<00:00,  1.38s/it, Epoch=2, LR=1.23e-7, Train_Loss=1.92]\n","100%|██████████| 77/77 [02:43<00:00,  2.12s/it, Epoch=2, LR=1.23e-7, Valid_Loss=1.94]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Accuracy: 0.438\n","Precision: 0.396\n","Recall: 0.438\n","F_score: 0.380\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.45      0.48      0.47       430\n","           1       0.00      0.00      0.00        52\n","           2       0.62      0.90      0.74      1041\n","           3       0.44      0.59      0.50       664\n","           4       0.00      0.00      0.00       104\n","           5       0.33      0.64      0.43       520\n","           6       0.00      0.00      0.00       105\n","           7       0.33      0.11      0.17       157\n","           8       0.27      0.13      0.18       314\n","           9       0.79      0.07      0.13       207\n","          10       0.25      0.18      0.21       355\n","          11       0.00      0.00      0.00       104\n","          12       0.00      0.00      0.00        52\n","          13       0.47      0.13      0.21        53\n","          14       0.18      0.19      0.19       278\n","          15       0.36      0.14      0.20       173\n","          16       0.29      0.23      0.26       157\n","          17       0.50      0.01      0.02       105\n","\n","    accuracy                           0.44      4871\n","   macro avg       0.29      0.21      0.21      4871\n","weighted avg       0.40      0.44      0.38      4871\n","\n","(0.4376924656128105, 0.39597268445738515, 0.4376924656128105, 0.37985744752618505)\n","0.20530633560886302\n","Validation Loss Improved (0.15423573778957173 ---> 0.20530633560886302)\n","Model Saved\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1274/1274 [29:24<00:00,  1.38s/it, Epoch=3, LR=3.6e-6, Train_Loss=1.82]\n","100%|██████████| 77/77 [02:43<00:00,  2.13s/it, Epoch=3, LR=3.6e-6, Valid_Loss=1.92]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Accuracy: 0.443\n","Precision: 0.410\n","Recall: 0.443\n","F_score: 0.390\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.41      0.51      0.46       430\n","           1       0.00      0.00      0.00        52\n","           2       0.66      0.88      0.75      1041\n","           3       0.44      0.59      0.50       664\n","           4       0.00      0.00      0.00       104\n","           5       0.34      0.63      0.44       520\n","           6       0.00      0.00      0.00       105\n","           7       0.25      0.12      0.16       157\n","           8       0.33      0.20      0.25       314\n","           9       0.78      0.09      0.16       207\n","          10       0.23      0.26      0.24       355\n","          11       0.17      0.01      0.02       104\n","          12       0.00      0.00      0.00        52\n","          13       0.37      0.25      0.30        53\n","          14       0.25      0.15      0.19       278\n","          15       0.25      0.06      0.09       173\n","          16       0.30      0.22      0.25       157\n","          17       0.86      0.06      0.11       105\n","\n","    accuracy                           0.44      4871\n","   macro avg       0.31      0.22      0.22      4871\n","weighted avg       0.41      0.44      0.39      4871\n","\n","(0.4430301786080887, 0.4102077975391038, 0.4430301786080887, 0.3904289226655872)\n","0.21760933045464606\n","Validation Loss Improved (0.20530633560886302 ---> 0.21760933045464606)\n","Model Saved\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1274/1274 [29:23<00:00,  1.38s/it, Epoch=4, LR=4.56e-6, Train_Loss=1.74]\n","100%|██████████| 77/77 [02:42<00:00,  2.12s/it, Epoch=4, LR=4.56e-6, Valid_Loss=1.93]\n"]},{"output_type":"stream","name":"stdout","text":["Accuracy: 0.448\n","Precision: 0.437\n","Recall: 0.448\n","F_score: 0.400\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.39      0.56      0.46       430\n","           1       0.00      0.00      0.00        52\n","           2       0.64      0.90      0.74      1041\n","           3       0.62      0.49      0.55       664\n","           4       0.00      0.00      0.00       104\n","           5       0.30      0.71      0.42       520\n","           6       0.10      0.01      0.02       105\n","           7       0.35      0.13      0.19       157\n","           8       0.37      0.18      0.24       314\n","           9       0.81      0.08      0.15       207\n","          10       0.25      0.26      0.26       355\n","          11       0.25      0.02      0.04       104\n","          12       0.00      0.00      0.00        52\n","          13       0.35      0.25      0.29        53\n","          14       0.26      0.17      0.21       278\n","          15       0.31      0.11      0.16       173\n","          16       0.33      0.21      0.26       157\n","          17       0.69      0.09      0.15       105\n","\n","    accuracy                           0.45      4871\n","   macro avg       0.33      0.23      0.23      4871\n","weighted avg       0.44      0.45      0.40      4871\n","\n","(0.4483678916033669, 0.43691800494447924, 0.4483678916033669, 0.39958380536651605)\n","0.22942222315068828\n","Validation Loss Improved (0.21760933045464606 ---> 0.22942222315068828)\n","Model Saved\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1274/1274 [29:20<00:00,  1.38s/it, Epoch=5, LR=7.97e-7, Train_Loss=1.65]\n","100%|██████████| 77/77 [02:40<00:00,  2.08s/it, Epoch=5, LR=7.97e-7, Valid_Loss=1.9]\n"]},{"output_type":"stream","name":"stdout","text":["Accuracy: 0.456\n","Precision: 0.426\n","Recall: 0.456\n","F_score: 0.412\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.47      0.47      0.47       430\n","           1       0.00      0.00      0.00        52\n","           2       0.64      0.89      0.75      1041\n","           3       0.50      0.59      0.54       664\n","           4       0.00      0.00      0.00       104\n","           5       0.34      0.61      0.43       520\n","           6       0.20      0.01      0.02       105\n","           7       0.41      0.12      0.19       157\n","           8       0.36      0.32      0.34       314\n","           9       0.67      0.09      0.15       207\n","          10       0.27      0.27      0.27       355\n","          11       0.11      0.04      0.06       104\n","          12       0.00      0.00      0.00        52\n","          13       0.33      0.25      0.28        53\n","          14       0.24      0.21      0.22       278\n","          15       0.34      0.14      0.20       173\n","          16       0.30      0.29      0.30       157\n","          17       0.59      0.10      0.16       105\n","\n","    accuracy                           0.46      4871\n","   macro avg       0.32      0.24      0.24      4871\n","weighted avg       0.43      0.46      0.41      4871\n","\n","(0.45637446109628416, 0.42550961549930005, 0.45637446109628416, 0.4123098324631866)\n","0.24304748911013363\n","Validation Loss Improved (0.22942222315068828 ---> 0.24304748911013363)\n","Model Saved\n","\n","Training complete in 2h 40m 29s\n","Best Loss: 0.2430\n","\n"]}],"source":["\n","    \n","# Create Dataloaders\n","train_loader, valid_loader = prepare_loaders(train,valid)\n","\n","model = NADIModel(pretrained_path='CAMeL-Lab/bert-base-arabic-camelbert-mix')\n","model.to(CONFIG['device'])\n","torch.cuda.empty_cache()\n","# model.load_state_dict(torch.load('/content/drive/MyDrive/wanlp/NADI/Saved_Models/Camel_bert/Loss-Fold-4.bin'))\n","\n","# Define Optimizer and Scheduler\n","optimizer = AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n","scheduler = fetch_scheduler(optimizer)\n","\n","model, history = run_training(model, optimizer, scheduler,\n","                              device=CONFIG['device'],\n","                              num_epochs=CONFIG['epochs'],\n","                              fold=1)\n","\n","\n","del model, history, train_loader, valid_loader\n","_ = gc.collect()\n","print()"]},{"cell_type":"code","execution_count":58,"metadata":{"id":"4TZwIpgR2NNg","executionInfo":{"status":"ok","timestamp":1662653812627,"user_tz":-120,"elapsed":3,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}}},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}