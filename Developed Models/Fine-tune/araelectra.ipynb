{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27039,
     "status": "ok",
     "timestamp": 1661450521152,
     "user": {
      "displayName": "Reem Elsayed",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "vMmciFBRyRFe",
    "outputId": "ab78d585-e22a-4670-b760-d8cc0cf60394"
   },
   "outputs": [],
   "source": [
    "# !pip install sentencepiece\n",
    "# !pip  install transformers\n",
    "# !pip install pytorch-ignite\n",
    "# !pip install datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 6960,
     "status": "ok",
     "timestamp": 1661450528107,
     "user": {
      "displayName": "Reem Elsayed",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "AtS4CD_krp3d"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import AutoModel\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "# Any results you write to the current directory are saved as output.\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "from transformers import BertTokenizer,BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from argparse import ArgumentParser\n",
    "# from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "# from ignite.metrics import Accuracy, Loss\n",
    "# from ignite.engine.engine import Engine, State, Events\n",
    "# from ignite.handlers import EarlyStopping\n",
    "# from ignite.contrib.handlers import TensorboardLogger, ProgressBar\n",
    "# from ignite.utils import convert_tensor\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')\n",
    "import gc\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "\n",
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Pytorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Utils\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Sklearn Imports\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW\n",
    "import random\n",
    "import os\n",
    "from urllib import request\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, BertTokenizer\n",
    "from transformers.data.processors import SingleSentenceClassificationProcessor\n",
    "from transformers import Trainer , TrainingArguments\n",
    "from transformers.trainer_utils import EvaluationStrategy\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARjNBDtgwfVJ"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1661450528108,
     "user": {
      "displayName": "Reem Elsayed",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "lLRNRjusoELI"
   },
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length):\n",
    "        self.df = df\n",
    "        self.max_len = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text = df['#2_content'].values\n",
    "        self.label=df['#3_label'].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.text[index]\n",
    "        # summary = self.summary[index]\n",
    "        inputs_text = self.tokenizer.encode_plus(\n",
    "                                text,\n",
    "                                truncation=True,\n",
    "                                add_special_tokens=True,\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length'\n",
    "                            )\n",
    "        \n",
    "                            \n",
    "        target = self.label[index]\n",
    "        \n",
    "        text_ids = inputs_text['input_ids']\n",
    "        text_mask = inputs_text['attention_mask']\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        return {\n",
    "            \n",
    "            'text_ids': torch.tensor(text_ids, dtype=torch.long),\n",
    "            'text_mask': torch.tensor(text_mask, dtype=torch.long),\n",
    "            'target': torch.tensor(target, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1661450528108,
     "user": {
      "displayName": "Reem Elsayed",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "0ZOanFa2wz3L"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# import Dataset\n",
    "# import os\n",
    "# import numpy as np\n",
    "# from emoji import UNICODE_EMOJI\n",
    "# import TweetNormalizer\n",
    "# import re\n",
    "# import text_normalization\n",
    "\n",
    "\n",
    "# dic = {\n",
    "#       \"egypt\": 'المصرية',\n",
    "# \t  \"nile\": 'المصرية',\n",
    "# \t  \"msa\": \"اللغة العربية الفصحى\",\n",
    "# \t  \"magreb\": \"المغربية\",\n",
    "# \t  \"gulf\": \"الخليجية\",\n",
    "# \t  \"levant\": \"الشامية\"\n",
    "# }\n",
    "\n",
    "# def is_emoji(s):\n",
    "#     return s in UNICODE_EMOJI\n",
    "\n",
    "# # add space near your emoji\n",
    "# def add_space(text):\n",
    "#     return ''.join(' ' + char if is_emoji(char) else char for char in text).strip()\n",
    "\n",
    "# def preprocess(text, lang='ar'):\n",
    "#     if lang == 'ar':\n",
    "#         sent = add_space(text)\n",
    "#         sent = re.sub(r'(?:@[\\w_]+)', \"user\", sent)\n",
    "#         sent = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', \"url\", sent)\n",
    "#         sent = sent.replace('_', ' ')\n",
    "#         sent = sent.replace('#', ' ')\n",
    "#     else:\n",
    "#         sent = add_space(text)\n",
    "#         sent = re.sub(r'(?:@[\\w_]+)', \"@user\", sent)\n",
    "#         sent = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', \"http\", sent)\n",
    "#         sent = sent.replace('_', ' ')\n",
    "#         sent = sent.replace('#', ' ')\n",
    "\n",
    "#     return sent\n",
    "\n",
    "# def prepare_text(df, col='tweet'):\n",
    "#     if col == 'tweet':\n",
    "#         df['dialect'] = df['dialect'].map(dic)   \n",
    "#     for i in range(df.shape[0]):\n",
    "#         df.loc[i, col] = df.loc[i, 'dialect'] + ' [SEP] ' + df.loc[i, col]\n",
    "\n",
    "\n",
    "#     return df\n",
    "\n",
    "# def augment_data(df_train,text_col,label_col):\n",
    "#     df_aug = pd.DataFrame(columns=[text_col, label_col)\n",
    "#     dic_dup = {1: 3,\n",
    "#                0: 1\n",
    "#                }\n",
    "#     for i in range(df_train.shape[0]):\n",
    "#         current = df_train.iloc[i]\n",
    "#         text = current[text_col]\n",
    "#         label_cat = current[label_col]\n",
    "\n",
    "#         aug_ratio = dic_dup[label_cat]\n",
    "#         for k in range(aug_ratio):\n",
    "#             tokens = text.split(' ')\n",
    "#             l = len(tokens)\n",
    "#             n = int(0.1 * l)\n",
    "#             indices = np.random.choice(l, n, replace=False)\n",
    "#             for j in range(len(indices)):\n",
    "#                 tokens[indices[j]] = '[MASK]'\n",
    "#             new_text = ' '.join(tokens)\n",
    "#             entry = {text_col: new_text, label_col: label_cat}\n",
    "#             df_aug = df_aug.append(entry, ignore_index=True)\n",
    "#     df_aug.drop_duplicates(subset=[text_col], keep='first', inplace=True)\n",
    "#     df = pd.concat([df_train,df_aug])\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1661450528109,
     "user": {
      "displayName": "Reem Elsayed",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "QcOa9agTwhIg"
   },
   "outputs": [],
   "source": [
    "# '''\n",
    "# Created by: Mohamed Salem Elhady  \n",
    "# Email: mohamed.elaraby@alumni.ubc.ca\n",
    "# Text Normalization: V1 \n",
    "# '''\n",
    "# import sys\n",
    "# import re\n",
    "# import emojis\n",
    "# from emoji import UNICODE_EMOJI\n",
    "# #sys.setdefaultencoding('utf-8')\n",
    "# ##########################Clean Text Data #######################################\n",
    "# ########################Global Variable Declaration##############################\n",
    "# list_seeds = ['سبحان الله', 'الله أكبر', 'اللهم', 'بسم الله', 'يا رب', 'العضيم', 'سبحان', 'يارب', 'قران', 'quran',\n",
    "#               'حديث', 'hadith', 'صلاه_الفجر', '﴾', 'ﷺ', 'صحيح البخاري', 'صحيح مسلم', 'يآرب', 'سورة']\n",
    "# MaxWordPerTweet=7\n",
    "# #################################################################################\n",
    "# def is_emoji(s):\n",
    "#     return s in UNICODE_EMOJI\n",
    "\n",
    "# # add space near your emoji\n",
    "# def add_space(text):\n",
    "#     return ''.join(' ' + char if is_emoji(char) else char for char in text).strip()\n",
    "\n",
    "# def clean(sent):\n",
    "#     \"\"\"clean data from any English char, emoticons, underscore, and repeated > 2\n",
    "#     str -> str\"\"\"\n",
    "#     p1 = re.compile('\\W')\n",
    "#     p2 = re.compile('\\s+')\n",
    "#     sent = re.sub(r\"http\\S+\", \"\", sent)\n",
    "#     sent = ReplaceThreeOrMore(sent)\n",
    "#     sent = remove_unicode_diac(sent)\n",
    "#     sent = sent.replace('_', ' ')\n",
    "#     sent = re.sub(r'[A-Za-z0-9]', r'', sent)\n",
    "#     sent = re.sub(p1, ' ', sent)\n",
    "#     sent = re.sub(p2, ' ', sent)\n",
    "#     return sent\n",
    "\n",
    "# def tokenize_emojis(tweet):\n",
    "#     return list(emojis.get(tweet))\n",
    "\n",
    "# def replace_emoji(sent):\n",
    "#     emoji_pattern = re.compile(\"[\"\n",
    "#                                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "#                                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "#                                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "#                                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "#                                \"]+\", flags=re.UNICODE)\n",
    "#     return emoji_pattern.sub(r'[MASK]', sent)\n",
    "\n",
    "# def preprocess(tweet):\n",
    "#     tweet = add_space(tweet)\n",
    "#     emos = tokenize_emojis(tweet)\n",
    "#     sent = remove_unicode_diac(tweet)\n",
    "#     sent = re.sub(r'(?:@[\\w_]+)', \"user\", sent)\n",
    "#     sent = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', \"url\", sent)\n",
    "#     sent = sent.replace('_', ' ')\n",
    "#     sent = sent.replace('#', ' ')\n",
    "#     if len(emos) > 0:\n",
    "#         sent = sent + ' [SEP] '  + ' '.join(emos)\n",
    "#     #    #sent = sent + ' [SEP] ' + clean_unicode(tweet) + ' [SEP] ' + ' '.join(emos)\n",
    "\n",
    "#     #else:\n",
    "#     #    sent = sent + ' [SEP] ' + clean_unicode(tweet)\n",
    "#     return sent\n",
    "\n",
    "# def preprocess_last(tweet, k=0):\n",
    "#     emos = tokenize_emojis(tweet)\n",
    "#     sent = remove_unicode_diac(tweet)\n",
    "#     sent = re.sub(r'(?:@[\\w_]+)', \"\", sent)\n",
    "#     sent = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', \"\", sent)\n",
    "#     sent = sent.replace('_', ' ')\n",
    "#     sent = sent.replace('#', ' ')\n",
    "#     if k == 0:\n",
    "#         sent = sent\n",
    "#     elif k ==1 :\n",
    "#         sent = sent + ' [SEP] ' + ' '.join(emos)\n",
    "#     elif k==2 :\n",
    "#         sent = sent + ' [SEP] ' + clean_unicode(tweet) + ' [SEP] ' + ' '.join(emos)\n",
    "#     elif k == 3:\n",
    "#         sent = sent + ' [SEP] ' + clean_unicode(tweet)\n",
    "#     else:\n",
    "#         sent = replace_emoji(sent)\n",
    "#         sent = sent + ' [SEP] ' + clean_unicode(tweet) + ' [SEP] ' + ' '.join(emos)\n",
    "\n",
    "#     return sent\n",
    "\n",
    "\n",
    "# def normalize(sent):\n",
    "#     \"\"\"clean data from any English char, emoticons, underscore, and repeated > 2\n",
    "#     str -> str\"\"\"\n",
    "#     sent = re.sub(r'(?:@[\\w_]+)', \"user\", sent)\n",
    "#     sent = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', \"url\", sent)\n",
    "#     #sent = re.sub(r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", \"hashtag\", sent)\n",
    "#     sent = ReplaceThreeOrMore(sent)\n",
    "#     sent = remove_unicode_diac(sent)\n",
    "#     sent = sent.replace('_', ' ')\n",
    "#     return sent\n",
    "\n",
    "# def ReplaceThreeOrMore(s):\n",
    "#     # pattern to look for three or more repetitions of any character, including\n",
    "#     # newlines.\n",
    "#     pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL)\n",
    "#     return pattern.sub(r\"\\1\\1\", s)\n",
    "# def norm_alif(text):\n",
    "#     text = text.replace(u\"\\u0625\", u\"\\u0627\")  # HAMZA below, with LETTER ALEF\n",
    "#     #text = text.replace(u\"\\u0621\", u\"\\u0627\")  # HAMZA, with LETTER ALEF\n",
    "#     text = text.replace(u\"\\u0622\", u\"\\u0627\")  # ALEF WITH MADDA ABOVE, with LETTER ALEF\n",
    "#     text = text.replace(u\"\\u0623\", u\"\\u0627\")  # ALEF WITH HAMZA ABOVE, with LETTER ALEF\n",
    "#     return text\n",
    "# def remove_unicode_diac(text):\n",
    "#     \"\"\"Takes Arabic in utf-8 and returns same text without diac\"\"\"\n",
    "#     # Replace diacritics with nothing\n",
    "#     text = text.replace(u\"\\u064B\", \"\")  # fatHatayn\n",
    "#     text = text.replace(u\"\\u064C\", \"\")  # Dammatayn\n",
    "#     text = text.replace(u\"\\u064D\", \"\")  # kasratayn\n",
    "#     text = text.replace(u\"\\u064E\", \"\")  # fatHa\n",
    "#     text = text.replace(u\"\\u064F\", \"\")  # Damma\n",
    "#     text = text.replace(u\"\\u0650\", \"\")  # kasra\n",
    "#     text = text.replace(u\"\\u0651\", \"\")  # shaddah\n",
    "#     text = text.replace(u\"\\u0652\", \"\")  # sukuun\n",
    "#     text = text.replace(u\"\\u0670\", \"`\")  # dagger 'alif\n",
    "#     return text\n",
    "# def norm_taa(text):\n",
    "#     text=text.replace(u\"\\u0629\", u\"\\u0647\") # taa' marbuuTa, with haa'\n",
    "#     #text=text.replace(u\"\\u064A\", u\"\\u0649\") # yaa' with 'alif maqSuura\n",
    "#     return text\n",
    "# def norm_yaa(text):\n",
    "#     if len(text)!=0:\n",
    "#         if text[-1] == u\"\\u064A\":\n",
    "#             text = text[:-1] + text[-1].replace(u\"\\u064A\", u\"\\u0649\")  # yaa' with 'alif maqSuura\n",
    "#     return text\n",
    "\n",
    "# def NormForWord2Vec(text):\n",
    "#     text=norm_taa(text)\n",
    "#     text=norm_yaa(text)\n",
    "#     text=norm_alif(text)\n",
    "#     return text\n",
    "\n",
    "# def remove_nonunicode2(Tweet):\n",
    "#     ## defining set of unicode ##\n",
    "#     #u\"\"\n",
    "#     #Tweet=Tweet.decode(\"utf-8\")\n",
    "#     UniLex={ ## This is list of all arabic unicode characters in addition to space (to separate words)\n",
    "#             u\"\\u0622\",\n",
    "#             u\"\\u0626\",\n",
    "#             u\"\\u0628\",\n",
    "#             u\"\\u062a\",\n",
    "#             u\"\\u062c\",\n",
    "#             u\"\\u06af\",\n",
    "#             u\"\\u062e\",\n",
    "#             u\"\\u0630\",\n",
    "#             u\"\\u0632\",\n",
    "#             u\"\\u0634\",\n",
    "#             u\"\\u0636\",\n",
    "#             u\"\\u0638\",\n",
    "#             u\"\\u063a\",\n",
    "#             u\"\\u0640\",\n",
    "#             u\"\\u0642\",\n",
    "#             u\"\\u0644\",\n",
    "#             u\"\\u0646\",\n",
    "#             u\"\\u0648\",\n",
    "#             u\"\\u064a\",\n",
    "#             u\"\\u0670\",\n",
    "#             u\"\\u067e\",\n",
    "#             u\"\\u0686\",\n",
    "#             u\"\\u0621\",\n",
    "#             u\"\\u0623\",\n",
    "#             u\"\\u0625\",\n",
    "#             u\"\\u06a4\",\n",
    "#             u\"\\u0627\",\n",
    "#             u\"\\u0629\",\n",
    "#             u\"\\u062b\",\n",
    "#             u\"\\u062d\",\n",
    "#             u\"\\u062f\",\n",
    "#             u\"\\u0631\",\n",
    "#             u\"\\u0633\",\n",
    "#             u\"\\u0635\",\n",
    "#             u\"\\u0637\",\n",
    "#             u\"\\u0639\",\n",
    "#             u\"\\u0641\",\n",
    "#             u\"\\u0643\",\n",
    "#             u\"\\u0645\",\n",
    "#             u\"\\u0647\",\n",
    "#             u\"\\u0649\",\n",
    "#             u\"\\u0671\",\n",
    "#             ' ',\n",
    "#             '\\n'\n",
    "#           }\n",
    "#     fin_tweet=\"\"\n",
    "#     for c in Tweet:\n",
    "#         if c in UniLex:\n",
    "#            fin_tweet=fin_tweet+c\n",
    "#     return fin_tweet\n",
    "\n",
    "# ###### Heuristics Calculations ######\n",
    "# def diac_counter(text):\n",
    "#     #text=text.decode(\"utf-8\")\n",
    "#     diac = [u\"\\u064B\",u\"\\u064C\", u\"\\u064D\", u\"\\u064E\", u\"\\u064F\", u\"\\u0650\", u\"\\u0651\", u\"\\u0652\", u\"\\u0670\"]\n",
    "#     diac_count=0\n",
    "#     for d in diac:\n",
    "#         diac_count+=text.count(d)\n",
    "# #         if d in text:\n",
    "# #             print(d)\n",
    "# #             diac_count+=1\n",
    "#     return diac_count\n",
    "# def check_seed(list_seeds, text):\n",
    "#     \"\"\n",
    "#     for word in list_seeds:\n",
    "#         text = text.lower()\n",
    "#         if word.decode(\"utf-8\") in text:\n",
    "#             return True\n",
    "#     return False\n",
    "# def EnglishCount(text):\n",
    "#     printable = ['e', 'a', 'o', 't', 'i']\n",
    "#     count = 0\n",
    "#     for ch in printable:\n",
    "#         count += text.count(ch.lower())\n",
    "#     return count\n",
    "# ########################################\n",
    "\n",
    "\n",
    "\n",
    "# def eliminate_single_char_words(Tweet):\n",
    "#     parts = Tweet.split(\" \")\n",
    "#     cleaned_line_parts = []\n",
    "#     for P in parts:\n",
    "#         if len(P) != 1:\n",
    "#             cleaned_line_parts.append(P)\n",
    "#     cleaned_line = ' '.join(cleaned_line_parts)\n",
    "#     return cleaned_line\n",
    "# def clean_unicode(Tweet):\n",
    "#     tweet=normalize(Tweet.strip(\"\\n\"))\n",
    "#     if len(tweet) !=0:\n",
    "#         sentence = []\n",
    "#         for word in tweet.split(\" \"):\n",
    "#             word = remove_unicode_diac(word)\n",
    "#             word = norm_alif(word)\n",
    "#             word = norm_taa(word)\n",
    "#             word = norm_yaa(word)\n",
    "#             word = normalize(word)\n",
    "#             sentence.append(word)\n",
    "#         tweet = ' '.join(sentence)\n",
    "#         tweet =remove_nonunicode2(tweet)\n",
    "#         tweet =eliminate_single_char_words(tweet)\n",
    "#     return tweet\n",
    "\n",
    "# def clean_unicode2(Tweet):\n",
    "#     KeepUniOnly(Tweet)\n",
    "#     tweet=normalize(Tweet.strip(\"\\n\"))\n",
    "#     if len(tweet) !=0:\n",
    "#         sentence = []\n",
    "#         for word in tweet.split(\" \"):\n",
    "#             word = remove_unicode_diac(word)\n",
    "#             word = normalize(word)\n",
    "#             sentence.append(word)\n",
    "#         tweet = ' '.join(sentence)\n",
    "#         tweet =remove_nonunicode2(tweet)\n",
    "#         tweet =eliminate_single_char_words(tweet)\n",
    "#     return tweet\n",
    "\n",
    "# def NormCorpusFinal(Tweet):\n",
    "#     tweet=KeepUniOnly(Tweet)\n",
    "#     tweet=NormForWord2Vec(tweet)\n",
    "#     return tweet\n",
    "\n",
    "# def KeepUniOnly(Tweet):## this one is without normalization\n",
    "#     tweet=Tweet.replace(\"# \",\" \")\n",
    "#     tweet=tweet.replace(\"#\",\" \")\n",
    "#     tweet=tweet.replace(\"_\",\" \")\n",
    "#     tweet=tweet.replace(u\"\\u0657\",\" \")\n",
    "#     tweet=tweet.replace(\"\\n\",\" \")\n",
    "#     tweet=remove_nonunicode2(tweet)\n",
    "#     tweet=eliminate_single_char_words(tweet)\n",
    "#     tweet=ReplaceThreeOrMore(tweet)\n",
    "#     return tweet\n",
    "\n",
    "# def get_charset(rawtext):\n",
    "#     chars = sorted(list(set(rawtext)))\n",
    "#     return chars\n",
    "\n",
    "# def DialectChecker(text):\n",
    "#     ##Based on Hueristics done by Hassan\n",
    "#     if (diac_counter(text)>5 or check_seed(list_seeds,text) or EnglishCount(text)>4 or \"<URL>\"  in text\n",
    "#         or text.count('#') >2 or '\"'  in text or text.count('@') or \"\\\"RT\" in text or len(text.split(\" \")) <7):\n",
    "#         return False\n",
    "#     else:\n",
    "#         return True\n",
    "\n",
    "# ###############################################################\n",
    "# '''\n",
    "# Fread=open(\"Egypt_portion.txt\",'r')\n",
    "# Fwriter=open(\"Egypt_portion_norm.txt\",'w')\n",
    "# for line in Fread:\n",
    "#     cleaned_line=clean_unicode_for_w2v(line)\n",
    "#     Fwriter.write(str(cleaned_line))\n",
    "# Fwriter.close()\n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVcFbe1Bvcqh"
   },
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1661450528109,
     "user": {
      "displayName": "Reem Elsayed",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "RGw6mbs0uIlN"
   },
   "outputs": [],
   "source": [
    "class F1_Loss(nn.Module):\n",
    "    '''Calculate F1 score. Can work with gpu tensors\n",
    "    \n",
    "    The original implmentation is written by Michal Haltuf on Kaggle.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        `ndim` == 1. epsilon <= val <= 1\n",
    "    \n",
    "    Reference\n",
    "    ---------\n",
    "    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\n",
    "    - http://www.ryanzhang.info/python/writing-your-own-loss-function-module-for-pytorch/\n",
    "    '''\n",
    "    def __init__(self, epsilon=1e-7):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def forward(self, y_pred, y_true,):\n",
    "        # assert y_pred.ndim == 2\n",
    "        # assert y_true.ndim == 1\n",
    "        # print(y_pred.shape)\n",
    "        # print(y_true.shape)\n",
    "        # y_pred[y_pred<0.5]=0\n",
    "        # y_pred[y_pred>=0.5]=0\n",
    "\n",
    "\n",
    "        \n",
    "        y_true_one_hot = F.one_hot(y_true.to(torch.int64), 3).to(torch.float32)\n",
    "        # y_pred_one_hot = F.one_hot(y_pred.to(torch.int64), 2).to(torch.float32)\n",
    "        \n",
    "        tp = (y_true_one_hot * y_pred).sum(dim=0).to(torch.float32)\n",
    "        tn = ((1 - y_true_one_hot) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "        fp = ((1 - y_true_one_hot) * y_pred).sum(dim=0).to(torch.float32)\n",
    "        fn = (y_true_one_hot * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "\n",
    "        precision = tp / (tp + fp + self.epsilon)\n",
    "        recall = tp / (tp + fn + self.epsilon)\n",
    "\n",
    "        f1 = 2* (precision*recall) / (precision + recall + self.epsilon)\n",
    "        f1 = f1.clamp(min=self.epsilon, max=1-self.epsilon)\n",
    "        f1=f1.detach()\n",
    "        # print(f1.shape)\n",
    "        # y_pred=y_pred.reshape((y_pred.shape[0], 1))\n",
    "        # y_true=y_true.reshape((y_true.shape[0], 1))\n",
    "\n",
    "        # p1=y_true*(math.log(sigmoid(y_pred)))*(1-f1)[1]\n",
    "        # p0=(1-y_true)*math.log(1-sigmoid(y_pred))*(1-f1)[0]\n",
    "\n",
    "\n",
    "        # y_true_one_hot = F.one_hot(y_true.to(torch.int64), 2)\n",
    "        # print(y_pred)\n",
    "        # print(y_true_one_hot)\n",
    "        CE =torch.nn.CrossEntropyLoss(weight=( 1 - f1))(y_pred, y_true)\n",
    "        # loss = ( 1 - f1)  * CE\n",
    "        return  CE.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1661450528109,
     "user": {
      "displayName": "Reem Elsayed",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "Pdd02bGivpr5"
   },
   "outputs": [],
   "source": [
    "class Recall_Loss(nn.Module):\n",
    "    '''Calculate Recall score. Can work with gpu tensors\n",
    "    \n",
    "    The original implmentation is written by Michal Haltuf on Kaggle.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        `ndim` == 1. epsilon <= val <= 1\n",
    "    \n",
    "    Reference\n",
    "    ---------\n",
    "    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\n",
    "    - http://www.ryanzhang.info/python/writing-your-own-loss-function-module-for-pytorch/\n",
    "    '''\n",
    "    def __init__(self, epsilon=1e-7):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def forward(self, y_pred, y_true,):\n",
    "        # assert y_pred.ndim == 2\n",
    "        # assert y_true.ndim == 1\n",
    "        # print(y_pred.shape)\n",
    "        # print(y_true.shape)\n",
    "        # y_pred[y_pred<0.5]=0\n",
    "        # y_pred[y_pred>=0.5]=0\n",
    "\n",
    "\n",
    "        \n",
    "        y_true_one_hot = F.one_hot(y_true.to(torch.int64), 3).to(torch.float32)\n",
    "        # y_pred_one_hot = F.one_hot(y_pred.to(torch.int64), 2).to(torch.float32)\n",
    "        \n",
    "        tp = (y_true_one_hot * y_pred).sum(dim=0).to(torch.float32)\n",
    "        tn = ((1 - y_true_one_hot) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "        fp = ((1 - y_true_one_hot) * y_pred).sum(dim=0).to(torch.float32)\n",
    "        fn = (y_true_one_hot * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "\n",
    "        precision = tp / (tp + fp + self.epsilon)\n",
    "        recall = tp / (tp + fn + self.epsilon)\n",
    "\n",
    "        # f1 = 2* (precision*recall) / (precision + recall + self.epsilon)\n",
    "        # f1 = f1.clamp(min=self.epsilon, max=1-self.epsilon)\n",
    "        # f1=f1.detach()\n",
    "        # print(f1.shape)\n",
    "        # y_pred=y_pred.reshape((y_pred.shape[0], 1))\n",
    "        # y_true=y_true.reshape((y_true.shape[0], 1))\n",
    "\n",
    "        # p1=y_true*(math.log(sigmoid(y_pred)))*(1-f1)[1]\n",
    "        # p0=(1-y_true)*math.log(1-sigmoid(y_pred))*(1-f1)[0]\n",
    "\n",
    "\n",
    "        # y_true_one_hot = F.one_hot(y_true.to(torch.int64), 2)\n",
    "        # print(y_pred)\n",
    "        # print(y_true_one_hot)\n",
    "        recall=recall.detach()\n",
    "        CE =torch.nn.CrossEntropyLoss(weight=( 1 - recall))(y_pred, y_true_one_hot)\n",
    "        # loss = ( 1 - f1)  * CE\n",
    "        return  CE.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1661450528110,
     "user": {
      "displayName": "Reem Elsayed",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "zKy23cw1wHB9"
   },
   "outputs": [],
   "source": [
    "## Based on https://github.com/AbdelkaderMH/iSarcasmEval/blob/8f28f24ebfb641415a604329ed859506ae687148/focal_loss.py\n",
    "class BinaryFocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a implementation of Focal Loss with smooth label cross entropy supported which is proposed in\n",
    "    'Focal Loss for Dense Object Detection. (https://arxiv.org/abs/1708.02002)'\n",
    "        Focal_Loss= -1*alpha*(1-pt)*log(pt)\n",
    "    :param alpha: (tensor) 3D or 4D the scalar factor for this criterion\n",
    "    :param gamma: (float,double) gamma > 0 reduces the relative loss for well-classified examples (p>0.5) putting more\n",
    "                    focus on hard misclassified example\n",
    "    :param reduction: `none`|`mean`|`sum`\n",
    "    :param **kwargs\n",
    "        balance_index: (int) balance class index, should be specific when alpha is float\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=3, gamma=2, ignore_index=None, reduction='mean', **kwargs):\n",
    "        super(BinaryFocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.smooth = 1e-6  # set '1e-4' when train with FP16\n",
    "        self.ignore_index = ignore_index\n",
    "        self.reduction = reduction\n",
    "\n",
    "        assert self.reduction in ['none', 'mean', 'sum']\n",
    "\n",
    "        # if self.alpha is None:\n",
    "        #     self.alpha = torch.ones(2)\n",
    "        # elif isinstance(self.alpha, (list, np.ndarray)):\n",
    "        #     self.alpha = np.asarray(self.alpha)\n",
    "        #     self.alpha = np.reshape(self.alpha, (2))\n",
    "        #     assert self.alpha.shape[0] == 2, \\\n",
    "        #         'the `alpha` shape is not match the number of class'\n",
    "        # elif isinstance(self.alpha, (float, int)):\n",
    "        #     self.alpha = np.asarray([self.alpha, 1.0 - self.alpha], dtype=np.float).view(2)\n",
    "\n",
    "        # else:\n",
    "        #     raise TypeError('{} not supported'.format(type(self.alpha)))\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        prob = torch.sigmoid(output)\n",
    "        prob = torch.clamp(prob, self.smooth, 1.0 - self.smooth)\n",
    "\n",
    "        valid_mask = None\n",
    "        if self.ignore_index is not None:\n",
    "            valid_mask = (target != self.ignore_index).float()\n",
    "\n",
    "        pos_mask = (target == 1).float()\n",
    "        neg_mask = (target == 0).float()\n",
    "        if valid_mask is not None:\n",
    "            pos_mask = pos_mask * valid_mask\n",
    "            neg_mask = neg_mask * valid_mask\n",
    "\n",
    "        pos_weight = (pos_mask * torch.pow(1 - prob, self.gamma)).detach()\n",
    "        pos_loss = -pos_weight * torch.log(prob)  # / (torch.sum(pos_weight) + 1e-4)\n",
    "\n",
    "        neg_weight = (neg_mask * torch.pow(prob, self.gamma)).detach()\n",
    "        neg_loss = -self.alpha * neg_weight * F.logsigmoid(-output)  # / (torch.sum(neg_weight) + 1e-4)\n",
    "        loss = pos_loss + neg_loss\n",
    "        loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "class FocalLoss_Ori(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a implementation of Focal Loss with smooth label cross entropy supported which is proposed in\n",
    "    'Focal Loss for Dense Object Detection. (https://arxiv.org/abs/1708.02002)'\n",
    "    Focal_Loss= -1*alpha*((1-pt)**gamma)*log(pt)\n",
    "    Args:\n",
    "        num_class: number of classes\n",
    "        alpha: class balance factor\n",
    "        gamma:\n",
    "        ignore_index:\n",
    "        reduction:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_class, alpha=None, gamma=2, ignore_index=None, reduction='mean'):\n",
    "        super(FocalLoss_Ori, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.smooth = 1e-4\n",
    "        self.ignore_index = ignore_index\n",
    "        self.alpha = alpha\n",
    "        if alpha is None:\n",
    "            self.alpha = torch.ones(num_class, )\n",
    "        elif isinstance(alpha, (int, float)):\n",
    "            self.alpha = torch.as_tensor([alpha] * num_class)\n",
    "        elif isinstance(alpha, (list, np.ndarray)):\n",
    "            self.alpha = torch.as_tensor(alpha)\n",
    "        if self.alpha.shape[0] != num_class:\n",
    "            raise RuntimeError('the length not equal to number of class')\n",
    "\n",
    "        # if isinstance(self.alpha, (list, tuple, np.ndarray)):\n",
    "        #     assert len(self.alpha) == self.num_class\n",
    "        #     self.alpha = torch.Tensor(list(self.alpha))\n",
    "        # elif isinstance(self.alpha, (float, int)):\n",
    "        #     assert 0 < self.alpha < 1.0, 'alpha should be in `(0,1)`)'\n",
    "        #     assert balance_index > -1\n",
    "        #     alpha = torch.ones((self.num_class))\n",
    "        #     alpha *= 1 - self.alpha\n",
    "        #     alpha[balance_index] = self.alpha\n",
    "        #     self.alpha = alpha\n",
    "        # elif isinstance(self.alpha, torch.Tensor):\n",
    "        #     self.alpha = self.alpha\n",
    "        # else:\n",
    "        #     raise TypeError('Not support alpha type, expect `int|float|list|tuple|torch.Tensor`')\n",
    "\n",
    "    def forward(self, logit, target):\n",
    "        # assert isinstance(self.alpha,torch.Tensor)\\\n",
    "        N, C = logit.shape[:2]\n",
    "        alpha = self.alpha.to(logit.device)\n",
    "        prob = F.softmax(logit, dim=1)\n",
    "        if prob.dim() > 2:\n",
    "            # N,C,d1,d2 -> N,C,m (m=d1*d2*...)\n",
    "            prob = prob.view(N, C, -1)\n",
    "            prob = prob.transpose(1, 2).contiguous()  # [N,C,d1*d2..] -> [N,d1*d2..,C]\n",
    "            prob = prob.view(-1, prob.size(-1))  # [N,d1*d2..,C]-> [N*d1*d2..,C]\n",
    "        ori_shp = target.shape\n",
    "        target = target.view(-1, 1)  # [N,d1,d2,...]->[N*d1*d2*...,1]\n",
    "        valid_mask = None\n",
    "        if self.ignore_index is not None:\n",
    "            valid_mask = target != self.ignore_index\n",
    "            target = target * valid_mask\n",
    "\n",
    "        # ----------memory saving way--------\n",
    "        prob = prob.gather(1, target).view(-1) + self.smooth  # avoid nan\n",
    "        logpt = torch.log(prob)\n",
    "        # alpha_class = alpha.gather(0, target.view(-1))\n",
    "        alpha_class = alpha[target.squeeze().long()]\n",
    "        class_weight = -alpha_class * torch.pow(torch.sub(1.0, prob), self.gamma)\n",
    "        loss = class_weight * logpt\n",
    "        if valid_mask is not None:\n",
    "            loss = loss * valid_mask.squeeze()\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "            if valid_mask is not None:\n",
    "                loss = loss.sum() / valid_mask.sum()\n",
    "        elif self.reduction == 'none':\n",
    "            loss = loss.view(ori_shp)\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "def weighted_binary_cross_entropy(input, targets, pos_weight, weight=None, size_average=True, reduce=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sigmoid_x: predicted probability of size [N,C], N sample and C Class. Eg. Must be in range of [0,1], i.e. Output from Sigmoid.\n",
    "        targets: true value, one-hot-like vector of size [N,C]\n",
    "        pos_weight: Weight for postive sample\n",
    "    \"\"\"\n",
    "    sigmoid_x = torch.sigmoid(input)\n",
    "    if not (targets.size() == sigmoid_x.size()):\n",
    "        raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(targets.size(), sigmoid_x.size()))\n",
    "\n",
    "    loss = -pos_weight* targets * sigmoid_x.log() - (1-targets)*(1-sigmoid_x).log()\n",
    "\n",
    "    if weight is not None:\n",
    "        loss = loss * weight\n",
    "\n",
    "    if not reduce:\n",
    "        return loss\n",
    "    elif size_average:\n",
    "        return loss.mean()\n",
    "    else:\n",
    "        return loss.sum()\n",
    "\n",
    "class WeightedBCELoss(nn.Module):\n",
    "    def __init__(self, pos_weight= 1, weight=None, PosWeightIsDynamic= True, WeightIsDynamic= False, size_average=True, reduce=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pos_weight = Weight for postive samples. Size [1,C]\n",
    "            weight = Weight for Each class. Size [1,C]\n",
    "            PosWeightIsDynamic: If True, the pos_weight is computed on each batch. If pos_weight is None, then it remains None.\n",
    "            WeightIsDynamic: If True, the weight is computed on each batch. If weight is None, then it remains None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        #self.register_buffer('weight', weight)\n",
    "        #self.register_buffer('pos_weight', pos_weight)\n",
    "        self.size_average = size_average\n",
    "        self.reduce = reduce\n",
    "        self.PosWeightIsDynamic = PosWeightIsDynamic\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # pos_weight = Variable(self.pos_weight) if not isinstance(self.pos_weight, Variable) else self.pos_weight\n",
    "        if self.PosWeightIsDynamic:\n",
    "            positive_counts = target.sum(dim=0)\n",
    "            nBatch = len(target)\n",
    "            self.pos_weight = (nBatch - positive_counts)/(positive_counts +1e-5)\n",
    "\n",
    "\n",
    "        return weighted_binary_cross_entropy(input, target,\n",
    "                                                self.pos_weight,\n",
    "                                                weight=None,\n",
    "                                                size_average=self.size_average,\n",
    "                                                reduce=self.reduce)\n",
    "\n",
    "\n",
    "class WeightedCELoss(nn.Module):\n",
    "    def __init__(self, size_average=True, reduce=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pos_weight = Weight for postive samples. Size [1,C]\n",
    "            weight = Weight for Each class. Size [1,C]\n",
    "            PosWeightIsDynamic: If True, the pos_weight is computed on each batch. If pos_weight is None, then it remains None.\n",
    "            WeightIsDynamic: If True, the weight is computed on each batch. If weight is None, then it remains None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.size_average = size_average\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        positive_counts = target.sum(dim=0)\n",
    "        nBatch = len(target)\n",
    "        pos_weight = (nBatch - positive_counts)/(positive_counts +1e-5)\n",
    "        neg_count = nBatch - positive_counts\n",
    "        neg_weight = (nBatch - neg_count)/(neg_count +1e-5)\n",
    "\n",
    "        weight = torch.tensor([neg_weight, pos_weight], device=target.device)\n",
    "  \n",
    "\n",
    "\n",
    "        return F.cross_entropy(input, target, weight=weight)\n",
    "\n",
    "\n",
    "\n",
    "class FLMultiLoss(nn.Module):\n",
    "    def __init__(self, gamma= 2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pos_weight = Weight for postive samples. Size [1,C]\n",
    "            weight = Weight for Each class. Size [1,C]\n",
    "            PosWeightIsDynamic: If True, the pos_weight is computed on each batch. If pos_weight is None, then it remains None.\n",
    "            WeightIsDynamic: If True, the weight is computed on each batch. If weight is None, then it remains None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.gamma = gamma\n",
    "    def forward(self, input, target):\n",
    "\n",
    "\n",
    "        return focal_binary_cross_entropy(input, target, gamma=2)\n",
    "\n",
    "def EntropyLoss(input_):\n",
    "    mask = input_.ge(0.000001)\n",
    "    mask_out = torch.masked_select(input_, mask)\n",
    "    entropy = -(torch.sum(mask_out * torch.log(mask_out)))\n",
    "    return entropy / float(input_.size(0))\n",
    "\n",
    "def focal_binary_cross_entropy(logits, targets, gamma=2):\n",
    "    num_label = targets.shape[1]\n",
    "    l = logits.reshape(-1)\n",
    "    t = targets.reshape(-1)\n",
    "    p = torch.sigmoid(l)\n",
    "    p = torch.where(t >= 0.5, p, 1-p)\n",
    "    logp = - torch.log(torch.clamp(p, 1e-4, 1-1e-4))\n",
    "    loss = logp*((1-p)**gamma)\n",
    "    loss = num_label*loss.mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1661450528385,
     "user": {
      "displayName": "Reem Elsayed",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "dG5x6Ck0nOh_"
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Sequence\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\" Focal Loss, as described in https://arxiv.org/abs/1708.02002.\n",
    "    It is essentially an enhancement to cross entropy loss and is\n",
    "    useful for classification tasks when there is a large class imbalance.\n",
    "    x is expected to contain raw, unnormalized scores for each class.\n",
    "    y is expected to contain class labels.\n",
    "    Shape:\n",
    "        - x: (batch_size, C) or (batch_size, C, d1, d2, ..., dK), K > 0.\n",
    "        - y: (batch_size,) or (batch_size, d1, d2, ..., dK), K > 0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 alpha: Optional[Tensor] = None,\n",
    "                 gamma: float = 0.,\n",
    "                 reduction: str = 'mean',\n",
    "                 ignore_index: int = -100):\n",
    "        \"\"\"Constructor.\n",
    "        Args:\n",
    "            alpha (Tensor, optional): Weights for each class. Defaults to None.\n",
    "            gamma (float, optional): A constant, as described in the paper.\n",
    "                Defaults to 0.\n",
    "            reduction (str, optional): 'mean', 'sum' or 'none'.\n",
    "                Defaults to 'mean'.\n",
    "            ignore_index (int, optional): class label to ignore.\n",
    "                Defaults to -100.\n",
    "        \"\"\"\n",
    "        if reduction not in ('mean', 'sum', 'none'):\n",
    "            raise ValueError(\n",
    "                'Reduction must be one of: \"mean\", \"sum\", \"none\".')\n",
    "\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ignore_index = ignore_index\n",
    "        self.reduction = reduction\n",
    "\n",
    "        self.nll_loss = nn.NLLLoss(\n",
    "            weight=alpha, reduction='none', ignore_index=ignore_index)\n",
    "\n",
    "    def __repr__(self):\n",
    "        arg_keys = ['alpha', 'gamma', 'ignore_index', 'reduction']\n",
    "        arg_vals = [self.__dict__[k] for k in arg_keys]\n",
    "        arg_strs = [f'{k}={v}' for k, v in zip(arg_keys, arg_vals)]\n",
    "        arg_str = ', '.join(arg_strs)\n",
    "        return f'{type(self).__name__}({arg_str})'\n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor) -> Tensor:\n",
    "        if x.ndim > 2:\n",
    "            # (N, C, d1, d2, ..., dK) --> (N * d1 * ... * dK, C)\n",
    "            c = x.shape[1]\n",
    "            x = x.permute(0, *range(2, x.ndim), 1).reshape(-1, c)\n",
    "            # (N, d1, d2, ..., dK) --> (N * d1 * ... * dK,)\n",
    "            y = y.view(-1)\n",
    "\n",
    "        unignored_mask = y != self.ignore_index\n",
    "        y = y[unignored_mask]\n",
    "        if len(y) == 0:\n",
    "            return torch.tensor(0.)\n",
    "        x = x[unignored_mask]\n",
    "\n",
    "        # compute weighted cross entropy term: -alpha * log(pt)\n",
    "        # (alpha is already part of self.nll_loss)\n",
    "        log_p = F.log_softmax(x, dim=-1)\n",
    "        ce = self.nll_loss(log_p, y)\n",
    "\n",
    "        # get true class column from each row\n",
    "        all_rows = torch.arange(len(x))\n",
    "        log_pt = log_p[all_rows, y]\n",
    "\n",
    "        # compute focal term: (1 - pt)^gamma\n",
    "        pt = log_pt.exp()\n",
    "        focal_term = (1 - pt)**self.gamma\n",
    "\n",
    "        # the full loss: -alpha * ((1 - pt)^gamma) * log(pt)\n",
    "        loss = focal_term * ce\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def focal_loss(alpha: Optional[Sequence] = None,\n",
    "               gamma: float = 0.,\n",
    "               reduction: str = 'mean',\n",
    "               ignore_index: int = -100,\n",
    "               device='cuda',\n",
    "               dtype=torch.float32) -> FocalLoss:\n",
    "    \"\"\"Factory function for FocalLoss.\n",
    "    Args:\n",
    "        alpha (Sequence, optional): Weights for each class. Will be converted\n",
    "            to a Tensor if not None. Defaults to None.\n",
    "        gamma (float, optional): A constant, as described in the paper.\n",
    "            Defaults to 0.\n",
    "        reduction (str, optional): 'mean', 'sum' or 'none'.\n",
    "            Defaults to 'mean'.\n",
    "        ignore_index (int, optional): class label to ignore.\n",
    "            Defaults to -100.\n",
    "        device (str, optional): Device to move alpha to. Defaults to 'cpu'.\n",
    "        dtype (torch.dtype, optional): dtype to cast alpha to.\n",
    "            Defaults to torch.float32.\n",
    "    Returns:\n",
    "        A FocalLoss object\n",
    "    \"\"\"\n",
    "    if alpha is not None:\n",
    "        if not isinstance(alpha, Tensor):\n",
    "            alpha = torch.tensor(alpha)\n",
    "        alpha = alpha.to(device=device, dtype=dtype)\n",
    "\n",
    "    fl = FocalLoss(\n",
    "        alpha=alpha,\n",
    "        gamma=gamma,\n",
    "        reduction=reduction,\n",
    "        ignore_index=ignore_index)\n",
    "    return fl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvByMHwKuI4d"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1661450528385,
     "user": {
      "displayName": "Reem Elsayed",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "FW53GnvRuJ6J"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import AutoModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def init_weights(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv2d') != -1 or classname.find('ConvTranspose2d') != -1:\n",
    "        nn.init.kaiming_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "class AttentionWithContext(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AttentionWithContext, self).__init__()\n",
    "\n",
    "        self.attn = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.contx = nn.Linear(hidden_dim, 1, bias=False)\n",
    "        #self.apply(init_weights)\n",
    "    def forward(self, inp):\n",
    "        u = torch.tanh_(self.attn(inp))\n",
    "        a = F.softmax(self.contx(u))\n",
    "        s = (a * inp).sum(1)\n",
    "        return s\n",
    "\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 pretrained_path='aubmindlab/bert-base-arabert'):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "\n",
    "        \n",
    "        self.transformer = AutoModel.from_pretrained(pretrained_path, output_hidden_states=True)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None):\n",
    "        outputs = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        #(output_last_layer, pooled_cls, (output_layers))\n",
    "        #output[0] (8, seqlen=64, 768) cls [8, 768] ( 12 (8, seqlen=64, 768))\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def output_num(self):\n",
    "        return self.transformer.config.hidden_size\n",
    "\n",
    "class ATTClassifier(nn.Module):\n",
    "    def __init__(self, in_feature, class_num=1, dropout_prob=0.2):\n",
    "        super(ATTClassifier, self).__init__()\n",
    "        self.attention = AttentionWithContext(in_feature)\n",
    "\n",
    "        self.Classifier = nn.Sequential(\n",
    "            nn.Linear(2 * in_feature, 512),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, class_num)\n",
    "        )\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        att = self.attention(x[0]) #(X[0] (bs, seqlenght, embedD) att = \\sum_i alpha_i x[0][i]\n",
    "\n",
    "        xx = torch.cat([att, x[1]], 1)\n",
    "\n",
    "        out = self.Classifier(xx)\n",
    "        return out\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        \n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.kaiming_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim \n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        \n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "            \n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        \n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / (torch.sum(a, 1, keepdim=True) + 1e-10)\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)\n",
    "\n",
    "class NADIModel(nn.Module):\n",
    "  def __init__(self, pretrained_path='aubmindlab/bert-base-arabert',in_feature=768, class_num=3, dropout_prob=0.2):\n",
    "        super(NADIModel, self).__init__()\n",
    "        self.base_model=TransformerLayer(pretrained_path).to('cuda')\n",
    "        self.att=Attention(self.base_model.output_num(), 512).to('cuda')\n",
    "        self.classifier=nn.Linear(self.base_model.output_num(), class_num)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        self.dropout4 = nn.Dropout(0.4)\n",
    "        self.dropout5 = nn.Dropout(0.5)\n",
    "  def forward(self, ids,mask):\n",
    "    output=self.base_model(ids,mask)\n",
    "    output=self.att(output.last_hidden_state)\n",
    "    logits1 = self.classifier(self.dropout1(output))\n",
    "    logits2 = self.classifier(self.dropout2(output))\n",
    "    logits3 = self.classifier(self.dropout3(output))\n",
    "    logits4 = self.classifier(self.dropout4(output))\n",
    "    logits5 = self.classifier(self.dropout5(output))\n",
    "    logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\n",
    "#     output=self.classifier(output)\n",
    "    # output=torch.softmax(output, dim=1)\n",
    "    return logits\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYQWLGzVyqt_"
   },
   "source": [
    "# Train, Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1661450528386,
     "user": {
      "displayName": "Reem Elsayed",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "pjYrf5IerGh7"
   },
   "outputs": [],
   "source": [
    "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"\n",
    "    Samples elements randomly from a given list of indices for imbalanced dataset\n",
    "    Arguments:\n",
    "        indices (list, optional): a list of indices\n",
    "        num_samples (int, optional): number of samples to draw\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, indices=None, num_samples=None):\n",
    "        # if indices is not provided,\n",
    "        # all elements in the dataset will be considered\n",
    "        self.indices = list(range(len(dataset['#3_label']))) \\\n",
    "            if indices is None else indices\n",
    "\n",
    "        # if num_samples is not provided,\n",
    "        # draw `len(indices)` samples in each iteration\n",
    "        self.num_samples = len(self.indices) \\\n",
    "            if num_samples is None else num_samples\n",
    "\n",
    "        # distribution of classes in the dataset\n",
    "        label_to_count = {}\n",
    "        for idx in self.indices:\n",
    "            label = self._get_label(dataset, idx)\n",
    "            if label in label_to_count:\n",
    "                label_to_count[label] += 1\n",
    "            else:\n",
    "                label_to_count[label] = 1\n",
    "\n",
    "        # weight for each sample\n",
    "        weights = [1.0 / label_to_count[self._get_label(dataset, idx)] for idx in self.indices]\n",
    "        self.weights = torch.DoubleTensor(weights)\n",
    "\n",
    "    def _get_label(self, dataset, id_):\n",
    "        return dataset['#3_label'][id_]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1661450528386,
     "user": {
      "displayName": "Reem Elsayed",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "ZWP3677hqMr1"
   },
   "outputs": [],
   "source": [
    "def criterion(outputs1,  targets):\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(outputs1, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1661450528387,
     "user": {
      "displayName": "Reem Elsayed",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "ltIB44KZyxPt"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, data in bar:\n",
    "        \n",
    "        text_ids = data['text_ids'].to(device, dtype = torch.long)\n",
    "        text_mask = data['text_mask'].to(device, dtype = torch.long)\n",
    "        targets = data['target'].to(device, dtype=torch.long)\n",
    "        \n",
    "        batch_size = text_ids.size(0)\n",
    "        # print(targets)\n",
    "\n",
    "        outputs = model(text_ids, text_mask)\n",
    "        # print(outputs.shape)\n",
    "        \n",
    "        # print(outputs.shape)\n",
    "\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        loss = loss / CONFIG['n_accumulate']\n",
    "        loss.backward()\n",
    "    \n",
    "        if (step + 1) % CONFIG['n_accumulate'] == 0:\n",
    "            optimizer.step()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "                \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        \n",
    "        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n",
    "                        LR=optimizer.param_groups[0]['lr'])\n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1661450528388,
     "user": {
      "displayName": "Reem Elsayed",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "U5Ro64pIy4HN"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, dataloader, device, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    prediction=[]\n",
    "    true_prediction=[]\n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, data in bar:        \n",
    "        \n",
    "        text_ids = data['text_ids'].to(device, dtype = torch.long)\n",
    "        text_mask = data['text_mask'].to(device, dtype = torch.long)\n",
    "        targets = data['target'].to(device, dtype=torch.long)\n",
    "        \n",
    "        batch_size = text_ids.size(0)\n",
    "\n",
    "        outputs = model(text_ids, text_mask)\n",
    "        prediction.append(F.softmax(outputs).to('cpu').numpy())\n",
    "        true_prediction.append(targets.to('cpu').numpy())\n",
    "        # outputs = outputs.argmax(dim=1)\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        \n",
    "        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n",
    "                        LR=optimizer.param_groups[0]['lr']) \n",
    "    gc.collect()\n",
    "    \n",
    "    prediction = np.concatenate(prediction)\n",
    "    true_prediction = np.concatenate(true_prediction)\n",
    "    prediction=np.argmax(np.array(prediction),axis=1)\n",
    "    print(print_statistics(np.array(true_prediction),prediction))\n",
    "    print(f1_score(np.array(true_prediction),prediction, average='macro'))\n",
    "\n",
    "    \n",
    "    return epoch_loss, f1_score(np.array(true_prediction),prediction, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score,f1_score,accuracy_score,recall_score,precision_score,classification_report\n",
    "def print_statistics(y, y_pred):\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision =precision_score(y, y_pred, average='weighted')\n",
    "    recall = recall_score(y, y_pred, average='weighted')\n",
    "    f_score = f1_score(y, y_pred, average='weighted')\n",
    "    print('Accuracy: %.3f\\nPrecision: %.3f\\nRecall: %.3f\\nF_score: %.3f\\n'\n",
    "          % (accuracy, precision, recall, f_score))\n",
    "    print(classification_report(y, y_pred))\n",
    "    return accuracy, precision, recall, f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1661450528388,
     "user": {
      "displayName": "Reem Elsayed",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "HpH2EB0snbV0"
   },
   "outputs": [],
   "source": [
    "def run_training(model, optimizer, scheduler, device, num_epochs, fold,train_loader, valid_loader):\n",
    "    # To automatically log gradients\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
    "    \n",
    "    start = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_epoch_loss = 0\n",
    "    history = defaultdict(list)\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1): \n",
    "        gc.collect()\n",
    "        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, \n",
    "                                           dataloader=train_loader, \n",
    "                                           device=CONFIG['device'], epoch=epoch)\n",
    "        \n",
    "        val_epoch_loss,f1_score = valid_one_epoch(model, valid_loader, device=CONFIG['device'], \n",
    "                                         epoch=epoch)\n",
    "    \n",
    "        history['Train Loss'].append(train_epoch_loss)\n",
    "        history['Valid Loss'].append(val_epoch_loss)\n",
    "        \n",
    "       \n",
    "        \n",
    "        # deep copy the model\n",
    "        if f1_score >= best_epoch_loss:\n",
    "            print(f\"Validation Loss Improved ({best_epoch_loss} ---> {f1_score})\")\n",
    "            best_epoch_loss = f1_score\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            PATH = f\"/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/araelectra/Loss-crossentropy-nosampler.bin\"\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            # Save a model file from the current directory\n",
    "            print(\"Model Saved\")\n",
    "            \n",
    "        print()\n",
    "    \n",
    "    end = time.time()\n",
    "    time_elapsed = end - start\n",
    "    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
    "    print(\"Best Loss: {:.4f}\".format(best_epoch_loss))\n",
    "    \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1661450528390,
     "user": {
      "displayName": "Reem Elsayed",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "JxT6JHg0nogC"
   },
   "outputs": [],
   "source": [
    "def fetch_scheduler(optimizer):\n",
    "    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CONFIG['T_max'], \n",
    "                                                   eta_min=CONFIG['min_lr'])\n",
    "    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n",
    "        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=CONFIG['T_0'], \n",
    "                                                             eta_min=CONFIG['min_lr'])\n",
    "    elif CONFIG['scheduler'] == None:\n",
    "        return None\n",
    "        \n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1661450528390,
     "user": {
      "displayName": "Reem Elsayed",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "x-x9oUAsnqqy"
   },
   "outputs": [],
   "source": [
    "def prepare_loaders(train,valid):\n",
    "#     df_train = train[train.kfold != fold].reset_index(drop=True)\n",
    "#     df_valid = train[train.kfold == fold].reset_index(drop=True)\n",
    "#     # print(len(df_valid))\n",
    "    sampler = ImbalancedDatasetSampler(train)\n",
    "    \n",
    "    train_dataset = TrainDataset(train, tokenizer=tokenizer, max_length=CONFIG['max_length'])\n",
    "    valid_dataset = TrainDataset(valid, tokenizer=tokenizer, max_length=CONFIG['max_length'])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], \n",
    "                              num_workers=2, shuffle=True, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], \n",
    "                              num_workers=2, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-MsRHzBnZX8"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 96913,
     "status": "ok",
     "timestamp": 1661450625294,
     "user": {
      "displayName": "Reem Elsayed",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "ih8htg1LoRZC",
    "outputId": "6567f6b7-4cf2-4d56-d0b4-3265c785cc6d"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1661450625295,
     "user": {
      "displayName": "Reem Elsayed",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "yeq4g6QBasWl"
   },
   "outputs": [],
   "source": [
    "# ,jmnbv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "executionInfo": {
     "elapsed": 1979,
     "status": "ok",
     "timestamp": 1661450627268,
     "user": {
      "displayName": "Reem Elsayed",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "5dHag8YTn7jE"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Dataset/NADI2022_Subtask2_TRAIN.tsv', sep='\\t', lineterminator='\\n')\n",
    "valid = pd.read_csv('/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Dataset/NADI2022_Subtask2_DEV.tsv', sep='\\t', lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1661450627281,
     "user": {
      "displayName": "Reem Elsayed",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "ZLZqgBrJoyaa",
    "outputId": "a0b0e21d-5b94-4403-e767-30e559707d08"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#1_id</th>\n",
       "      <th>#2_content</th>\n",
       "      <th>#3_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_983</td>\n",
       "      <td>أنا وطن بحاجة إلى شعب . . تعالي واسكنيني .</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_1430</td>\n",
       "      <td>#احذرو_الرافضه_غدر_وخيانه الرافضة شر وبال على ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_461</td>\n",
       "      <td>كلما راجعت أحلام الصبا قلت يا ليت الصبا لم يزل...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_962</td>\n",
       "      <td>مؤلم جدا عندما يبتعد أقرب أصدقائك بسبب أصدقاء جدد</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_1445</td>\n",
       "      <td>USER معنه ليلحين احس مابردت چبدي عدل بس اهم شي...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        #1_id                                         #2_content #3_label\n",
       "0   TRAIN_983         أنا وطن بحاجة إلى شعب . . تعالي واسكنيني .      pos\n",
       "1  TRAIN_1430  #احذرو_الرافضه_غدر_وخيانه الرافضة شر وبال على ...      neg\n",
       "2   TRAIN_461  كلما راجعت أحلام الصبا قلت يا ليت الصبا لم يزل...      pos\n",
       "3   TRAIN_962  مؤلم جدا عندما يبتعد أقرب أصدقائك بسبب أصدقاء جدد      neg\n",
       "4  TRAIN_1445  USER معنه ليلحين احس مابردت چبدي عدل بس اهم شي...      neg"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1661450627282,
     "user": {
      "displayName": "Reem Elsayed",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "gB9D4twgo0bI",
    "outputId": "a72cdc8e-e000-4e89-c06d-baf0f39b415c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1661450627282,
     "user": {
      "displayName": "Reem Elsayed",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "qYdSpBt2o2HR",
    "outputId": "aa64b63c-cb54-4f8d-b69c-4bd22b6d2f23"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['#3_label'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1661450627283,
     "user": {
      "displayName": "Reem Elsayed",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "LREmIaISpGJF",
    "outputId": "984f6514-87b8-438a-9b17-8b1d0b5b38c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid['#3_label'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1661450627283,
     "user": {
      "displayName": "Reem Elsayed",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "alyqp1JApKHQ"
   },
   "outputs": [],
   "source": [
    "df_train=train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1661450627284,
     "user": {
      "displayName": "Reem Elsayed",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "F8zmQRzrpDra"
   },
   "outputs": [],
   "source": [
    "num_classes=df_train['#3_label'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1661450627285,
     "user": {
      "displayName": "Reem Elsayed",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "0_HBDMdYn1Mi"
   },
   "outputs": [],
   "source": [
    "CONFIG = {\"seed\": 42,\n",
    "          \"epochs\": 30,\n",
    "          \"train_batch_size\": 4,\n",
    "          \"valid_batch_size\": 8,\n",
    "          \"max_length\": 512,\n",
    "          \"learning_rate\": 2e-5,\n",
    "          \"scheduler\": 'CosineAnnealingLR',\n",
    "          \"min_lr\": 1e-8,\n",
    "          \"T_max\": 500,\n",
    "          \"weight_decay\": 1e-8,\n",
    "          \"n_fold\": 5,\n",
    "          \"n_accumulate\": 1,\n",
    "          \"num_classes\": 18,\n",
    "          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "9277ce48447c4f8e95a162f7e3d65620",
      "328bf80910d64d83b4abfcac3e356210",
      "f7b2eff8e7184a72ba727dfac6ad5806",
      "7a2f611e11cd4e3b9302d745fbaf3382",
      "88a599f23423444794211f10b75a8e80",
      "2af5a19c66d04deba0c3a39d35a24104",
      "a5b7a737240f4a8aa1a1b982f2cb0c53",
      "97d4c78c87834f4286ef2cf231b5cbc2",
      "dc630edf4f224a2e973dfed484204ff1",
      "723ccadde5784350bc08e70f42b94174",
      "2138396876d74dd1ba0d640ca25c395f",
      "c3956996f80a49b7b46b978274b32ef0",
      "61401f4e43a141ae851718badcc629b9",
      "d4b408519ae24d868ee8b95d45ee707c",
      "6f35345c87ba4b0089200b8329dfb22b",
      "7f3883fd5022493c9da32650885e8e72",
      "1d7ec87993334c619346ce807e86823d",
      "358d653088be4813a59bb1a585c6d2e0",
      "cae94cfcd22d4eabbe366a54c9530c4f",
      "538fa883a58044fd9b245db52e197927",
      "9bf8113c04b8402c8de9c8b7d893a05d",
      "5ba8caae38b440e689367f38969c7204",
      "df619d778c7442a98215e48b2a0ac0db",
      "10e4fe6c4aed4a80affac3c6042a6ce5",
      "5823f3b1668a4a72a0f91bf9c1ef33f5",
      "7b34bfe94a3843f5a814a00467c64f99",
      "5a558a58c20247c9adbf61a3c63465b4",
      "b249d4c832e140d694d83894a9637422",
      "38b99803966c432c86302422797bba7e",
      "31e6aa7e72e24bfea2dd2579b0d5688a",
      "fddda1530aa94361a6e25ad6b7dc5af1",
      "da1758be1c334a28a94b84e1631be0c6",
      "2b6187da777a4e11b56e651045f6cbc5",
      "78a36e1614404f25a1183263d10731b9",
      "204ad3545a5e4d96a6b03f3919896372",
      "358e50c17d6143f6801a1368506298a3",
      "98aa2e02bf554e3a8b2b66eab9fd4106",
      "e3f8df364a2f4d8391a8e6192f69e139",
      "e7ad6e67202746078489d07c7d44a414",
      "839916b14ac644e2a4dcca7d017fd37d",
      "fbcc7b929fd144a181ada4bc96e17417",
      "d887e3f942944c9094a0d62ef3f02479",
      "b0279fd09c7b4e92922f7cf99f87810e",
      "219265426b01403e8a7cf4406edfd7a2",
      "dde1d50f4f3a42bfa0c9edf43a7006d3",
      "5b112eb5f0594e4095331a4d726d4b83",
      "858bfb98bf764279935a7db8af42ec39",
      "73f300a46e9e42d2ac2529f7148dd16a",
      "9d24316575d94223b837d7b2f8f63bf7",
      "c027494a95904d64948d66ee8e75ff8a",
      "3b5c9901d77a45c0a284323a6cdb9b24",
      "a4bfb7c2831a4f48aeec04f2c7a041b8",
      "0e1f58922f18474b9ec9d643d6b0f1ba",
      "9787540fcb9c4a7e8a44241ee4fba6ef",
      "f45c6b6be89645f48e5c933a3567405e"
     ]
    },
    "executionInfo": {
     "elapsed": 2957,
     "status": "ok",
     "timestamp": 1661450630221,
     "user": {
      "displayName": "Reem Elsayed",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "89VbI4nqq17L",
    "outputId": "0073b1a0-4b5a-4bfd-88c7-b4b63bf51c65"
   },
   "outputs": [],
   "source": [
    "tokenizer= AutoTokenizer.from_pretrained('aubmindlab/araelectra-base-discriminator')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1661450630223,
     "user": {
      "displayName": "Reem Elsayed",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "E9sfjlh6qj6t"
   },
   "outputs": [],
   "source": [
    "df_train.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1661450630223,
     "user": {
      "displayName": "Reem Elsayed",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "Dq0tSkUrntN4"
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=CONFIG['n_fold'], shuffle=True, random_state=CONFIG['seed'])\n",
    "\n",
    "for fold, ( _, val_) in enumerate(skf.split(X=df_train, y=df_train['#3_label'])):\n",
    "    df_train.loc[val_ , \"kfold\"] = int(fold)\n",
    "    \n",
    "df_train[\"kfold\"] = df_train[\"kfold\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1661450630224,
     "user": {
      "displayName": "Reem Elsayed",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "0_0l7FW06mgP",
    "outputId": "3d50d563-5d04-48e5-e83c-a0ae7ab01d31"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 1, 2, ..., 0, 4, 0])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['kfold'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1661450630225,
     "user": {
      "displayName": "Reem Elsayed",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "wOMg6NEVsEkU"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "train['#3_label'] = le.fit_transform(train['#3_label'].values)\n",
    "valid['#3_label'] = le.transform(valid['#3_label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at aubmindlab/araelectra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU: NVIDIA GeForce RTX 3080 Ti\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|██████████████████████████████████████████████| 375/375 [00:34<00:00, 10.73it/s, Epoch=1, LR=2.94e-6, Train_Loss=1]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████████| 63/63 [00:03<00:00, 20.22it/s, Epoch=1, LR=2.94e-6, Valid_Loss=0.896]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.612\n",
      "Precision: 0.615\n",
      "Recall: 0.612\n",
      "F_score: 0.607\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.58      0.63       190\n",
      "           1       0.52      0.42      0.46       113\n",
      "           2       0.59      0.75      0.66       197\n",
      "\n",
      "    accuracy                           0.61       500\n",
      "   macro avg       0.60      0.58      0.59       500\n",
      "weighted avg       0.61      0.61      0.61       500\n",
      "\n",
      "(0.612, 0.6148952222222221, 0.612, 0.6065826208659812)\n",
      "0.5865107650712834\n",
      "Validation Loss Improved (0 ---> 0.5865107650712834)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|██████████████████████████████████████████████| 375/375 [00:35<00:00, 10.65it/s, Epoch=2, LR=1e-5, Train_Loss=0.76]\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████████| 63/63 [00:03<00:00, 19.83it/s, Epoch=2, LR=1e-5, Valid_Loss=0.839]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.640\n",
      "Precision: 0.633\n",
      "Recall: 0.640\n",
      "F_score: 0.624\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.81      0.71       190\n",
      "           1       0.56      0.31      0.40       113\n",
      "           2       0.67      0.66      0.67       197\n",
      "\n",
      "    accuracy                           0.64       500\n",
      "   macro avg       0.62      0.60      0.59       500\n",
      "weighted avg       0.63      0.64      0.62       500\n",
      "\n",
      "(0.64, 0.6327355616733495, 0.64, 0.6239925925925925)\n",
      "0.5932098765432099\n",
      "Validation Loss Improved (0.5865107650712834 ---> 0.5932098765432099)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                           | 0/375 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|██████████████████████████████████████████| 375/375 [00:35<00:00, 10.59it/s, Epoch=3, LR=1.71e-5, Train_Loss=0.733]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████████| 63/63 [00:03<00:00, 20.09it/s, Epoch=3, LR=1.71e-5, Valid_Loss=0.866]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.636\n",
      "Precision: 0.629\n",
      "Recall: 0.636\n",
      "F_score: 0.630\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.69      0.68       190\n",
      "           1       0.54      0.40      0.46       113\n",
      "           2       0.65      0.72      0.68       197\n",
      "\n",
      "    accuracy                           0.64       500\n",
      "   macro avg       0.62      0.60      0.61       500\n",
      "weighted avg       0.63      0.64      0.63       500\n",
      "\n",
      "(0.636, 0.6290658730158729, 0.636, 0.6295061345778029)\n",
      "0.6055281951694238\n",
      "Validation Loss Improved (0.5932098765432099 ---> 0.6055281951694238)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|██████████████████████████████████████████████| 375/375 [00:35<00:00, 10.57it/s, Epoch=4, LR=1e-8, Train_Loss=0.37]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████████████| 63/63 [00:03<00:00, 19.86it/s, Epoch=4, LR=1e-8, Valid_Loss=1.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.622\n",
      "Precision: 0.624\n",
      "Recall: 0.622\n",
      "F_score: 0.623\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.67      0.68       190\n",
      "           1       0.46      0.48      0.47       113\n",
      "           2       0.66      0.65      0.66       197\n",
      "\n",
      "    accuracy                           0.62       500\n",
      "   macro avg       0.60      0.60      0.60       500\n",
      "weighted avg       0.62      0.62      0.62       500\n",
      "\n",
      "(0.622, 0.6242454316882575, 0.622, 0.6230422304672752)\n",
      "0.6016240269935251\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|██████████████████████████████████████████| 375/375 [00:35<00:00, 10.55it/s, Epoch=5, LR=1.71e-5, Train_Loss=0.259]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 63/63 [00:03<00:00, 19.96it/s, Epoch=5, LR=1.71e-5, Valid_Loss=1.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.588\n",
      "Precision: 0.591\n",
      "Recall: 0.588\n",
      "F_score: 0.583\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.55      0.60       190\n",
      "           1       0.46      0.41      0.43       113\n",
      "           2       0.59      0.73      0.65       197\n",
      "\n",
      "    accuracy                           0.59       500\n",
      "   macro avg       0.57      0.56      0.56       500\n",
      "weighted avg       0.59      0.59      0.58       500\n",
      "\n",
      "(0.588, 0.5905032521395656, 0.588, 0.5834404546499933)\n",
      "0.5621357145869952\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 375/375 [00:35<00:00, 10.51it/s, Epoch=6, LR=1e-5, Train_Loss=0.292]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████████████| 63/63 [00:03<00:00, 19.80it/s, Epoch=6, LR=1e-5, Valid_Loss=1.26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.606\n",
      "Precision: 0.602\n",
      "Recall: 0.606\n",
      "F_score: 0.603\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.67      0.68       190\n",
      "           1       0.43      0.38      0.40       113\n",
      "           2       0.62      0.68      0.65       197\n",
      "\n",
      "    accuracy                           0.61       500\n",
      "   macro avg       0.58      0.57      0.58       500\n",
      "weighted avg       0.60      0.61      0.60       500\n",
      "\n",
      "(0.606, 0.6022306630359698, 0.606, 0.6032759369934015)\n",
      "0.5755482039554692\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                           | 0/375 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████| 375/375 [00:35<00:00, 10.54it/s, Epoch=7, LR=2.94e-6, Train_Loss=0.0897]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 63/63 [00:03<00:00, 19.76it/s, Epoch=7, LR=2.94e-6, Valid_Loss=1.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.606\n",
      "Precision: 0.611\n",
      "Recall: 0.606\n",
      "F_score: 0.608\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.64      0.65       190\n",
      "           1       0.44      0.48      0.46       113\n",
      "           2       0.65      0.65      0.65       197\n",
      "\n",
      "    accuracy                           0.61       500\n",
      "   macro avg       0.59      0.59      0.59       500\n",
      "weighted avg       0.61      0.61      0.61       500\n",
      "\n",
      "(0.606, 0.6106639566395664, 0.606, 0.6079642693540999)\n",
      "0.5871424551971743\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 375/375 [00:35<00:00, 10.55it/s, Epoch=8, LR=2e-5, Train_Loss=0.103]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████████████| 63/63 [00:03<00:00, 19.84it/s, Epoch=8, LR=2e-5, Valid_Loss=1.87]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.546\n",
      "Precision: 0.597\n",
      "Recall: 0.546\n",
      "F_score: 0.516\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.33      0.46       190\n",
      "           1       0.48      0.32      0.38       113\n",
      "           2       0.51      0.88      0.65       197\n",
      "\n",
      "    accuracy                           0.55       500\n",
      "   macro avg       0.58      0.51      0.50       500\n",
      "weighted avg       0.60      0.55      0.52       500\n",
      "\n",
      "(0.546, 0.5973698752906361, 0.546, 0.5163199961133091)\n",
      "0.4967190863847399\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|██████████████████████████████████████████| 375/375 [00:35<00:00, 10.51it/s, Epoch=9, LR=2.94e-6, Train_Loss=0.107]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 63/63 [00:03<00:00, 19.88it/s, Epoch=9, LR=2.94e-6, Valid_Loss=1.62]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.598\n",
      "Precision: 0.612\n",
      "Recall: 0.598\n",
      "F_score: 0.603\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.61      0.65       190\n",
      "           1       0.42      0.51      0.46       113\n",
      "           2       0.65      0.63      0.64       197\n",
      "\n",
      "    accuracy                           0.60       500\n",
      "   macro avg       0.59      0.59      0.58       500\n",
      "weighted avg       0.61      0.60      0.60       500\n",
      "\n",
      "(0.598, 0.6118644578045926, 0.598, 0.6028528318360722)\n",
      "0.5831292646935106\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████| 375/375 [00:35<00:00, 10.56it/s, Epoch=10, LR=1e-5, Train_Loss=0.0389]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████████| 63/63 [00:03<00:00, 19.75it/s, Epoch=10, LR=1e-5, Valid_Loss=1.69]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.612\n",
      "Precision: 0.625\n",
      "Recall: 0.612\n",
      "F_score: 0.617\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.64      0.66       190\n",
      "           1       0.44      0.54      0.48       113\n",
      "           2       0.66      0.63      0.65       197\n",
      "\n",
      "    accuracy                           0.61       500\n",
      "   macro avg       0.60      0.60      0.60       500\n",
      "weighted avg       0.62      0.61      0.62       500\n",
      "\n",
      "(0.612, 0.6246947617638877, 0.612, 0.6165083943833943)\n",
      "0.5982651607651608\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████| 375/375 [00:35<00:00, 10.53it/s, Epoch=11, LR=1.71e-5, Train_Loss=0.106]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████████| 63/63 [00:03<00:00, 19.84it/s, Epoch=11, LR=1.71e-5, Valid_Loss=1.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.570\n",
      "Precision: 0.575\n",
      "Recall: 0.570\n",
      "F_score: 0.566\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.52      0.58       190\n",
      "           1       0.41      0.39      0.40       113\n",
      "           2       0.58      0.73      0.65       197\n",
      "\n",
      "    accuracy                           0.57       500\n",
      "   macro avg       0.55      0.54      0.54       500\n",
      "weighted avg       0.58      0.57      0.57       500\n",
      "\n",
      "(0.57, 0.5753004330977889, 0.57, 0.5657745905647359)\n",
      "0.5424001893393886\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████████| 375/375 [00:35<00:00, 10.54it/s, Epoch=12, LR=1e-8, Train_Loss=0.069]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████████| 63/63 [00:03<00:00, 19.88it/s, Epoch=12, LR=1e-8, Valid_Loss=1.78]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.586\n",
      "Precision: 0.612\n",
      "Recall: 0.586\n",
      "F_score: 0.593\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.56      0.61       190\n",
      "           1       0.40      0.57      0.47       113\n",
      "           2       0.67      0.62      0.65       197\n",
      "\n",
      "    accuracy                           0.59       500\n",
      "   macro avg       0.58      0.58      0.58       500\n",
      "weighted avg       0.61      0.59      0.59       500\n",
      "\n",
      "(0.586, 0.6117801816852876, 0.586, 0.5931879111434093)\n",
      "0.5757279661875446\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████| 375/375 [00:35<00:00, 10.55it/s, Epoch=13, LR=1.71e-5, Train_Loss=0.0428]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 63/63 [00:03<00:00, 19.81it/s, Epoch=13, LR=1.71e-5, Valid_Loss=1.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.594\n",
      "Precision: 0.609\n",
      "Recall: 0.594\n",
      "F_score: 0.596\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.70      0.66       190\n",
      "           1       0.42      0.50      0.46       113\n",
      "           2       0.71      0.55      0.62       197\n",
      "\n",
      "    accuracy                           0.59       500\n",
      "   macro avg       0.58      0.58      0.58       500\n",
      "weighted avg       0.61      0.59      0.60       500\n",
      "\n",
      "(0.594, 0.6090662023794718, 0.594, 0.5960488183421516)\n",
      "0.5770252792475015\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████████| 375/375 [00:35<00:00, 10.52it/s, Epoch=14, LR=1e-5, Train_Loss=0.098]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████████| 63/63 [00:03<00:00, 19.80it/s, Epoch=14, LR=1e-5, Valid_Loss=1.72]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.590\n",
      "Precision: 0.598\n",
      "Recall: 0.590\n",
      "F_score: 0.593\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.62      0.63       190\n",
      "           1       0.42      0.49      0.45       113\n",
      "           2       0.65      0.62      0.64       197\n",
      "\n",
      "    accuracy                           0.59       500\n",
      "   macro avg       0.57      0.58      0.57       500\n",
      "weighted avg       0.60      0.59      0.59       500\n",
      "\n",
      "(0.59, 0.5983285423460994, 0.59, 0.5933104680608379)\n",
      "0.5735416427744148\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████| 375/375 [00:35<00:00, 10.54it/s, Epoch=15, LR=2.94e-6, Train_Loss=0.031]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████████| 63/63 [00:03<00:00, 19.74it/s, Epoch=15, LR=2.94e-6, Valid_Loss=1.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.610\n",
      "Precision: 0.620\n",
      "Recall: 0.610\n",
      "F_score: 0.613\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.62      0.65       190\n",
      "           1       0.43      0.50      0.46       113\n",
      "           2       0.66      0.67      0.66       197\n",
      "\n",
      "    accuracy                           0.61       500\n",
      "   macro avg       0.59      0.59      0.59       500\n",
      "weighted avg       0.62      0.61      0.61       500\n",
      "\n",
      "(0.61, 0.6197276100998239, 0.61, 0.6134307649855307)\n",
      "0.5919381279808006\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                           | 0/375 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████| 375/375 [00:35<00:00, 10.57it/s, Epoch=16, LR=2e-5, Train_Loss=0.0469]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████████| 63/63 [00:03<00:00, 19.89it/s, Epoch=16, LR=2e-5, Valid_Loss=2.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.526\n",
      "Precision: 0.594\n",
      "Recall: 0.526\n",
      "F_score: 0.538\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.46      0.53       190\n",
      "           1       0.34      0.65      0.44       113\n",
      "           2       0.70      0.52      0.60       197\n",
      "\n",
      "    accuracy                           0.53       500\n",
      "   macro avg       0.56      0.54      0.52       500\n",
      "weighted avg       0.59      0.53      0.54       500\n",
      "\n",
      "(0.526, 0.59367056762645, 0.526, 0.5383718502171332)\n",
      "0.5248709258413974\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                           | 0/375 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████| 375/375 [00:35<00:00, 10.55it/s, Epoch=17, LR=2.94e-6, Train_Loss=0.0639]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████████| 63/63 [00:03<00:00, 19.98it/s, Epoch=17, LR=2.94e-6, Valid_Loss=1.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.612\n",
      "Precision: 0.607\n",
      "Recall: 0.612\n",
      "F_score: 0.609\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.65      0.65       190\n",
      "           1       0.45      0.40      0.42       113\n",
      "           2       0.65      0.70      0.67       197\n",
      "\n",
      "    accuracy                           0.61       500\n",
      "   macro avg       0.59      0.58      0.58       500\n",
      "weighted avg       0.61      0.61      0.61       500\n",
      "\n",
      "(0.612, 0.6065471779405429, 0.612, 0.6085414354421014)\n",
      "0.5829095027617138\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                           | 0/375 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████| 375/375 [00:35<00:00, 10.53it/s, Epoch=18, LR=1e-5, Train_Loss=0.0136]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████████| 63/63 [00:03<00:00, 19.86it/s, Epoch=18, LR=1e-5, Valid_Loss=1.99]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.596\n",
      "Precision: 0.597\n",
      "Recall: 0.596\n",
      "F_score: 0.596\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.60      0.62       190\n",
      "           1       0.42      0.43      0.43       113\n",
      "           2       0.65      0.69      0.67       197\n",
      "\n",
      "    accuracy                           0.60       500\n",
      "   macro avg       0.57      0.57      0.57       500\n",
      "weighted avg       0.60      0.60      0.60       500\n",
      "\n",
      "(0.596, 0.5971678016923742, 0.596, 0.5961092831663376)\n",
      "0.5725059453103313\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████| 375/375 [00:35<00:00, 10.53it/s, Epoch=19, LR=1.71e-5, Train_Loss=0.0177]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████████| 63/63 [00:03<00:00, 19.80it/s, Epoch=19, LR=1.71e-5, Valid_Loss=2.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.602\n",
      "Precision: 0.595\n",
      "Recall: 0.602\n",
      "F_score: 0.597\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.67      0.65       190\n",
      "           1       0.45      0.37      0.41       113\n",
      "           2       0.64      0.67      0.65       197\n",
      "\n",
      "    accuracy                           0.60       500\n",
      "   macro avg       0.57      0.57      0.57       500\n",
      "weighted avg       0.59      0.60      0.60       500\n",
      "\n",
      "(0.602, 0.5947376630299199, 0.602, 0.5972988847783851)\n",
      "0.570739582061481\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████| 375/375 [00:35<00:00, 10.55it/s, Epoch=20, LR=1e-8, Train_Loss=0.0255]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████████| 63/63 [00:03<00:00, 19.84it/s, Epoch=20, LR=1e-8, Valid_Loss=2.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.594\n",
      "Precision: 0.592\n",
      "Recall: 0.594\n",
      "F_score: 0.593\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.63      0.64       190\n",
      "           1       0.42      0.41      0.41       113\n",
      "           2       0.64      0.66      0.65       197\n",
      "\n",
      "    accuracy                           0.59       500\n",
      "   macro avg       0.57      0.57      0.57       500\n",
      "weighted avg       0.59      0.59      0.59       500\n",
      "\n",
      "(0.594, 0.5924160313337299, 0.594, 0.5930065410323475)\n",
      "0.5681794946311075\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████| 375/375 [00:35<00:00, 10.56it/s, Epoch=21, LR=1.71e-5, Train_Loss=0.0331]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████████| 63/63 [00:03<00:00, 19.83it/s, Epoch=21, LR=1.71e-5, Valid_Loss=2.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.546\n",
      "Precision: 0.584\n",
      "Recall: 0.546\n",
      "F_score: 0.523\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.38      0.51       190\n",
      "           1       0.40      0.29      0.34       113\n",
      "           2       0.52      0.85      0.65       197\n",
      "\n",
      "    accuracy                           0.55       500\n",
      "   macro avg       0.56      0.51      0.50       500\n",
      "weighted avg       0.58      0.55      0.52       500\n",
      "\n",
      "(0.546, 0.5838800120818546, 0.546, 0.5230769230769231)\n",
      "0.4966261808367071\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████| 375/375 [00:35<00:00, 10.56it/s, Epoch=22, LR=1e-5, Train_Loss=0.0828]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████████| 63/63 [00:03<00:00, 19.95it/s, Epoch=22, LR=1e-5, Valid_Loss=2.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.580\n",
      "Precision: 0.608\n",
      "Recall: 0.580\n",
      "F_score: 0.587\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.53      0.60       190\n",
      "           1       0.40      0.56      0.46       113\n",
      "           2       0.64      0.64      0.64       197\n",
      "\n",
      "    accuracy                           0.58       500\n",
      "   macro avg       0.58      0.58      0.57       500\n",
      "weighted avg       0.61      0.58      0.59       500\n",
      "\n",
      "(0.58, 0.6080749475890985, 0.58, 0.5865115357520253)\n",
      "0.5692065406362551\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████| 375/375 [00:35<00:00, 10.55it/s, Epoch=23, LR=2.94e-6, Train_Loss=0.0128]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 63/63 [00:03<00:00, 19.82it/s, Epoch=23, LR=2.94e-6, Valid_Loss=2.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.592\n",
      "Precision: 0.592\n",
      "Recall: 0.592\n",
      "F_score: 0.589\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.59      0.63       190\n",
      "           1       0.42      0.39      0.40       113\n",
      "           2       0.62      0.71      0.66       197\n",
      "\n",
      "    accuracy                           0.59       500\n",
      "   macro avg       0.57      0.56      0.56       500\n",
      "weighted avg       0.59      0.59      0.59       500\n",
      "\n",
      "(0.592, 0.591657214869571, 0.592, 0.5894328371824303)\n",
      "0.5632182743003292\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|██████████████████████████████████████████| 375/375 [00:35<00:00, 10.56it/s, Epoch=24, LR=2e-5, Train_Loss=0.00718]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████████| 63/63 [00:03<00:00, 19.83it/s, Epoch=24, LR=2e-5, Valid_Loss=2.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.592\n",
      "Precision: 0.590\n",
      "Recall: 0.592\n",
      "F_score: 0.591\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.63      0.63       190\n",
      "           1       0.43      0.41      0.42       113\n",
      "           2       0.64      0.66      0.65       197\n",
      "\n",
      "    accuracy                           0.59       500\n",
      "   macro avg       0.57      0.57      0.57       500\n",
      "weighted avg       0.59      0.59      0.59       500\n",
      "\n",
      "(0.592, 0.5895071511470632, 0.592, 0.5906036829250931)\n",
      "0.5666020843788825\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████| 375/375 [00:35<00:00, 10.53it/s, Epoch=25, LR=2.94e-6, Train_Loss=0.00732]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████████| 63/63 [00:03<00:00, 19.81it/s, Epoch=25, LR=2.94e-6, Valid_Loss=2.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.572\n",
      "Precision: 0.580\n",
      "Recall: 0.572\n",
      "F_score: 0.570\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.54      0.60       190\n",
      "           1       0.39      0.39      0.39       113\n",
      "           2       0.59      0.71      0.64       197\n",
      "\n",
      "    accuracy                           0.57       500\n",
      "   macro avg       0.55      0.55      0.54       500\n",
      "weighted avg       0.58      0.57      0.57       500\n",
      "\n",
      "(0.572, 0.5798989340702724, 0.572, 0.5702817617739803)\n",
      "0.5449428677991387\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                           | 0/375 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|██████████████████████████████████████████| 375/375 [00:35<00:00, 10.52it/s, Epoch=26, LR=1e-5, Train_Loss=0.00905]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████████| 63/63 [00:03<00:00, 19.99it/s, Epoch=26, LR=1e-5, Valid_Loss=2.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.580\n",
      "Precision: 0.587\n",
      "Recall: 0.580\n",
      "F_score: 0.579\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.56      0.62       190\n",
      "           1       0.39      0.39      0.39       113\n",
      "           2       0.60      0.71      0.65       197\n",
      "\n",
      "    accuracy                           0.58       500\n",
      "   macro avg       0.56      0.55      0.55       500\n",
      "weighted avg       0.59      0.58      0.58       500\n",
      "\n",
      "(0.58, 0.5873143890289988, 0.58, 0.5793270309583186)\n",
      "0.552592289837449\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████| 375/375 [00:35<00:00, 10.52it/s, Epoch=27, LR=1.71e-5, Train_Loss=0.1]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████████| 63/63 [00:03<00:00, 19.81it/s, Epoch=27, LR=1.71e-5, Valid_Loss=1.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.586\n",
      "Precision: 0.605\n",
      "Recall: 0.586\n",
      "F_score: 0.588\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.54      0.62       190\n",
      "           1       0.40      0.45      0.42       113\n",
      "           2       0.61      0.71      0.65       197\n",
      "\n",
      "    accuracy                           0.59       500\n",
      "   macro avg       0.58      0.57      0.56       500\n",
      "weighted avg       0.60      0.59      0.59       500\n",
      "\n",
      "(0.586, 0.6049938588543239, 0.586, 0.5879222986019926)\n",
      "0.5646250096547463\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████| 375/375 [00:35<00:00, 10.54it/s, Epoch=28, LR=1e-8, Train_Loss=0.0362]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████████████| 63/63 [00:03<00:00, 19.78it/s, Epoch=28, LR=1e-8, Valid_Loss=2.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.600\n",
      "Precision: 0.613\n",
      "Recall: 0.600\n",
      "F_score: 0.605\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.61      0.64       190\n",
      "           1       0.42      0.51      0.46       113\n",
      "           2       0.67      0.64      0.65       197\n",
      "\n",
      "    accuracy                           0.60       500\n",
      "   macro avg       0.59      0.59      0.58       500\n",
      "weighted avg       0.61      0.60      0.60       500\n",
      "\n",
      "(0.6, 0.613163675921887, 0.6, 0.6047876688967598)\n",
      "0.5846604573877301\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                           | 0/375 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████| 375/375 [00:35<00:00, 10.54it/s, Epoch=29, LR=1.71e-5, Train_Loss=0.0233]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████████| 63/63 [00:03<00:00, 19.72it/s, Epoch=29, LR=1.71e-5, Valid_Loss=2.33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.570\n",
      "Precision: 0.594\n",
      "Recall: 0.570\n",
      "F_score: 0.577\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.60      0.63       190\n",
      "           1       0.39      0.54      0.45       113\n",
      "           2       0.65      0.56      0.60       197\n",
      "\n",
      "    accuracy                           0.57       500\n",
      "   macro avg       0.57      0.57      0.56       500\n",
      "weighted avg       0.59      0.57      0.58       500\n",
      "\n",
      "(0.57, 0.5941948722745318, 0.57, 0.5772431274937466)\n",
      "0.5597659514153474\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████| 375/375 [00:35<00:00, 10.54it/s, Epoch=30, LR=1e-5, Train_Loss=0.0498]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████████| 63/63 [00:03<00:00, 19.83it/s, Epoch=30, LR=1e-5, Valid_Loss=2.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.582\n",
      "Precision: 0.580\n",
      "Recall: 0.582\n",
      "F_score: 0.581\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.66      0.64       190\n",
      "           1       0.43      0.41      0.42       113\n",
      "           2       0.62      0.61      0.62       197\n",
      "\n",
      "    accuracy                           0.58       500\n",
      "   macro avg       0.56      0.56      0.56       500\n",
      "weighted avg       0.58      0.58      0.58       500\n",
      "\n",
      "(0.582, 0.5796329717689216, 0.582, 0.580560372960373)\n",
      "0.5581973581973582\n",
      "\n",
      "Training complete in 0h 19m 30s\n",
      "Best Loss: 0.6055\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for fold in range(0, CONFIG['n_fold']):\n",
    "#     print(f\"====== Fold: {fold} ======\")\n",
    "\n",
    "    \n",
    "# Create Dataloaders\n",
    "train_loader, valid_loader = prepare_loaders(train,valid)\n",
    "\n",
    "model = NADIModel('aubmindlab/araelectra-base-discriminator')\n",
    "model.to(CONFIG['device'])\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Define Optimizer and Scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
    "scheduler = fetch_scheduler(optimizer)\n",
    "\n",
    "model, history = run_training(model, optimizer, scheduler,\n",
    "                              device=CONFIG['device'],\n",
    "                              num_epochs=CONFIG['epochs'],\n",
    "                              fold=0,train_loader=train_loader, valid_loader=valid_loader )\n",
    "\n",
    "\n",
    "del model, history, train_loader, valid_loader\n",
    "_ = gc.collect()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dsfgh' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_65363/815264782.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdsfgh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dsfgh' is not defined"
     ]
    }
   ],
   "source": [
    "dsfgh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at aubmindlab/araelectra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU: NVIDIA GeForce RTX 3080 Ti\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                           | 0/375 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████| 375/375 [00:35<00:00, 10.64it/s, Epoch=1, LR=2.94e-6, Train_Loss=0.93]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████████| 63/63 [00:03<00:00, 19.79it/s, Epoch=1, LR=2.94e-6, Valid_Loss=0.909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.606\n",
      "Precision: 0.600\n",
      "Recall: 0.606\n",
      "F_score: 0.600\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.73      0.67       190\n",
      "           1       0.48      0.39      0.43       113\n",
      "           2       0.64      0.61      0.63       197\n",
      "\n",
      "    accuracy                           0.61       500\n",
      "   macro avg       0.58      0.58      0.58       500\n",
      "weighted avg       0.60      0.61      0.60       500\n",
      "\n",
      "(0.606, 0.5998817478817479, 0.606, 0.6003106181686626)\n",
      "0.5771620953027573\n",
      "Validation Loss Improved (0 ---> 0.5771620953027573)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 375/375 [00:35<00:00, 10.57it/s, Epoch=2, LR=1e-5, Train_Loss=0.734]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████████████| 63/63 [00:03<00:00, 20.01it/s, Epoch=2, LR=1e-5, Valid_Loss=1.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.550\n",
      "Precision: 0.629\n",
      "Recall: 0.550\n",
      "F_score: 0.561\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.58      0.63       190\n",
      "           1       0.35      0.70      0.47       113\n",
      "           2       0.72      0.44      0.54       197\n",
      "\n",
      "    accuracy                           0.55       500\n",
      "   macro avg       0.59      0.57      0.55       500\n",
      "weighted avg       0.63      0.55      0.56       500\n",
      "\n",
      "(0.55, 0.6293592380926941, 0.55, 0.5609593907838124)\n",
      "0.5489086002508089\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|██████████████████████████████████████████| 375/375 [00:35<00:00, 10.51it/s, Epoch=3, LR=1.71e-5, Train_Loss=0.589]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 63/63 [00:03<00:00, 19.85it/s, Epoch=3, LR=1.71e-5, Valid_Loss=1.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.584\n",
      "Precision: 0.609\n",
      "Recall: 0.584\n",
      "F_score: 0.583\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.65       190\n",
      "           1       0.43      0.50      0.47       113\n",
      "           2       0.73      0.48      0.58       197\n",
      "\n",
      "    accuracy                           0.58       500\n",
      "   macro avg       0.58      0.57      0.57       500\n",
      "weighted avg       0.61      0.58      0.58       500\n",
      "\n",
      "(0.584, 0.6090433977786919, 0.584, 0.5826869781231535)\n",
      "0.5668504950924323\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 375/375 [00:35<00:00, 10.52it/s, Epoch=4, LR=1e-8, Train_Loss=0.348]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████████████| 63/63 [00:03<00:00, 19.85it/s, Epoch=4, LR=1e-8, Valid_Loss=1.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.608\n",
      "Precision: 0.605\n",
      "Recall: 0.608\n",
      "F_score: 0.605\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.62      0.64       190\n",
      "           1       0.47      0.42      0.44       113\n",
      "           2       0.62      0.71      0.66       197\n",
      "\n",
      "    accuracy                           0.61       500\n",
      "   macro avg       0.59      0.58      0.58       500\n",
      "weighted avg       0.61      0.61      0.60       500\n",
      "\n",
      "(0.608, 0.605470101010101, 0.608, 0.6047893719438512)\n",
      "0.5823242473654472\n",
      "Validation Loss Improved (0.5771620953027573 ---> 0.5823242473654472)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|██████████████████████████████████████████| 375/375 [00:35<00:00, 10.56it/s, Epoch=5, LR=1.71e-5, Train_Loss=0.214]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 63/63 [00:03<00:00, 19.70it/s, Epoch=5, LR=1.71e-5, Valid_Loss=1.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.570\n",
      "Precision: 0.588\n",
      "Recall: 0.570\n",
      "F_score: 0.574\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.54      0.60       190\n",
      "           1       0.39      0.48      0.43       113\n",
      "           2       0.60      0.65      0.63       197\n",
      "\n",
      "    accuracy                           0.57       500\n",
      "   macro avg       0.56      0.56      0.55       500\n",
      "weighted avg       0.59      0.57      0.57       500\n",
      "\n",
      "(0.57, 0.5878313473977838, 0.57, 0.5739206074677664)\n",
      "0.5538554688658373\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 375/375 [00:35<00:00, 10.53it/s, Epoch=6, LR=1e-5, Train_Loss=0.196]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████████████| 63/63 [00:03<00:00, 19.81it/s, Epoch=6, LR=1e-5, Valid_Loss=1.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.580\n",
      "Precision: 0.605\n",
      "Recall: 0.580\n",
      "F_score: 0.588\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.59      0.64       190\n",
      "           1       0.38      0.52      0.44       113\n",
      "           2       0.66      0.60      0.63       197\n",
      "\n",
      "    accuracy                           0.58       500\n",
      "   macro avg       0.57      0.57      0.57       500\n",
      "weighted avg       0.60      0.58      0.59       500\n",
      "\n",
      "(0.58, 0.604557119582926, 0.58, 0.5880648654586372)\n",
      "0.5676376402442526\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|██████████████████████████████████████████| 375/375 [00:35<00:00, 10.53it/s, Epoch=7, LR=2.94e-6, Train_Loss=0.103]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 63/63 [00:03<00:00, 19.82it/s, Epoch=7, LR=2.94e-6, Valid_Loss=1.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.612\n",
      "Precision: 0.611\n",
      "Recall: 0.612\n",
      "F_score: 0.611\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.71      0.68       190\n",
      "           1       0.43      0.42      0.43       113\n",
      "           2       0.67      0.63      0.65       197\n",
      "\n",
      "    accuracy                           0.61       500\n",
      "   macro avg       0.59      0.59      0.59       500\n",
      "weighted avg       0.61      0.61      0.61       500\n",
      "\n",
      "(0.612, 0.6112338348200417, 0.612, 0.6111147183127332)\n",
      "0.586008284352015\n",
      "Validation Loss Improved (0.5823242473654472 ---> 0.586008284352015)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 375/375 [00:35<00:00, 10.57it/s, Epoch=8, LR=2e-5, Train_Loss=0.121]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████████████| 63/63 [00:03<00:00, 20.03it/s, Epoch=8, LR=2e-5, Valid_Loss=1.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.596\n",
      "Precision: 0.591\n",
      "Recall: 0.596\n",
      "F_score: 0.591\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.64      0.65       190\n",
      "           1       0.45      0.36      0.40       113\n",
      "           2       0.59      0.69      0.64       197\n",
      "\n",
      "    accuracy                           0.60       500\n",
      "   macro avg       0.57      0.56      0.56       500\n",
      "weighted avg       0.59      0.60      0.59       500\n",
      "\n",
      "(0.596, 0.5912598866441662, 0.596, 0.5909517529128134)\n",
      "0.5648374969833131\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                           | 0/375 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|██████████████████████████████████████████| 375/375 [00:35<00:00, 10.56it/s, Epoch=9, LR=2.94e-6, Train_Loss=0.119]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 63/63 [00:03<00:00, 20.06it/s, Epoch=9, LR=2.94e-6, Valid_Loss=1.63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.592\n",
      "Precision: 0.587\n",
      "Recall: 0.592\n",
      "F_score: 0.586\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.61      0.64       190\n",
      "           1       0.39      0.33      0.36       113\n",
      "           2       0.61      0.73      0.66       197\n",
      "\n",
      "    accuracy                           0.59       500\n",
      "   macro avg       0.56      0.55      0.55       500\n",
      "weighted avg       0.59      0.59      0.59       500\n",
      "\n",
      "(0.592, 0.5869888525911626, 0.592, 0.5859213441777956)\n",
      "0.5541468167635454\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████| 375/375 [00:35<00:00, 10.54it/s, Epoch=10, LR=1e-5, Train_Loss=0.0414]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████████| 63/63 [00:03<00:00, 19.67it/s, Epoch=10, LR=1e-5, Valid_Loss=1.73]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.596\n",
      "Precision: 0.582\n",
      "Recall: 0.596\n",
      "F_score: 0.584\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67       190\n",
      "           1       0.41      0.27      0.33       113\n",
      "           2       0.60      0.71      0.65       197\n",
      "\n",
      "    accuracy                           0.60       500\n",
      "   macro avg       0.56      0.55      0.55       500\n",
      "weighted avg       0.58      0.60      0.58       500\n",
      "\n",
      "(0.596, 0.5821367375886524, 0.596, 0.5839022852639875)\n",
      "0.5487854782740934\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████| 375/375 [00:35<00:00, 10.53it/s, Epoch=11, LR=1.71e-5, Train_Loss=0.0686]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████████| 63/63 [00:03<00:00, 19.86it/s, Epoch=11, LR=1.71e-5, Valid_Loss=1.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.590\n",
      "Precision: 0.595\n",
      "Recall: 0.590\n",
      "F_score: 0.590\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.68      0.64       190\n",
      "           1       0.46      0.49      0.47       113\n",
      "           2       0.66      0.56      0.60       197\n",
      "\n",
      "    accuracy                           0.59       500\n",
      "   macro avg       0.58      0.58      0.57       500\n",
      "weighted avg       0.59      0.59      0.59       500\n",
      "\n",
      "(0.59, 0.5948158610917644, 0.59, 0.5898414959912057)\n",
      "0.5740326306219102\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████| 375/375 [00:35<00:00, 10.54it/s, Epoch=12, LR=1e-8, Train_Loss=0.0444]\n",
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████████| 63/63 [00:03<00:00, 19.79it/s, Epoch=12, LR=1e-8, Valid_Loss=1.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.622\n",
      "Precision: 0.615\n",
      "Recall: 0.622\n",
      "F_score: 0.617\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.69      0.68       190\n",
      "           1       0.48      0.39      0.43       113\n",
      "           2       0.64      0.69      0.66       197\n",
      "\n",
      "    accuracy                           0.62       500\n",
      "   macro avg       0.60      0.59      0.59       500\n",
      "weighted avg       0.61      0.62      0.62       500\n",
      "\n",
      "(0.622, 0.6147060041407867, 0.622, 0.6169472565531906)\n",
      "0.5910237757358702\n",
      "Validation Loss Improved (0.586008284352015 ---> 0.5910237757358702)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████| 375/375 [00:35<00:00, 10.57it/s, Epoch=13, LR=1.71e-5, Train_Loss=0.0231]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████| 63/63 [00:03<00:00, 19.93it/s, Epoch=13, LR=1.71e-5, Valid_Loss=1.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.622\n",
      "Precision: 0.620\n",
      "Recall: 0.622\n",
      "F_score: 0.621\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.69      0.68       190\n",
      "           1       0.46      0.44      0.45       113\n",
      "           2       0.66      0.65      0.66       197\n",
      "\n",
      "    accuracy                           0.62       500\n",
      "   macro avg       0.60      0.60      0.60       500\n",
      "weighted avg       0.62      0.62      0.62       500\n",
      "\n",
      "(0.622, 0.6198643235071807, 0.622, 0.6208153050455271)\n",
      "0.5976383537451077\n",
      "Validation Loss Improved (0.5910237757358702 ---> 0.5976383537451077)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                           | 0/375 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████| 375/375 [00:35<00:00, 10.58it/s, Epoch=14, LR=1e-5, Train_Loss=0.0701]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████████| 63/63 [00:03<00:00, 19.73it/s, Epoch=14, LR=1e-5, Valid_Loss=2.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.610\n",
      "Precision: 0.619\n",
      "Recall: 0.610\n",
      "F_score: 0.600\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.82      0.68       190\n",
      "           1       0.51      0.36      0.42       113\n",
      "           2       0.72      0.55      0.63       197\n",
      "\n",
      "    accuracy                           0.61       500\n",
      "   macro avg       0.60      0.58      0.58       500\n",
      "weighted avg       0.62      0.61      0.60       500\n",
      "\n",
      "(0.61, 0.6185817771578702, 0.61, 0.5995471053245717)\n",
      "0.5753243630589453\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████| 375/375 [00:35<00:00, 10.55it/s, Epoch=15, LR=2.94e-6, Train_Loss=0.0232]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████████| 63/63 [00:03<00:00, 19.73it/s, Epoch=15, LR=2.94e-6, Valid_Loss=1.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.626\n",
      "Precision: 0.623\n",
      "Recall: 0.626\n",
      "F_score: 0.624\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.72      0.69       190\n",
      "           1       0.49      0.45      0.47       113\n",
      "           2       0.66      0.64      0.65       197\n",
      "\n",
      "    accuracy                           0.63       500\n",
      "   macro avg       0.60      0.60      0.60       500\n",
      "weighted avg       0.62      0.63      0.62       500\n",
      "\n",
      "(0.626, 0.6230209922712541, 0.626, 0.6239750518632606)\n",
      "0.6025765914295311\n",
      "Validation Loss Improved (0.5976383537451077 ---> 0.6025765914295311)\n",
      "Model Saved\n",
      "\n",
      "Training complete in 0h 9m 50s\n",
      "Best Loss: 0.6026\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for fold in range(0, CONFIG['n_fold']):\n",
    "#     print(f\"====== Fold: {fold} ======\")\n",
    "\n",
    "    \n",
    "# Create Dataloaders\n",
    "train_loader, valid_loader = prepare_loaders(train,valid)\n",
    "\n",
    "model = NADIModel('aubmindlab/araelectra-base-discriminator')\n",
    "model.to(CONFIG['device'])\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Define Optimizer and Scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
    "scheduler = fetch_scheduler(optimizer)\n",
    "\n",
    "model, history = run_training(model, optimizer, scheduler,\n",
    "                              device=CONFIG['device'],\n",
    "                              num_epochs=CONFIG['epochs'],\n",
    "                              fold=0,train_loader=train_loader, valid_loader=valid_loader )\n",
    "\n",
    "\n",
    "del model, history, train_loader, valid_loader\n",
    "_ = gc.collect()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 689,
     "referenced_widgets": [
      "cdef05bc07fa455188c387559708971d",
      "0008fbdc931e439d9c229b814fa797e2",
      "87e3f328cdee4fff97cf3b619f185f29",
      "a21e734be7e64e01adbaf5cd8c955447",
      "072ab38f07314a2b8a873261c3c9942b",
      "df68f730f618418697ed5c689153782e",
      "5d9565bcee2c4ff38cba0dd8d9b8af74",
      "31cb97c304fc4ea7a3960dc32b5c3640",
      "2a5065b3872e417991c3f25ba02726d9",
      "29256e6c19fc4a8081a05b81774aa781",
      "586b83f574854f588085805f3a8455df"
     ]
    },
    "executionInfo": {
     "elapsed": 11795142,
     "status": "ok",
     "timestamp": 1661462425355,
     "user": {
      "displayName": "Reem Elsayed",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "RdoS6yH5nwWG",
    "outputId": "831f7280-5ee1-44d8-fff8-5c0a2955bd41"
   },
   "outputs": [],
   "source": [
    "# for fold in range(0, CONFIG['n_fold']):\n",
    "#     print(f\"====== Fold: {fold} ======\")\n",
    "\n",
    "    \n",
    "# Create Dataloaders\n",
    "train_loader, valid_loader = prepare_loaders(train,valid)\n",
    "\n",
    "model = NADIModel('aubmindlab/araelectra-base-discriminator')\n",
    "model.to(CONFIG['device'])\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Define Optimizer and Scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
    "scheduler = fetch_scheduler(optimizer)\n",
    "\n",
    "model, history = run_training(model, optimizer, scheduler,\n",
    "                              device=CONFIG['device'],\n",
    "                              num_epochs=CONFIG['epochs'],\n",
    "                              fold=0,train_loader=train_loader, valid_loader=valid_loader )\n",
    "\n",
    "\n",
    "del model, history, train_loader, valid_loader\n",
    "_ = gc.collect()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfghjhgfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NADIModel(nn.Module):\n",
    "  def __init__(self, pretrained_path='aubmindlab/bert-base-arabert',in_feature=768, class_num=3, dropout_prob=0.2):\n",
    "        super(NADIModel, self).__init__()\n",
    "        self.base_model=TransformerLayer(pretrained_path).to('cuda')\n",
    "        self.att=Attention(self.base_model.output_num(), 512).to('cuda')\n",
    "        self.classifier=nn.Linear(self.base_model.output_num(), class_num)\n",
    "  def forward(self, ids,mask):\n",
    "    output=self.base_model(ids,mask)\n",
    "#     output=self.att(output.last_hidden_state)\n",
    "#     output=self.classifier(output)\n",
    "    # output=torch.softmax(output, dim=1)\n",
    "    return output\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at aubmindlab/araelectra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NADIModel('aubmindlab/araelectra-base-discriminator')\n",
    "model.to(CONFIG['device'])\n",
    "model.load_state_dict(torch.load('/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/araelectra/Loss.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new model class, takes a list of strings as input and outputs the binary classification probability p(good | review) / (p(good | review) + p(bad | review))\n",
    "class gpt_prompt(torch.nn.Module):\n",
    "    def __init__(self, gpt_model, tokenizer, prompt='ما هو شعور الكاتب؟', pos_token=\"good\", neg_token=\"bad\", neu_token=\"neu\"):\n",
    "        super().__init__()\n",
    "        self.gpt = gpt_model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompt = prompt\n",
    "        self.prompt_length = len(tokenizer.encode(prompt))\n",
    "        self.pos_token = tokenizer.encode(pos_token)[0]\n",
    "        self.neg_token = tokenizer.encode(neg_token)[0]\n",
    "        self.neutral_token = tokenizer.encode(neu_token)[0]\n",
    "        self.max_length=512\n",
    "        \n",
    "    def forward(self, sentences):\n",
    "        # Loop through and add prompts to the sentences, then perform prediction\n",
    "        preds = []\n",
    "        for string in sentences:\n",
    "            length = len(self.tokenizer.encode(string))\n",
    "            if length + self.prompt_length <= self.max_length:\n",
    "                encodings_dict = self.tokenizer.encode_plus(string + \" \" + self.prompt, truncation=True, max_length=self.max_length, padding=\"max_length\")\n",
    "                prompt_end_idx = length + self.prompt_length - 1\n",
    "            else: # encode full sentence then replace the (truncated) end with the prompt\n",
    "                encodings_dict = self.tokenizer.encode_plus(string, truncation=True, max_length=self.max_length, padding=\"max_length\")\n",
    "                encodings_dict[\"input_ids\"][-self.prompt_length:] = self.tokenizer.encode(self.prompt)\n",
    "                prompt_end_idx = self.max_length - 1\n",
    "            preds.append(self.predict(encodings_dict, prompt_end_idx))\n",
    "            # Tests\n",
    "            # print(self.tokenizer.decode(encodings_dict[\"input_ids\"]))\n",
    "            #print(prompt_end_idx, self.tokenizer.decode(encodings_dict[\"input_ids\"][prompt_end_idx]))\n",
    "            assert self.tokenizer.decode(encodings_dict[\"input_ids\"][prompt_end_idx]) != \"<|endoftext|>\", \"Prompt_end_idx points to a padding token not the prompt\"\n",
    "        return preds\n",
    "    \n",
    "    # given an encoded sentence + prompt and the end of the prompt return binary classification probability\n",
    "    def predict(self, encodings_dict, prompt_end_idx):\n",
    "        input_ids = torch.tensor([encodings_dict[\"input_ids\"]]).long().to(device)\n",
    "        attn_mask = torch.tensor([encodings_dict[\"attention_mask\"]]).long().to(device)\n",
    "        word_preds = self.gpt(input_ids, attn_mask)[0][0, prompt_end_idx, :]\n",
    "        word_preds = torch.nn.functional.softmax(word_preds)\n",
    "        pos_pred=word_preds[self.pos_token]/(word_preds[self.pos_token] + word_preds[self.neg_token]+word_preds[self.neutral_token])\n",
    "        neu_pred=word_preds[self.neutral_token]/(word_preds[self.pos_token] + word_preds[self.neg_token]+word_preds[self.neutral_token])\n",
    "        neg_pred=word_preds[self.neg_token]/(word_preds[self.pos_token] + word_preds[self.neg_token]+word_preds[self.neutral_token])\n",
    "#         print(type(pos_pred))\n",
    "        \n",
    "        return torch.stack(((neg_pred),(neu_pred),(pos_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbalizer = {i: [l] for (i, l) in enumerate(le.classes_)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['neg'], 1: ['neut'], 2: ['pos']}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataset class for prompted BERT fine tuning\n",
    "# The dataset takes in query and passage, and then construct a training sample as:  <prompt> + [MASK] + <text>, returning the position of the mask\n",
    "# It also performs padding and stores the labels\n",
    "class BERTPromptDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, labels, tokenizer, prompt=\"ما هو شعور الكاتب؟\", max_length=512):\n",
    "        self.text=dataset['#2_content']\n",
    "        self.labels=labels\n",
    "        # Construct the input sentence\n",
    "#         input_sentences = [\"{} {} {}\".format(prompt, tokenizer.mask_token, text) for _, (text) in dataset.iterrows()]\n",
    "        \n",
    "#         # Encode and store\n",
    "#         encodings_dict = tokenizer.batch_encode_plus(input_sentences, truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "#         self.input_ids = encodings_dict['input_ids']\n",
    "#         self.attn_masks = encodings_dict['attention_mask']\n",
    "#         self.labels = labels\n",
    "\n",
    "#         # Calculate the position of the mask using self.input_ids\n",
    "#         mask_id = tokenizer.encode(tokenizer.mask_token)[1] # 103\n",
    "#         self.mask_pos = [sent_ids.index(mask_id) for sent_ids in self.input_ids]\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return_dict = {\"text\": self.text[idx],                       \n",
    "                       \"labels\": torch.tensor(self.labels[idx]).float()} \n",
    "        return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, dataloader, device, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    prediction=[]\n",
    "    true_prediction=[]\n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, data in bar:        \n",
    "        \n",
    "        text_ids = data['text']\n",
    "#         text_mask = data['text_mask'].to(device, dtype = torch.long)\n",
    "#         mask_pos =data['mask_pos'].to(device, dtype = torch.long)\n",
    "        targets = torch.tensor(data['labels']).to(device, dtype=torch.long)\n",
    "        \n",
    "        batch_size = len(text_ids)\n",
    "\n",
    "        outputs = model(text_ids)\n",
    "        \n",
    "        probs = [t.detach().to('cpu').numpy() for t in outputs]\n",
    "        probs=torch.tensor(probs)\n",
    "        \n",
    "        prediction.append(probs.detach().numpy())\n",
    "        true_prediction.append(targets.to('cpu').numpy())\n",
    "\n",
    "        # outputs = outputs.argmax(dim=1)\n",
    "        probs = [t.detach().cpu().numpy() for t in outputs]\n",
    "        probs=torch.tensor(probs).cuda()\n",
    "        loss = criterion(probs, targets)\n",
    "        \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        del probs,outputs\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        \n",
    "        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n",
    "                        LR=optimizer.param_groups[0]['lr']) \n",
    "    \n",
    "    gc.collect()\n",
    "#     print(np.array(true_prediction).shape)\n",
    "    prediction=np.argmax(np.array(prediction)[0],axis=1)\n",
    "    print(print_statistics(np.array(true_prediction)[0],prediction))\n",
    "    print(f1_score(np.array(true_prediction)[0],prediction, average='macro'))\n",
    "\n",
    "    \n",
    "    return epoch_loss, f1_score(np.array(true_prediction)[0],prediction, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, data in bar:\n",
    "        \n",
    "        text_ids = data['text']\n",
    "        targets = torch.tensor(data['labels']).to(device, dtype=torch.long)\n",
    "        \n",
    "        batch_size = len(text_ids)\n",
    "        # print(targets)\n",
    "\n",
    "        outputs = model(text_ids)\n",
    "        probs = [t.detach().cpu().numpy() for t in outputs]\n",
    "        probs=torch.tensor(probs).requires_grad_().cuda()\n",
    "#         print(outputs[0].shape)\n",
    "#         print(outputs[0][0])\n",
    "        \n",
    "#         print(outputs)\n",
    "\n",
    "        \n",
    "        loss = criterion(probs, targets)\n",
    "        loss = loss / CONFIG['n_accumulate']\n",
    "        loss.backward()\n",
    "        del probs,outputs\n",
    "    \n",
    "        if (step + 1) % CONFIG['n_accumulate'] == 0:\n",
    "            optimizer.step()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "                \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        \n",
    "        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n",
    "                        LR=optimizer.param_groups[0]['lr'])\n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model, optimizer, scheduler, device, num_epochs, train_loader, valid_loader):\n",
    "    # To automatically log gradients\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
    "    \n",
    "    start = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_epoch_loss = 0\n",
    "    history = defaultdict(list)\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1): \n",
    "        gc.collect()\n",
    "        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, \n",
    "                                           dataloader=train_loader, \n",
    "                                           device=CONFIG['device'], epoch=epoch)\n",
    "        \n",
    "        val_epoch_loss,f1_score = valid_one_epoch(model, valid_loader, device=CONFIG['device'], \n",
    "                                         epoch=epoch)\n",
    "    \n",
    "        history['Train Loss'].append(train_epoch_loss)\n",
    "        history['Valid Loss'].append(val_epoch_loss)\n",
    "        \n",
    "       \n",
    "        \n",
    "        # deep copy the model\n",
    "        if f1_score >= best_epoch_loss:\n",
    "            print(f\"Validation Loss Improved ({best_epoch_loss} ---> {f1_score})\")\n",
    "            best_epoch_loss = f1_score\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            PATH = \"/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/araelectra/Prompt_Loss.bin\"\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            # Save a model file from the current directory\n",
    "            print(\"Model Saved\")\n",
    "            \n",
    "        print()\n",
    "    \n",
    "    end = time.time()\n",
    "    time_elapsed = end - start\n",
    "    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
    "    print(\"Best Loss: {:.4f}\".format(best_epoch_loss))\n",
    "    \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_loaders(train,valid):\n",
    "   \n",
    "    \n",
    "    train_dataset = BERTPromptDataset(train[['#2_content']], train['#3_label'], tokenizer)\n",
    "    valid_dataset = BERTPromptDataset(valid[['#2_content']], valid['#3_label'], tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], \n",
    "                              num_workers=2, shuffle=True, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], \n",
    "                              num_workers=2, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    return train_loader, valid_loader\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(outputs1,  targets):\n",
    "\n",
    "    criterion = F1_Loss()\n",
    "    criterion_2 = FocalLoss()\n",
    "    loss = 0.5*criterion(outputs1, targets)+0.5*criterion_2(outputs1, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda:0\"\n",
    "prompted = gpt_prompt(model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.iloc[fine_tune_idx].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU: NVIDIA GeForce RTX 3080 Ti\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/250 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|██████████████████████████████████████████████▊| 249/250 [00:12<00:00, 22.01it/s, Epoch=1, LR=1e-5, Train_Loss=1.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 250/250 [00:12<00:00, 20.51it/s, Epoch=1, LR=1e-5, Train_Loss=1.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      " 97%|███████████████████████████████████████████████▍ | 61/63 [00:04<00:00, 15.02it/s, Epoch=1, LR=1e-5, Valid_Loss=1.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 63/63 [00:04<00:00, 14.07it/s, Epoch=1, LR=1e-5, Valid_Loss=1.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.250\n",
      "Precision: 0.062\n",
      "Recall: 0.250\n",
      "F_score: 0.100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      1.00      0.40         2\n",
      "           1       0.00      0.00      0.00         3\n",
      "           2       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.25         8\n",
      "   macro avg       0.08      0.33      0.13         8\n",
      "weighted avg       0.06      0.25      0.10         8\n",
      "\n",
      "(0.25, 0.0625, 0.25, 0.1)\n",
      "0.13333333333333333\n",
      "Validation Loss Improved (0 ---> 0.13333333333333333)\n",
      "Model Saved\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/250 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████████| 250/250 [00:11<00:00, 21.08it/s, Epoch=2, LR=1e-8, Train_Loss=1.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 250/250 [00:11<00:00, 21.07it/s, Epoch=2, LR=1e-8, Train_Loss=1.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/63 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████████████| 63/63 [00:04<00:00, 14.57it/s, Epoch=2, LR=1e-8, Valid_Loss=1.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.250\n",
      "Precision: 0.062\n",
      "Recall: 0.250\n",
      "F_score: 0.100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      1.00      0.40         2\n",
      "           1       0.00      0.00      0.00         3\n",
      "           2       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.25         8\n",
      "   macro avg       0.08      0.33      0.13         8\n",
      "weighted avg       0.06      0.25      0.10         8\n",
      "\n",
      "(0.25, 0.0625, 0.25, 0.1)\n",
      "0.13333333333333333\n",
      "Validation Loss Improved (0.13333333333333333 ---> 0.13333333333333333)\n",
      "Model Saved\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/250 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      " 29%|█████████████▏                               | 73/250 [00:03<00:08, 19.99it/s, Epoch=3, LR=1.04e-6, Train_Loss=1.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_143328/3479905228.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m model, history = run_training(prompted, optimizer, scheduler,\n\u001b[0m\u001b[1;32m     16\u001b[0m                               \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                               \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_143328/2311814640.py\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(model, optimizer, scheduler, device, num_epochs, train_loader, valid_loader)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         train_epoch_loss = train_one_epoch(model, optimizer, scheduler, \n\u001b[0m\u001b[1;32m     15\u001b[0m                                            \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                            device=CONFIG['device'], epoch=epoch)\n",
      "\u001b[0;32m/tmp/ipykernel_143328/2261367203.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, scheduler, dataloader, device, epoch)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# print(targets)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_143328/1238159131.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mencodings_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mprompt_end_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencodings_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt_end_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0;31m# Tests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# print(self.tokenizer.decode(encodings_dict[\"input_ids\"]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_143328/1238159131.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, encodings_dict, prompt_end_idx)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencodings_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mattn_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencodings_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mword_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt_end_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mword_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mpos_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_token\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mword_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneg_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mword_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneutral_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_143328/2177000523.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, ids, mask)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#     output=self.att(output.last_hidden_state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#     output=self.classifier(output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_143328/337059365.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         outputs = self.transformer(\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/electra/modeling_electra.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings_project\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m         hidden_states = self.encoder(\n\u001b[0m\u001b[1;32m    918\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/electra/modeling_electra.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    583\u001b[0m                 )\n\u001b[1;32m    584\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    586\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/electra/modeling_electra.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    471\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/electra/modeling_electra.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 397\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    398\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/electra/modeling_electra.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_probs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_samples = 1000\n",
    "fine_tune_idx = np.random.choice(len(train), size=n_samples)\n",
    "\n",
    "    \n",
    "# Create Dataloaders\n",
    "train_loader, valid_loader = prepare_loaders(train.iloc[fine_tune_idx].reset_index(),valid)\n",
    "\n",
    "prompted.to(CONFIG['device'])\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Define Optimizer and Scheduler\n",
    "optimizer = AdamW(prompted.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
    "scheduler = fetch_scheduler(optimizer)\n",
    "\n",
    "model, history = run_training(prompted, optimizer, scheduler,\n",
    "                              device=CONFIG['device'],\n",
    "                              num_epochs=CONFIG['epochs'],\n",
    "                              train_loader=train_loader, valid_loader=valid_loader )\n",
    "\n",
    "\n",
    "del model, history, train_loader, valid_loader\n",
    "_ = gc.collect()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "arabert_f1_loss_w_imablancesampler.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0008fbdc931e439d9c229b814fa797e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df68f730f618418697ed5c689153782e",
      "placeholder": "​",
      "style": "IPY_MODEL_5d9565bcee2c4ff38cba0dd8d9b8af74",
      "value": "Downloading pytorch_model.bin: 100%"
     }
    },
    "072ab38f07314a2b8a873261c3c9942b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0e1f58922f18474b9ec9d643d6b0f1ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "10e4fe6c4aed4a80affac3c6042a6ce5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b249d4c832e140d694d83894a9637422",
      "placeholder": "​",
      "style": "IPY_MODEL_38b99803966c432c86302422797bba7e",
      "value": "Downloading vocab.txt: 100%"
     }
    },
    "1d7ec87993334c619346ce807e86823d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "204ad3545a5e4d96a6b03f3919896372": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e7ad6e67202746078489d07c7d44a414",
      "placeholder": "​",
      "style": "IPY_MODEL_839916b14ac644e2a4dcca7d017fd37d",
      "value": "Downloading tokenizer.json: 100%"
     }
    },
    "2138396876d74dd1ba0d640ca25c395f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "219265426b01403e8a7cf4406edfd7a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "29256e6c19fc4a8081a05b81774aa781": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a5065b3872e417991c3f25ba02726d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2af5a19c66d04deba0c3a39d35a24104": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2b6187da777a4e11b56e651045f6cbc5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "31cb97c304fc4ea7a3960dc32b5c3640": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31e6aa7e72e24bfea2dd2579b0d5688a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "328bf80910d64d83b4abfcac3e356210": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2af5a19c66d04deba0c3a39d35a24104",
      "placeholder": "​",
      "style": "IPY_MODEL_a5b7a737240f4a8aa1a1b982f2cb0c53",
      "value": "Downloading tokenizer_config.json: 100%"
     }
    },
    "358d653088be4813a59bb1a585c6d2e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "358e50c17d6143f6801a1368506298a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fbcc7b929fd144a181ada4bc96e17417",
      "max": 2642361,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d887e3f942944c9094a0d62ef3f02479",
      "value": 2642361
     }
    },
    "38b99803966c432c86302422797bba7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3b5c9901d77a45c0a284323a6cdb9b24": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "538fa883a58044fd9b245db52e197927": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5823f3b1668a4a72a0f91bf9c1ef33f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_31e6aa7e72e24bfea2dd2579b0d5688a",
      "max": 824793,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fddda1530aa94361a6e25ad6b7dc5af1",
      "value": 824793
     }
    },
    "586b83f574854f588085805f3a8455df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5a558a58c20247c9adbf61a3c63465b4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b112eb5f0594e4095331a4d726d4b83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c027494a95904d64948d66ee8e75ff8a",
      "placeholder": "​",
      "style": "IPY_MODEL_3b5c9901d77a45c0a284323a6cdb9b24",
      "value": "Downloading special_tokens_map.json: 100%"
     }
    },
    "5ba8caae38b440e689367f38969c7204": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5d9565bcee2c4ff38cba0dd8d9b8af74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "61401f4e43a141ae851718badcc629b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1d7ec87993334c619346ce807e86823d",
      "placeholder": "​",
      "style": "IPY_MODEL_358d653088be4813a59bb1a585c6d2e0",
      "value": "Downloading config.json: 100%"
     }
    },
    "6f35345c87ba4b0089200b8329dfb22b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9bf8113c04b8402c8de9c8b7d893a05d",
      "placeholder": "​",
      "style": "IPY_MODEL_5ba8caae38b440e689367f38969c7204",
      "value": " 503/503 [00:00&lt;00:00, 15.8kB/s]"
     }
    },
    "723ccadde5784350bc08e70f42b94174": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73f300a46e9e42d2ac2529f7148dd16a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9787540fcb9c4a7e8a44241ee4fba6ef",
      "placeholder": "​",
      "style": "IPY_MODEL_f45c6b6be89645f48e5c933a3567405e",
      "value": " 112/112 [00:00&lt;00:00, 3.00kB/s]"
     }
    },
    "78a36e1614404f25a1183263d10731b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_204ad3545a5e4d96a6b03f3919896372",
       "IPY_MODEL_358e50c17d6143f6801a1368506298a3",
       "IPY_MODEL_98aa2e02bf554e3a8b2b66eab9fd4106"
      ],
      "layout": "IPY_MODEL_e3f8df364a2f4d8391a8e6192f69e139"
     }
    },
    "7a2f611e11cd4e3b9302d745fbaf3382": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_723ccadde5784350bc08e70f42b94174",
      "placeholder": "​",
      "style": "IPY_MODEL_2138396876d74dd1ba0d640ca25c395f",
      "value": " 392/392 [00:00&lt;00:00, 7.14kB/s]"
     }
    },
    "7b34bfe94a3843f5a814a00467c64f99": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_da1758be1c334a28a94b84e1631be0c6",
      "placeholder": "​",
      "style": "IPY_MODEL_2b6187da777a4e11b56e651045f6cbc5",
      "value": " 805k/805k [00:00&lt;00:00, 1.56MB/s]"
     }
    },
    "7f3883fd5022493c9da32650885e8e72": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "839916b14ac644e2a4dcca7d017fd37d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "858bfb98bf764279935a7db8af42ec39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a4bfb7c2831a4f48aeec04f2c7a041b8",
      "max": 112,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0e1f58922f18474b9ec9d643d6b0f1ba",
      "value": 112
     }
    },
    "87e3f328cdee4fff97cf3b619f185f29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_31cb97c304fc4ea7a3960dc32b5c3640",
      "max": 540862295,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2a5065b3872e417991c3f25ba02726d9",
      "value": 540862295
     }
    },
    "88a599f23423444794211f10b75a8e80": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9277ce48447c4f8e95a162f7e3d65620": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_328bf80910d64d83b4abfcac3e356210",
       "IPY_MODEL_f7b2eff8e7184a72ba727dfac6ad5806",
       "IPY_MODEL_7a2f611e11cd4e3b9302d745fbaf3382"
      ],
      "layout": "IPY_MODEL_88a599f23423444794211f10b75a8e80"
     }
    },
    "9787540fcb9c4a7e8a44241ee4fba6ef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "97d4c78c87834f4286ef2cf231b5cbc2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98aa2e02bf554e3a8b2b66eab9fd4106": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b0279fd09c7b4e92922f7cf99f87810e",
      "placeholder": "​",
      "style": "IPY_MODEL_219265426b01403e8a7cf4406edfd7a2",
      "value": " 2.52M/2.52M [00:00&lt;00:00, 2.39MB/s]"
     }
    },
    "9bf8113c04b8402c8de9c8b7d893a05d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9d24316575d94223b837d7b2f8f63bf7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a21e734be7e64e01adbaf5cd8c955447": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_29256e6c19fc4a8081a05b81774aa781",
      "placeholder": "​",
      "style": "IPY_MODEL_586b83f574854f588085805f3a8455df",
      "value": " 516M/516M [00:09&lt;00:00, 52.3MB/s]"
     }
    },
    "a4bfb7c2831a4f48aeec04f2c7a041b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5b7a737240f4a8aa1a1b982f2cb0c53": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b0279fd09c7b4e92922f7cf99f87810e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b249d4c832e140d694d83894a9637422": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c027494a95904d64948d66ee8e75ff8a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3956996f80a49b7b46b978274b32ef0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_61401f4e43a141ae851718badcc629b9",
       "IPY_MODEL_d4b408519ae24d868ee8b95d45ee707c",
       "IPY_MODEL_6f35345c87ba4b0089200b8329dfb22b"
      ],
      "layout": "IPY_MODEL_7f3883fd5022493c9da32650885e8e72"
     }
    },
    "cae94cfcd22d4eabbe366a54c9530c4f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cdef05bc07fa455188c387559708971d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0008fbdc931e439d9c229b814fa797e2",
       "IPY_MODEL_87e3f328cdee4fff97cf3b619f185f29",
       "IPY_MODEL_a21e734be7e64e01adbaf5cd8c955447"
      ],
      "layout": "IPY_MODEL_072ab38f07314a2b8a873261c3c9942b"
     }
    },
    "d4b408519ae24d868ee8b95d45ee707c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cae94cfcd22d4eabbe366a54c9530c4f",
      "max": 503,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_538fa883a58044fd9b245db52e197927",
      "value": 503
     }
    },
    "d887e3f942944c9094a0d62ef3f02479": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "da1758be1c334a28a94b84e1631be0c6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc630edf4f224a2e973dfed484204ff1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dde1d50f4f3a42bfa0c9edf43a7006d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5b112eb5f0594e4095331a4d726d4b83",
       "IPY_MODEL_858bfb98bf764279935a7db8af42ec39",
       "IPY_MODEL_73f300a46e9e42d2ac2529f7148dd16a"
      ],
      "layout": "IPY_MODEL_9d24316575d94223b837d7b2f8f63bf7"
     }
    },
    "df619d778c7442a98215e48b2a0ac0db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_10e4fe6c4aed4a80affac3c6042a6ce5",
       "IPY_MODEL_5823f3b1668a4a72a0f91bf9c1ef33f5",
       "IPY_MODEL_7b34bfe94a3843f5a814a00467c64f99"
      ],
      "layout": "IPY_MODEL_5a558a58c20247c9adbf61a3c63465b4"
     }
    },
    "df68f730f618418697ed5c689153782e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3f8df364a2f4d8391a8e6192f69e139": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e7ad6e67202746078489d07c7d44a414": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f45c6b6be89645f48e5c933a3567405e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f7b2eff8e7184a72ba727dfac6ad5806": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_97d4c78c87834f4286ef2cf231b5cbc2",
      "max": 392,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_dc630edf4f224a2e973dfed484204ff1",
      "value": 392
     }
    },
    "fbcc7b929fd144a181ada4bc96e17417": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fddda1530aa94361a6e25ad6b7dc5af1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
