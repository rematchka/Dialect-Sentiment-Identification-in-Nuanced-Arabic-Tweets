{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32663,
     "status": "ok",
     "timestamp": 1661185825577,
     "user": {
      "displayName": "pink panther",
      "userId": "06900797602742482056"
     },
     "user_tz": -120
    },
    "id": "vMmciFBRyRFe",
    "outputId": "5bc0bbe3-786b-499b-f92f-269e2c39a8e6"
   },
   "outputs": [],
   "source": [
    "# !pip install sentencepiece\n",
    "# !pip  install transformers\n",
    "# !pip install pytorch-ignite\n",
    "# !pip install datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5642,
     "status": "ok",
     "timestamp": 1661185831207,
     "user": {
      "displayName": "pink panther",
      "userId": "06900797602742482056"
     },
     "user_tz": -120
    },
    "id": "AtS4CD_krp3d"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import AutoModel\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "# Any results you write to the current directory are saved as output.\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "from transformers import BertTokenizer,BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from argparse import ArgumentParser\n",
    "# from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "# from ignite.metrics import Accuracy, Loss\n",
    "# from ignite.engine.engine import Engine, State, Events\n",
    "# from ignite.handlers import EarlyStopping\n",
    "# from ignite.contrib.handlers import TensorboardLogger, ProgressBar\n",
    "# from ignite.utils import convert_tensor\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')\n",
    "import gc\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "\n",
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Pytorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Utils\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Sklearn Imports\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW\n",
    "import random\n",
    "import os\n",
    "from urllib import request\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, BertTokenizer\n",
    "from transformers.data.processors import SingleSentenceClassificationProcessor\n",
    "from transformers import Trainer , TrainingArguments\n",
    "from transformers.trainer_utils import EvaluationStrategy\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARjNBDtgwfVJ"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1661185831208,
     "user": {
      "displayName": "pink panther",
      "userId": "06900797602742482056"
     },
     "user_tz": -120
    },
    "id": "lLRNRjusoELI"
   },
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length):\n",
    "        self.df = df\n",
    "        self.max_len = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text = df['#2_content'].values\n",
    "        self.label=df['#3_label'].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.text[index]\n",
    "        # summary = self.summary[index]\n",
    "        inputs_text = self.tokenizer.encode_plus(\n",
    "                                text,\n",
    "                                truncation=True,\n",
    "                                add_special_tokens=True,\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length'\n",
    "                            )\n",
    "        \n",
    "                            \n",
    "        target = self.label[index]\n",
    "        \n",
    "        text_ids = inputs_text['input_ids']\n",
    "        text_mask = inputs_text['attention_mask']\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        return {\n",
    "            \n",
    "            'text_ids': torch.tensor(text_ids, dtype=torch.long),\n",
    "            'text_mask': torch.tensor(text_mask, dtype=torch.long),\n",
    "            'target': torch.tensor(target, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1661185831208,
     "user": {
      "displayName": "pink panther",
      "userId": "06900797602742482056"
     },
     "user_tz": -120
    },
    "id": "0ZOanFa2wz3L"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# import Dataset\n",
    "# import os\n",
    "# import numpy as np\n",
    "# from emoji import UNICODE_EMOJI\n",
    "# import TweetNormalizer\n",
    "# import re\n",
    "# import text_normalization\n",
    "\n",
    "\n",
    "# dic = {\n",
    "#       \"egypt\": 'المصرية',\n",
    "# \t  \"nile\": 'المصرية',\n",
    "# \t  \"msa\": \"اللغة العربية الفصحى\",\n",
    "# \t  \"magreb\": \"المغربية\",\n",
    "# \t  \"gulf\": \"الخليجية\",\n",
    "# \t  \"levant\": \"الشامية\"\n",
    "# }\n",
    "\n",
    "# def is_emoji(s):\n",
    "#     return s in UNICODE_EMOJI\n",
    "\n",
    "# # add space near your emoji\n",
    "# def add_space(text):\n",
    "#     return ''.join(' ' + char if is_emoji(char) else char for char in text).strip()\n",
    "\n",
    "# def preprocess(text, lang='ar'):\n",
    "#     if lang == 'ar':\n",
    "#         sent = add_space(text)\n",
    "#         sent = re.sub(r'(?:@[\\w_]+)', \"user\", sent)\n",
    "#         sent = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', \"url\", sent)\n",
    "#         sent = sent.replace('_', ' ')\n",
    "#         sent = sent.replace('#', ' ')\n",
    "#     else:\n",
    "#         sent = add_space(text)\n",
    "#         sent = re.sub(r'(?:@[\\w_]+)', \"@user\", sent)\n",
    "#         sent = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', \"http\", sent)\n",
    "#         sent = sent.replace('_', ' ')\n",
    "#         sent = sent.replace('#', ' ')\n",
    "\n",
    "#     return sent\n",
    "\n",
    "# def prepare_text(df, col='tweet'):\n",
    "#     if col == 'tweet':\n",
    "#         df['dialect'] = df['dialect'].map(dic)   \n",
    "#     for i in range(df.shape[0]):\n",
    "#         df.loc[i, col] = df.loc[i, 'dialect'] + ' [SEP] ' + df.loc[i, col]\n",
    "\n",
    "\n",
    "#     return df\n",
    "\n",
    "# def augment_data(df_train,text_col,label_col):\n",
    "#     df_aug = pd.DataFrame(columns=[text_col, label_col)\n",
    "#     dic_dup = {1: 3,\n",
    "#                0: 1\n",
    "#                }\n",
    "#     for i in range(df_train.shape[0]):\n",
    "#         current = df_train.iloc[i]\n",
    "#         text = current[text_col]\n",
    "#         label_cat = current[label_col]\n",
    "\n",
    "#         aug_ratio = dic_dup[label_cat]\n",
    "#         for k in range(aug_ratio):\n",
    "#             tokens = text.split(' ')\n",
    "#             l = len(tokens)\n",
    "#             n = int(0.1 * l)\n",
    "#             indices = np.random.choice(l, n, replace=False)\n",
    "#             for j in range(len(indices)):\n",
    "#                 tokens[indices[j]] = '[MASK]'\n",
    "#             new_text = ' '.join(tokens)\n",
    "#             entry = {text_col: new_text, label_col: label_cat}\n",
    "#             df_aug = df_aug.append(entry, ignore_index=True)\n",
    "#     df_aug.drop_duplicates(subset=[text_col], keep='first', inplace=True)\n",
    "#     df = pd.concat([df_train,df_aug])\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1661185831209,
     "user": {
      "displayName": "pink panther",
      "userId": "06900797602742482056"
     },
     "user_tz": -120
    },
    "id": "QcOa9agTwhIg"
   },
   "outputs": [],
   "source": [
    "# '''\n",
    "# Created by: Mohamed Salem Elhady  \n",
    "# Email: mohamed.elaraby@alumni.ubc.ca\n",
    "# Text Normalization: V1 \n",
    "# '''\n",
    "# import sys\n",
    "# import re\n",
    "# import emojis\n",
    "# from emoji import UNICODE_EMOJI\n",
    "# #sys.setdefaultencoding('utf-8')\n",
    "# ##########################Clean Text Data #######################################\n",
    "# ########################Global Variable Declaration##############################\n",
    "# list_seeds = ['سبحان الله', 'الله أكبر', 'اللهم', 'بسم الله', 'يا رب', 'العضيم', 'سبحان', 'يارب', 'قران', 'quran',\n",
    "#               'حديث', 'hadith', 'صلاه_الفجر', '﴾', 'ﷺ', 'صحيح البخاري', 'صحيح مسلم', 'يآرب', 'سورة']\n",
    "# MaxWordPerTweet=7\n",
    "# #################################################################################\n",
    "# def is_emoji(s):\n",
    "#     return s in UNICODE_EMOJI\n",
    "\n",
    "# # add space near your emoji\n",
    "# def add_space(text):\n",
    "#     return ''.join(' ' + char if is_emoji(char) else char for char in text).strip()\n",
    "\n",
    "# def clean(sent):\n",
    "#     \"\"\"clean data from any English char, emoticons, underscore, and repeated > 2\n",
    "#     str -> str\"\"\"\n",
    "#     p1 = re.compile('\\W')\n",
    "#     p2 = re.compile('\\s+')\n",
    "#     sent = re.sub(r\"http\\S+\", \"\", sent)\n",
    "#     sent = ReplaceThreeOrMore(sent)\n",
    "#     sent = remove_unicode_diac(sent)\n",
    "#     sent = sent.replace('_', ' ')\n",
    "#     sent = re.sub(r'[A-Za-z0-9]', r'', sent)\n",
    "#     sent = re.sub(p1, ' ', sent)\n",
    "#     sent = re.sub(p2, ' ', sent)\n",
    "#     return sent\n",
    "\n",
    "# def tokenize_emojis(tweet):\n",
    "#     return list(emojis.get(tweet))\n",
    "\n",
    "# def replace_emoji(sent):\n",
    "#     emoji_pattern = re.compile(\"[\"\n",
    "#                                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "#                                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "#                                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "#                                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "#                                \"]+\", flags=re.UNICODE)\n",
    "#     return emoji_pattern.sub(r'[MASK]', sent)\n",
    "\n",
    "# def preprocess(tweet):\n",
    "#     tweet = add_space(tweet)\n",
    "#     emos = tokenize_emojis(tweet)\n",
    "#     sent = remove_unicode_diac(tweet)\n",
    "#     sent = re.sub(r'(?:@[\\w_]+)', \"user\", sent)\n",
    "#     sent = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', \"url\", sent)\n",
    "#     sent = sent.replace('_', ' ')\n",
    "#     sent = sent.replace('#', ' ')\n",
    "#     if len(emos) > 0:\n",
    "#         sent = sent + ' [SEP] '  + ' '.join(emos)\n",
    "#     #    #sent = sent + ' [SEP] ' + clean_unicode(tweet) + ' [SEP] ' + ' '.join(emos)\n",
    "\n",
    "#     #else:\n",
    "#     #    sent = sent + ' [SEP] ' + clean_unicode(tweet)\n",
    "#     return sent\n",
    "\n",
    "# def preprocess_last(tweet, k=0):\n",
    "#     emos = tokenize_emojis(tweet)\n",
    "#     sent = remove_unicode_diac(tweet)\n",
    "#     sent = re.sub(r'(?:@[\\w_]+)', \"\", sent)\n",
    "#     sent = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', \"\", sent)\n",
    "#     sent = sent.replace('_', ' ')\n",
    "#     sent = sent.replace('#', ' ')\n",
    "#     if k == 0:\n",
    "#         sent = sent\n",
    "#     elif k ==1 :\n",
    "#         sent = sent + ' [SEP] ' + ' '.join(emos)\n",
    "#     elif k==2 :\n",
    "#         sent = sent + ' [SEP] ' + clean_unicode(tweet) + ' [SEP] ' + ' '.join(emos)\n",
    "#     elif k == 3:\n",
    "#         sent = sent + ' [SEP] ' + clean_unicode(tweet)\n",
    "#     else:\n",
    "#         sent = replace_emoji(sent)\n",
    "#         sent = sent + ' [SEP] ' + clean_unicode(tweet) + ' [SEP] ' + ' '.join(emos)\n",
    "\n",
    "#     return sent\n",
    "\n",
    "\n",
    "# def normalize(sent):\n",
    "#     \"\"\"clean data from any English char, emoticons, underscore, and repeated > 2\n",
    "#     str -> str\"\"\"\n",
    "#     sent = re.sub(r'(?:@[\\w_]+)', \"user\", sent)\n",
    "#     sent = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', \"url\", sent)\n",
    "#     #sent = re.sub(r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", \"hashtag\", sent)\n",
    "#     sent = ReplaceThreeOrMore(sent)\n",
    "#     sent = remove_unicode_diac(sent)\n",
    "#     sent = sent.replace('_', ' ')\n",
    "#     return sent\n",
    "\n",
    "# def ReplaceThreeOrMore(s):\n",
    "#     # pattern to look for three or more repetitions of any character, including\n",
    "#     # newlines.\n",
    "#     pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL)\n",
    "#     return pattern.sub(r\"\\1\\1\", s)\n",
    "# def norm_alif(text):\n",
    "#     text = text.replace(u\"\\u0625\", u\"\\u0627\")  # HAMZA below, with LETTER ALEF\n",
    "#     #text = text.replace(u\"\\u0621\", u\"\\u0627\")  # HAMZA, with LETTER ALEF\n",
    "#     text = text.replace(u\"\\u0622\", u\"\\u0627\")  # ALEF WITH MADDA ABOVE, with LETTER ALEF\n",
    "#     text = text.replace(u\"\\u0623\", u\"\\u0627\")  # ALEF WITH HAMZA ABOVE, with LETTER ALEF\n",
    "#     return text\n",
    "# def remove_unicode_diac(text):\n",
    "#     \"\"\"Takes Arabic in utf-8 and returns same text without diac\"\"\"\n",
    "#     # Replace diacritics with nothing\n",
    "#     text = text.replace(u\"\\u064B\", \"\")  # fatHatayn\n",
    "#     text = text.replace(u\"\\u064C\", \"\")  # Dammatayn\n",
    "#     text = text.replace(u\"\\u064D\", \"\")  # kasratayn\n",
    "#     text = text.replace(u\"\\u064E\", \"\")  # fatHa\n",
    "#     text = text.replace(u\"\\u064F\", \"\")  # Damma\n",
    "#     text = text.replace(u\"\\u0650\", \"\")  # kasra\n",
    "#     text = text.replace(u\"\\u0651\", \"\")  # shaddah\n",
    "#     text = text.replace(u\"\\u0652\", \"\")  # sukuun\n",
    "#     text = text.replace(u\"\\u0670\", \"`\")  # dagger 'alif\n",
    "#     return text\n",
    "# def norm_taa(text):\n",
    "#     text=text.replace(u\"\\u0629\", u\"\\u0647\") # taa' marbuuTa, with haa'\n",
    "#     #text=text.replace(u\"\\u064A\", u\"\\u0649\") # yaa' with 'alif maqSuura\n",
    "#     return text\n",
    "# def norm_yaa(text):\n",
    "#     if len(text)!=0:\n",
    "#         if text[-1] == u\"\\u064A\":\n",
    "#             text = text[:-1] + text[-1].replace(u\"\\u064A\", u\"\\u0649\")  # yaa' with 'alif maqSuura\n",
    "#     return text\n",
    "\n",
    "# def NormForWord2Vec(text):\n",
    "#     text=norm_taa(text)\n",
    "#     text=norm_yaa(text)\n",
    "#     text=norm_alif(text)\n",
    "#     return text\n",
    "\n",
    "# def remove_nonunicode2(Tweet):\n",
    "#     ## defining set of unicode ##\n",
    "#     #u\"\"\n",
    "#     #Tweet=Tweet.decode(\"utf-8\")\n",
    "#     UniLex={ ## This is list of all arabic unicode characters in addition to space (to separate words)\n",
    "#             u\"\\u0622\",\n",
    "#             u\"\\u0626\",\n",
    "#             u\"\\u0628\",\n",
    "#             u\"\\u062a\",\n",
    "#             u\"\\u062c\",\n",
    "#             u\"\\u06af\",\n",
    "#             u\"\\u062e\",\n",
    "#             u\"\\u0630\",\n",
    "#             u\"\\u0632\",\n",
    "#             u\"\\u0634\",\n",
    "#             u\"\\u0636\",\n",
    "#             u\"\\u0638\",\n",
    "#             u\"\\u063a\",\n",
    "#             u\"\\u0640\",\n",
    "#             u\"\\u0642\",\n",
    "#             u\"\\u0644\",\n",
    "#             u\"\\u0646\",\n",
    "#             u\"\\u0648\",\n",
    "#             u\"\\u064a\",\n",
    "#             u\"\\u0670\",\n",
    "#             u\"\\u067e\",\n",
    "#             u\"\\u0686\",\n",
    "#             u\"\\u0621\",\n",
    "#             u\"\\u0623\",\n",
    "#             u\"\\u0625\",\n",
    "#             u\"\\u06a4\",\n",
    "#             u\"\\u0627\",\n",
    "#             u\"\\u0629\",\n",
    "#             u\"\\u062b\",\n",
    "#             u\"\\u062d\",\n",
    "#             u\"\\u062f\",\n",
    "#             u\"\\u0631\",\n",
    "#             u\"\\u0633\",\n",
    "#             u\"\\u0635\",\n",
    "#             u\"\\u0637\",\n",
    "#             u\"\\u0639\",\n",
    "#             u\"\\u0641\",\n",
    "#             u\"\\u0643\",\n",
    "#             u\"\\u0645\",\n",
    "#             u\"\\u0647\",\n",
    "#             u\"\\u0649\",\n",
    "#             u\"\\u0671\",\n",
    "#             ' ',\n",
    "#             '\\n'\n",
    "#           }\n",
    "#     fin_tweet=\"\"\n",
    "#     for c in Tweet:\n",
    "#         if c in UniLex:\n",
    "#            fin_tweet=fin_tweet+c\n",
    "#     return fin_tweet\n",
    "\n",
    "# ###### Heuristics Calculations ######\n",
    "# def diac_counter(text):\n",
    "#     #text=text.decode(\"utf-8\")\n",
    "#     diac = [u\"\\u064B\",u\"\\u064C\", u\"\\u064D\", u\"\\u064E\", u\"\\u064F\", u\"\\u0650\", u\"\\u0651\", u\"\\u0652\", u\"\\u0670\"]\n",
    "#     diac_count=0\n",
    "#     for d in diac:\n",
    "#         diac_count+=text.count(d)\n",
    "# #         if d in text:\n",
    "# #             print(d)\n",
    "# #             diac_count+=1\n",
    "#     return diac_count\n",
    "# def check_seed(list_seeds, text):\n",
    "#     \"\"\n",
    "#     for word in list_seeds:\n",
    "#         text = text.lower()\n",
    "#         if word.decode(\"utf-8\") in text:\n",
    "#             return True\n",
    "#     return False\n",
    "# def EnglishCount(text):\n",
    "#     printable = ['e', 'a', 'o', 't', 'i']\n",
    "#     count = 0\n",
    "#     for ch in printable:\n",
    "#         count += text.count(ch.lower())\n",
    "#     return count\n",
    "# ########################################\n",
    "\n",
    "\n",
    "\n",
    "# def eliminate_single_char_words(Tweet):\n",
    "#     parts = Tweet.split(\" \")\n",
    "#     cleaned_line_parts = []\n",
    "#     for P in parts:\n",
    "#         if len(P) != 1:\n",
    "#             cleaned_line_parts.append(P)\n",
    "#     cleaned_line = ' '.join(cleaned_line_parts)\n",
    "#     return cleaned_line\n",
    "# def clean_unicode(Tweet):\n",
    "#     tweet=normalize(Tweet.strip(\"\\n\"))\n",
    "#     if len(tweet) !=0:\n",
    "#         sentence = []\n",
    "#         for word in tweet.split(\" \"):\n",
    "#             word = remove_unicode_diac(word)\n",
    "#             word = norm_alif(word)\n",
    "#             word = norm_taa(word)\n",
    "#             word = norm_yaa(word)\n",
    "#             word = normalize(word)\n",
    "#             sentence.append(word)\n",
    "#         tweet = ' '.join(sentence)\n",
    "#         tweet =remove_nonunicode2(tweet)\n",
    "#         tweet =eliminate_single_char_words(tweet)\n",
    "#     return tweet\n",
    "\n",
    "# def clean_unicode2(Tweet):\n",
    "#     KeepUniOnly(Tweet)\n",
    "#     tweet=normalize(Tweet.strip(\"\\n\"))\n",
    "#     if len(tweet) !=0:\n",
    "#         sentence = []\n",
    "#         for word in tweet.split(\" \"):\n",
    "#             word = remove_unicode_diac(word)\n",
    "#             word = normalize(word)\n",
    "#             sentence.append(word)\n",
    "#         tweet = ' '.join(sentence)\n",
    "#         tweet =remove_nonunicode2(tweet)\n",
    "#         tweet =eliminate_single_char_words(tweet)\n",
    "#     return tweet\n",
    "\n",
    "# def NormCorpusFinal(Tweet):\n",
    "#     tweet=KeepUniOnly(Tweet)\n",
    "#     tweet=NormForWord2Vec(tweet)\n",
    "#     return tweet\n",
    "\n",
    "# def KeepUniOnly(Tweet):## this one is without normalization\n",
    "#     tweet=Tweet.replace(\"# \",\" \")\n",
    "#     tweet=tweet.replace(\"#\",\" \")\n",
    "#     tweet=tweet.replace(\"_\",\" \")\n",
    "#     tweet=tweet.replace(u\"\\u0657\",\" \")\n",
    "#     tweet=tweet.replace(\"\\n\",\" \")\n",
    "#     tweet=remove_nonunicode2(tweet)\n",
    "#     tweet=eliminate_single_char_words(tweet)\n",
    "#     tweet=ReplaceThreeOrMore(tweet)\n",
    "#     return tweet\n",
    "\n",
    "# def get_charset(rawtext):\n",
    "#     chars = sorted(list(set(rawtext)))\n",
    "#     return chars\n",
    "\n",
    "# def DialectChecker(text):\n",
    "#     ##Based on Hueristics done by Hassan\n",
    "#     if (diac_counter(text)>5 or check_seed(list_seeds,text) or EnglishCount(text)>4 or \"<URL>\"  in text\n",
    "#         or text.count('#') >2 or '\"'  in text or text.count('@') or \"\\\"RT\" in text or len(text.split(\" \")) <7):\n",
    "#         return False\n",
    "#     else:\n",
    "#         return True\n",
    "\n",
    "# ###############################################################\n",
    "# '''\n",
    "# Fread=open(\"Egypt_portion.txt\",'r')\n",
    "# Fwriter=open(\"Egypt_portion_norm.txt\",'w')\n",
    "# for line in Fread:\n",
    "#     cleaned_line=clean_unicode_for_w2v(line)\n",
    "#     Fwriter.write(str(cleaned_line))\n",
    "# Fwriter.close()\n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVcFbe1Bvcqh"
   },
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1661185831210,
     "user": {
      "displayName": "pink panther",
      "userId": "06900797602742482056"
     },
     "user_tz": -120
    },
    "id": "RGw6mbs0uIlN"
   },
   "outputs": [],
   "source": [
    "class F1_Loss(nn.Module):\n",
    "    '''Calculate F1 score. Can work with gpu tensors\n",
    "    \n",
    "    The original implmentation is written by Michal Haltuf on Kaggle.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        `ndim` == 1. epsilon <= val <= 1\n",
    "    \n",
    "    Reference\n",
    "    ---------\n",
    "    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\n",
    "    - http://www.ryanzhang.info/python/writing-your-own-loss-function-module-for-pytorch/\n",
    "    '''\n",
    "    def __init__(self, epsilon=1e-7):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def forward(self, y_pred, y_true,):\n",
    "        # assert y_pred.ndim == 2\n",
    "        # assert y_true.ndim == 1\n",
    "        # print(y_pred.shape)\n",
    "        # print(y_true.shape)\n",
    "        # y_pred[y_pred<0.5]=0\n",
    "        # y_pred[y_pred>=0.5]=0\n",
    "\n",
    "\n",
    "        \n",
    "        y_true_one_hot = F.one_hot(y_true.to(torch.int64), 18).to(torch.float32)\n",
    "        # y_pred_one_hot = F.one_hot(y_pred.to(torch.int64), 2).to(torch.float32)\n",
    "        \n",
    "        tp = (y_true_one_hot * y_pred).sum(dim=0).to(torch.float32)\n",
    "        tn = ((1 - y_true_one_hot) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "        fp = ((1 - y_true_one_hot) * y_pred).sum(dim=0).to(torch.float32)\n",
    "        fn = (y_true_one_hot * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "\n",
    "        precision = tp / (tp + fp + self.epsilon)\n",
    "        recall = tp / (tp + fn + self.epsilon)\n",
    "\n",
    "        f1 = 2* (precision*recall) / (precision + recall + self.epsilon)\n",
    "        f1 = f1.clamp(min=self.epsilon, max=1-self.epsilon)\n",
    "        f1=f1.detach()\n",
    "        # print(f1.shape)\n",
    "        # y_pred=y_pred.reshape((y_pred.shape[0], 1))\n",
    "        # y_true=y_true.reshape((y_true.shape[0], 1))\n",
    "\n",
    "        # p1=y_true*(math.log(sigmoid(y_pred)))*(1-f1)[1]\n",
    "        # p0=(1-y_true)*math.log(1-sigmoid(y_pred))*(1-f1)[0]\n",
    "\n",
    "\n",
    "        # y_true_one_hot = F.one_hot(y_true.to(torch.int64), 2)\n",
    "        # print(y_pred)\n",
    "        # print(y_true_one_hot)\n",
    "        CE =torch.nn.CrossEntropyLoss(weight=( 1 - f1))(y_pred, y_true)\n",
    "        # loss = ( 1 - f1)  * CE\n",
    "        return  CE.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1661185831211,
     "user": {
      "displayName": "pink panther",
      "userId": "06900797602742482056"
     },
     "user_tz": -120
    },
    "id": "Pdd02bGivpr5"
   },
   "outputs": [],
   "source": [
    "class Recall_Loss(nn.Module):\n",
    "    '''Calculate Recall score. Can work with gpu tensors\n",
    "    \n",
    "    The original implmentation is written by Michal Haltuf on Kaggle.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        `ndim` == 1. epsilon <= val <= 1\n",
    "    \n",
    "    Reference\n",
    "    ---------\n",
    "    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\n",
    "    - http://www.ryanzhang.info/python/writing-your-own-loss-function-module-for-pytorch/\n",
    "    '''\n",
    "    def __init__(self, epsilon=1e-7):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def forward(self, y_pred, y_true,):\n",
    "        # assert y_pred.ndim == 2\n",
    "        # assert y_true.ndim == 1\n",
    "        # print(y_pred.shape)\n",
    "        # print(y_true.shape)\n",
    "        # y_pred[y_pred<0.5]=0\n",
    "        # y_pred[y_pred>=0.5]=0\n",
    "\n",
    "\n",
    "        \n",
    "        y_true_one_hot = F.one_hot(y_true.to(torch.int64), 18).to(torch.float32)\n",
    "        # y_pred_one_hot = F.one_hot(y_pred.to(torch.int64), 2).to(torch.float32)\n",
    "        \n",
    "        tp = (y_true_one_hot * y_pred).sum(dim=0).to(torch.float32)\n",
    "        tn = ((1 - y_true_one_hot) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "        fp = ((1 - y_true_one_hot) * y_pred).sum(dim=0).to(torch.float32)\n",
    "        fn = (y_true_one_hot * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "\n",
    "        precision = tp / (tp + fp + self.epsilon)\n",
    "        recall = tp / (tp + fn + self.epsilon)\n",
    "\n",
    "        # f1 = 2* (precision*recall) / (precision + recall + self.epsilon)\n",
    "        # f1 = f1.clamp(min=self.epsilon, max=1-self.epsilon)\n",
    "        # f1=f1.detach()\n",
    "        # print(f1.shape)\n",
    "        # y_pred=y_pred.reshape((y_pred.shape[0], 1))\n",
    "        # y_true=y_true.reshape((y_true.shape[0], 1))\n",
    "\n",
    "        # p1=y_true*(math.log(sigmoid(y_pred)))*(1-f1)[1]\n",
    "        # p0=(1-y_true)*math.log(1-sigmoid(y_pred))*(1-f1)[0]\n",
    "\n",
    "\n",
    "        # y_true_one_hot = F.one_hot(y_true.to(torch.int64), 2)\n",
    "        # print(y_pred)\n",
    "        # print(y_true_one_hot)\n",
    "        recall=recall.detach()\n",
    "        CE =torch.nn.CrossEntropyLoss(weight=( 1 - recall))(y_pred, y_true_one_hot)\n",
    "        # loss = ( 1 - f1)  * CE\n",
    "        return  CE.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1661185831211,
     "user": {
      "displayName": "pink panther",
      "userId": "06900797602742482056"
     },
     "user_tz": -120
    },
    "id": "zKy23cw1wHB9"
   },
   "outputs": [],
   "source": [
    "## Based on https://github.com/AbdelkaderMH/iSarcasmEval/blob/8f28f24ebfb641415a604329ed859506ae687148/focal_loss.py\n",
    "class BinaryFocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a implementation of Focal Loss with smooth label cross entropy supported which is proposed in\n",
    "    'Focal Loss for Dense Object Detection. (https://arxiv.org/abs/1708.02002)'\n",
    "        Focal_Loss= -1*alpha*(1-pt)*log(pt)\n",
    "    :param alpha: (tensor) 3D or 4D the scalar factor for this criterion\n",
    "    :param gamma: (float,double) gamma > 0 reduces the relative loss for well-classified examples (p>0.5) putting more\n",
    "                    focus on hard misclassified example\n",
    "    :param reduction: `none`|`mean`|`sum`\n",
    "    :param **kwargs\n",
    "        balance_index: (int) balance class index, should be specific when alpha is float\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=3, gamma=2, ignore_index=None, reduction='mean', **kwargs):\n",
    "        super(BinaryFocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.smooth = 1e-6  # set '1e-4' when train with FP16\n",
    "        self.ignore_index = ignore_index\n",
    "        self.reduction = reduction\n",
    "\n",
    "        assert self.reduction in ['none', 'mean', 'sum']\n",
    "\n",
    "        # if self.alpha is None:\n",
    "        #     self.alpha = torch.ones(2)\n",
    "        # elif isinstance(self.alpha, (list, np.ndarray)):\n",
    "        #     self.alpha = np.asarray(self.alpha)\n",
    "        #     self.alpha = np.reshape(self.alpha, (2))\n",
    "        #     assert self.alpha.shape[0] == 2, \\\n",
    "        #         'the `alpha` shape is not match the number of class'\n",
    "        # elif isinstance(self.alpha, (float, int)):\n",
    "        #     self.alpha = np.asarray([self.alpha, 1.0 - self.alpha], dtype=np.float).view(2)\n",
    "\n",
    "        # else:\n",
    "        #     raise TypeError('{} not supported'.format(type(self.alpha)))\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        prob = torch.sigmoid(output)\n",
    "        prob = torch.clamp(prob, self.smooth, 1.0 - self.smooth)\n",
    "\n",
    "        valid_mask = None\n",
    "        if self.ignore_index is not None:\n",
    "            valid_mask = (target != self.ignore_index).float()\n",
    "\n",
    "        pos_mask = (target == 1).float()\n",
    "        neg_mask = (target == 0).float()\n",
    "        if valid_mask is not None:\n",
    "            pos_mask = pos_mask * valid_mask\n",
    "            neg_mask = neg_mask * valid_mask\n",
    "\n",
    "        pos_weight = (pos_mask * torch.pow(1 - prob, self.gamma)).detach()\n",
    "        pos_loss = -pos_weight * torch.log(prob)  # / (torch.sum(pos_weight) + 1e-4)\n",
    "\n",
    "        neg_weight = (neg_mask * torch.pow(prob, self.gamma)).detach()\n",
    "        neg_loss = -self.alpha * neg_weight * F.logsigmoid(-output)  # / (torch.sum(neg_weight) + 1e-4)\n",
    "        loss = pos_loss + neg_loss\n",
    "        loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "class FocalLoss_Ori(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a implementation of Focal Loss with smooth label cross entropy supported which is proposed in\n",
    "    'Focal Loss for Dense Object Detection. (https://arxiv.org/abs/1708.02002)'\n",
    "    Focal_Loss= -1*alpha*((1-pt)**gamma)*log(pt)\n",
    "    Args:\n",
    "        num_class: number of classes\n",
    "        alpha: class balance factor\n",
    "        gamma:\n",
    "        ignore_index:\n",
    "        reduction:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_class, alpha=None, gamma=2, ignore_index=None, reduction='mean'):\n",
    "        super(FocalLoss_Ori, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.smooth = 1e-4\n",
    "        self.ignore_index = ignore_index\n",
    "        self.alpha = alpha\n",
    "        if alpha is None:\n",
    "            self.alpha = torch.ones(num_class, )\n",
    "        elif isinstance(alpha, (int, float)):\n",
    "            self.alpha = torch.as_tensor([alpha] * num_class)\n",
    "        elif isinstance(alpha, (list, np.ndarray)):\n",
    "            self.alpha = torch.as_tensor(alpha)\n",
    "        if self.alpha.shape[0] != num_class:\n",
    "            raise RuntimeError('the length not equal to number of class')\n",
    "\n",
    "        # if isinstance(self.alpha, (list, tuple, np.ndarray)):\n",
    "        #     assert len(self.alpha) == self.num_class\n",
    "        #     self.alpha = torch.Tensor(list(self.alpha))\n",
    "        # elif isinstance(self.alpha, (float, int)):\n",
    "        #     assert 0 < self.alpha < 1.0, 'alpha should be in `(0,1)`)'\n",
    "        #     assert balance_index > -1\n",
    "        #     alpha = torch.ones((self.num_class))\n",
    "        #     alpha *= 1 - self.alpha\n",
    "        #     alpha[balance_index] = self.alpha\n",
    "        #     self.alpha = alpha\n",
    "        # elif isinstance(self.alpha, torch.Tensor):\n",
    "        #     self.alpha = self.alpha\n",
    "        # else:\n",
    "        #     raise TypeError('Not support alpha type, expect `int|float|list|tuple|torch.Tensor`')\n",
    "\n",
    "    def forward(self, logit, target):\n",
    "        # assert isinstance(self.alpha,torch.Tensor)\\\n",
    "        N, C = logit.shape[:2]\n",
    "        alpha = self.alpha.to(logit.device)\n",
    "        prob = F.softmax(logit, dim=1)\n",
    "        if prob.dim() > 2:\n",
    "            # N,C,d1,d2 -> N,C,m (m=d1*d2*...)\n",
    "            prob = prob.view(N, C, -1)\n",
    "            prob = prob.transpose(1, 2).contiguous()  # [N,C,d1*d2..] -> [N,d1*d2..,C]\n",
    "            prob = prob.view(-1, prob.size(-1))  # [N,d1*d2..,C]-> [N*d1*d2..,C]\n",
    "        ori_shp = target.shape\n",
    "        target = target.view(-1, 1)  # [N,d1,d2,...]->[N*d1*d2*...,1]\n",
    "        valid_mask = None\n",
    "        if self.ignore_index is not None:\n",
    "            valid_mask = target != self.ignore_index\n",
    "            target = target * valid_mask\n",
    "\n",
    "        # ----------memory saving way--------\n",
    "        prob = prob.gather(1, target).view(-1) + self.smooth  # avoid nan\n",
    "        logpt = torch.log(prob)\n",
    "        # alpha_class = alpha.gather(0, target.view(-1))\n",
    "        alpha_class = alpha[target.squeeze().long()]\n",
    "        class_weight = -alpha_class * torch.pow(torch.sub(1.0, prob), self.gamma)\n",
    "        loss = class_weight * logpt\n",
    "        if valid_mask is not None:\n",
    "            loss = loss * valid_mask.squeeze()\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "            if valid_mask is not None:\n",
    "                loss = loss.sum() / valid_mask.sum()\n",
    "        elif self.reduction == 'none':\n",
    "            loss = loss.view(ori_shp)\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "def weighted_binary_cross_entropy(input, targets, pos_weight, weight=None, size_average=True, reduce=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sigmoid_x: predicted probability of size [N,C], N sample and C Class. Eg. Must be in range of [0,1], i.e. Output from Sigmoid.\n",
    "        targets: true value, one-hot-like vector of size [N,C]\n",
    "        pos_weight: Weight for postive sample\n",
    "    \"\"\"\n",
    "    sigmoid_x = torch.sigmoid(input)\n",
    "    if not (targets.size() == sigmoid_x.size()):\n",
    "        raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(targets.size(), sigmoid_x.size()))\n",
    "\n",
    "    loss = -pos_weight* targets * sigmoid_x.log() - (1-targets)*(1-sigmoid_x).log()\n",
    "\n",
    "    if weight is not None:\n",
    "        loss = loss * weight\n",
    "\n",
    "    if not reduce:\n",
    "        return loss\n",
    "    elif size_average:\n",
    "        return loss.mean()\n",
    "    else:\n",
    "        return loss.sum()\n",
    "\n",
    "class WeightedBCELoss(nn.Module):\n",
    "    def __init__(self, pos_weight= 1, weight=None, PosWeightIsDynamic= True, WeightIsDynamic= False, size_average=True, reduce=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pos_weight = Weight for postive samples. Size [1,C]\n",
    "            weight = Weight for Each class. Size [1,C]\n",
    "            PosWeightIsDynamic: If True, the pos_weight is computed on each batch. If pos_weight is None, then it remains None.\n",
    "            WeightIsDynamic: If True, the weight is computed on each batch. If weight is None, then it remains None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        #self.register_buffer('weight', weight)\n",
    "        #self.register_buffer('pos_weight', pos_weight)\n",
    "        self.size_average = size_average\n",
    "        self.reduce = reduce\n",
    "        self.PosWeightIsDynamic = PosWeightIsDynamic\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # pos_weight = Variable(self.pos_weight) if not isinstance(self.pos_weight, Variable) else self.pos_weight\n",
    "        if self.PosWeightIsDynamic:\n",
    "            positive_counts = target.sum(dim=0)\n",
    "            nBatch = len(target)\n",
    "            self.pos_weight = (nBatch - positive_counts)/(positive_counts +1e-5)\n",
    "\n",
    "\n",
    "        return weighted_binary_cross_entropy(input, target,\n",
    "                                                self.pos_weight,\n",
    "                                                weight=None,\n",
    "                                                size_average=self.size_average,\n",
    "                                                reduce=self.reduce)\n",
    "\n",
    "\n",
    "class WeightedCELoss(nn.Module):\n",
    "    def __init__(self, size_average=True, reduce=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pos_weight = Weight for postive samples. Size [1,C]\n",
    "            weight = Weight for Each class. Size [1,C]\n",
    "            PosWeightIsDynamic: If True, the pos_weight is computed on each batch. If pos_weight is None, then it remains None.\n",
    "            WeightIsDynamic: If True, the weight is computed on each batch. If weight is None, then it remains None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.size_average = size_average\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        positive_counts = target.sum(dim=0)\n",
    "        nBatch = len(target)\n",
    "        pos_weight = (nBatch - positive_counts)/(positive_counts +1e-5)\n",
    "        neg_count = nBatch - positive_counts\n",
    "        neg_weight = (nBatch - neg_count)/(neg_count +1e-5)\n",
    "\n",
    "        weight = torch.tensor([neg_weight, pos_weight], device=target.device)\n",
    "  \n",
    "\n",
    "\n",
    "        return F.cross_entropy(input, target, weight=weight)\n",
    "\n",
    "\n",
    "\n",
    "class FLMultiLoss(nn.Module):\n",
    "    def __init__(self, gamma= 2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pos_weight = Weight for postive samples. Size [1,C]\n",
    "            weight = Weight for Each class. Size [1,C]\n",
    "            PosWeightIsDynamic: If True, the pos_weight is computed on each batch. If pos_weight is None, then it remains None.\n",
    "            WeightIsDynamic: If True, the weight is computed on each batch. If weight is None, then it remains None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.gamma = gamma\n",
    "    def forward(self, input, target):\n",
    "\n",
    "\n",
    "        return focal_binary_cross_entropy(input, target, gamma=2)\n",
    "\n",
    "def EntropyLoss(input_):\n",
    "    mask = input_.ge(0.000001)\n",
    "    mask_out = torch.masked_select(input_, mask)\n",
    "    entropy = -(torch.sum(mask_out * torch.log(mask_out)))\n",
    "    return entropy / float(input_.size(0))\n",
    "\n",
    "def focal_binary_cross_entropy(logits, targets, gamma=2):\n",
    "    num_label = targets.shape[1]\n",
    "    l = logits.reshape(-1)\n",
    "    t = targets.reshape(-1)\n",
    "    p = torch.sigmoid(l)\n",
    "    p = torch.where(t >= 0.5, p, 1-p)\n",
    "    logp = - torch.log(torch.clamp(p, 1e-4, 1-1e-4))\n",
    "    loss = logp*((1-p)**gamma)\n",
    "    loss = num_label*loss.mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1661185831212,
     "user": {
      "displayName": "pink panther",
      "userId": "06900797602742482056"
     },
     "user_tz": -120
    },
    "id": "dG5x6Ck0nOh_"
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Sequence\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\" Focal Loss, as described in https://arxiv.org/abs/1708.02002.\n",
    "    It is essentially an enhancement to cross entropy loss and is\n",
    "    useful for classification tasks when there is a large class imbalance.\n",
    "    x is expected to contain raw, unnormalized scores for each class.\n",
    "    y is expected to contain class labels.\n",
    "    Shape:\n",
    "        - x: (batch_size, C) or (batch_size, C, d1, d2, ..., dK), K > 0.\n",
    "        - y: (batch_size,) or (batch_size, d1, d2, ..., dK), K > 0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 alpha: Optional[Tensor] = None,\n",
    "                 gamma: float = 0.,\n",
    "                 reduction: str = 'mean',\n",
    "                 ignore_index: int = -100):\n",
    "        \"\"\"Constructor.\n",
    "        Args:\n",
    "            alpha (Tensor, optional): Weights for each class. Defaults to None.\n",
    "            gamma (float, optional): A constant, as described in the paper.\n",
    "                Defaults to 0.\n",
    "            reduction (str, optional): 'mean', 'sum' or 'none'.\n",
    "                Defaults to 'mean'.\n",
    "            ignore_index (int, optional): class label to ignore.\n",
    "                Defaults to -100.\n",
    "        \"\"\"\n",
    "        if reduction not in ('mean', 'sum', 'none'):\n",
    "            raise ValueError(\n",
    "                'Reduction must be one of: \"mean\", \"sum\", \"none\".')\n",
    "\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ignore_index = ignore_index\n",
    "        self.reduction = reduction\n",
    "\n",
    "        self.nll_loss = nn.NLLLoss(\n",
    "            weight=alpha, reduction='none', ignore_index=ignore_index)\n",
    "\n",
    "    def __repr__(self):\n",
    "        arg_keys = ['alpha', 'gamma', 'ignore_index', 'reduction']\n",
    "        arg_vals = [self.__dict__[k] for k in arg_keys]\n",
    "        arg_strs = [f'{k}={v}' for k, v in zip(arg_keys, arg_vals)]\n",
    "        arg_str = ', '.join(arg_strs)\n",
    "        return f'{type(self).__name__}({arg_str})'\n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor) -> Tensor:\n",
    "        if x.ndim > 2:\n",
    "            # (N, C, d1, d2, ..., dK) --> (N * d1 * ... * dK, C)\n",
    "            c = x.shape[1]\n",
    "            x = x.permute(0, *range(2, x.ndim), 1).reshape(-1, c)\n",
    "            # (N, d1, d2, ..., dK) --> (N * d1 * ... * dK,)\n",
    "            y = y.view(-1)\n",
    "\n",
    "        unignored_mask = y != self.ignore_index\n",
    "        y = y[unignored_mask]\n",
    "        if len(y) == 0:\n",
    "            return torch.tensor(0.)\n",
    "        x = x[unignored_mask]\n",
    "\n",
    "        # compute weighted cross entropy term: -alpha * log(pt)\n",
    "        # (alpha is already part of self.nll_loss)\n",
    "        log_p = F.log_softmax(x, dim=-1)\n",
    "        ce = self.nll_loss(log_p, y)\n",
    "\n",
    "        # get true class column from each row\n",
    "        all_rows = torch.arange(len(x))\n",
    "        log_pt = log_p[all_rows, y]\n",
    "\n",
    "        # compute focal term: (1 - pt)^gamma\n",
    "        pt = log_pt.exp()\n",
    "        focal_term = (1 - pt)**self.gamma\n",
    "\n",
    "        # the full loss: -alpha * ((1 - pt)^gamma) * log(pt)\n",
    "        loss = focal_term * ce\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def focal_loss(alpha: Optional[Sequence] = None,\n",
    "               gamma: float = 0.,\n",
    "               reduction: str = 'mean',\n",
    "               ignore_index: int = -100,\n",
    "               device='cuda',\n",
    "               dtype=torch.float32) -> FocalLoss:\n",
    "    \"\"\"Factory function for FocalLoss.\n",
    "    Args:\n",
    "        alpha (Sequence, optional): Weights for each class. Will be converted\n",
    "            to a Tensor if not None. Defaults to None.\n",
    "        gamma (float, optional): A constant, as described in the paper.\n",
    "            Defaults to 0.\n",
    "        reduction (str, optional): 'mean', 'sum' or 'none'.\n",
    "            Defaults to 'mean'.\n",
    "        ignore_index (int, optional): class label to ignore.\n",
    "            Defaults to -100.\n",
    "        device (str, optional): Device to move alpha to. Defaults to 'cpu'.\n",
    "        dtype (torch.dtype, optional): dtype to cast alpha to.\n",
    "            Defaults to torch.float32.\n",
    "    Returns:\n",
    "        A FocalLoss object\n",
    "    \"\"\"\n",
    "    if alpha is not None:\n",
    "        if not isinstance(alpha, Tensor):\n",
    "            alpha = torch.tensor(alpha)\n",
    "        alpha = alpha.to(device=device, dtype=dtype)\n",
    "\n",
    "    fl = FocalLoss(\n",
    "        alpha=alpha,\n",
    "        gamma=gamma,\n",
    "        reduction=reduction,\n",
    "        ignore_index=ignore_index)\n",
    "    return fl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvByMHwKuI4d"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1661185831213,
     "user": {
      "displayName": "pink panther",
      "userId": "06900797602742482056"
     },
     "user_tz": -120
    },
    "id": "FW53GnvRuJ6J"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import AutoModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def init_weights(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv2d') != -1 or classname.find('ConvTranspose2d') != -1:\n",
    "        nn.init.kaiming_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "class AttentionWithContext(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AttentionWithContext, self).__init__()\n",
    "\n",
    "        self.attn = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.contx = nn.Linear(hidden_dim, 1, bias=False)\n",
    "        #self.apply(init_weights)\n",
    "    def forward(self, inp):\n",
    "        u = torch.tanh_(self.attn(inp))\n",
    "        a = F.softmax(self.contx(u), dim=-1)\n",
    "        s = (a * inp).sum(1)\n",
    "        return s\n",
    "\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 pretrained_path='aubmindlab/bert-base-arabert'):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "\n",
    "        \n",
    "        self.transformer = AutoModel.from_pretrained(pretrained_path, output_hidden_states=True)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None):\n",
    "        outputs = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        #(output_last_layer, pooled_cls, (output_layers))\n",
    "        #output[0] (8, seqlen=64, 768) cls [8, 768] ( 12 (8, seqlen=64, 768))\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def output_num(self):\n",
    "        return self.transformer.config.hidden_size\n",
    "\n",
    "class ATTClassifier(nn.Module):\n",
    "    def __init__(self, in_feature, class_num=1, dropout_prob=0.2):\n",
    "        super(ATTClassifier, self).__init__()\n",
    "        self.attention = AttentionWithContext(in_feature)\n",
    "\n",
    "        self.Classifier = nn.Sequential(\n",
    "            nn.Linear(2 * in_feature, 512),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, class_num)\n",
    "        )\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        att = self.attention(x[0]) #(X[0] (bs, seqlenght, embedD) att = \\sum_i alpha_i x[0][i]\n",
    "\n",
    "        xx = torch.cat([att, x[1]], 1)\n",
    "\n",
    "        out = self.Classifier(xx)\n",
    "        return out\n",
    "    \n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "class NADIModel(nn.Module):\n",
    "  def __init__(self, pretrained_path='aubmindlab/bert-base-arabert',in_feature=768, class_num=18, dropout_prob=0.2):\n",
    "        super(NADIModel, self).__init__()\n",
    "        self.base_model=TransformerLayer(pretrained_path).to('cuda')\n",
    "        self.pooler=MeanPooling()\n",
    "        self.classifier=nn.Linear(self.base_model.output_num(), class_num)\n",
    "#         print(self.base_model.output_num())\n",
    "#         self.classifier=ATTClassifier(self.base_model.output_num(), class_num=class_num).to('cuda')\n",
    "  def forward(self, ids,mask):\n",
    "    output=self.base_model(ids,mask)\n",
    "    sequence_output = output.last_hidden_state\n",
    "    sequence_output = self.pooler(sequence_output, mask)\n",
    "    output=self.classifier(sequence_output)\n",
    "    # output=torch.softmax(output, dim=1)\n",
    "    return output\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYQWLGzVyqt_"
   },
   "source": [
    "# Train, Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1661185831214,
     "user": {
      "displayName": "pink panther",
      "userId": "06900797602742482056"
     },
     "user_tz": -120
    },
    "id": "pjYrf5IerGh7"
   },
   "outputs": [],
   "source": [
    "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"\n",
    "    Samples elements randomly from a given list of indices for imbalanced dataset\n",
    "    Arguments:\n",
    "        indices (list, optional): a list of indices\n",
    "        num_samples (int, optional): number of samples to draw\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, indices=None, num_samples=None):\n",
    "        # if indices is not provided,\n",
    "        # all elements in the dataset will be considered\n",
    "        self.indices = list(range(len(dataset['#3_label']))) \\\n",
    "            if indices is None else indices\n",
    "\n",
    "        # if num_samples is not provided,\n",
    "        # draw `len(indices)` samples in each iteration\n",
    "        self.num_samples = len(self.indices) \\\n",
    "            if num_samples is None else num_samples\n",
    "\n",
    "        # distribution of classes in the dataset\n",
    "        label_to_count = {}\n",
    "        for idx in self.indices:\n",
    "            label = self._get_label(dataset, idx)\n",
    "            if label in label_to_count:\n",
    "                label_to_count[label] += 1\n",
    "            else:\n",
    "                label_to_count[label] = 1\n",
    "\n",
    "        # weight for each sample\n",
    "        weights = [1.0 / label_to_count[self._get_label(dataset, idx)] for idx in self.indices]\n",
    "        self.weights = torch.DoubleTensor(weights)\n",
    "\n",
    "    def _get_label(self, dataset, id_):\n",
    "        return dataset['#3_label'][id_]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1661185831215,
     "user": {
      "displayName": "pink panther",
      "userId": "06900797602742482056"
     },
     "user_tz": -120
    },
    "id": "ZWP3677hqMr1"
   },
   "outputs": [],
   "source": [
    "def criterion(outputs1,  targets):\n",
    "\n",
    "    criterion = F1_Loss()\n",
    "    loss = criterion(outputs1, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1661185831215,
     "user": {
      "displayName": "pink panther",
      "userId": "06900797602742482056"
     },
     "user_tz": -120
    },
    "id": "ltIB44KZyxPt"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, data in bar:\n",
    "        \n",
    "        text_ids = data['text_ids'].to(device, dtype = torch.long)\n",
    "        text_mask = data['text_mask'].to(device, dtype = torch.long)\n",
    "        targets = data['target'].to(device, dtype=torch.long)\n",
    "        \n",
    "        batch_size = text_ids.size(0)\n",
    "        # print(targets)\n",
    "\n",
    "        outputs = model(text_ids, text_mask)\n",
    "        # print(outputs.shape)\n",
    "        \n",
    "        # print(outputs.shape)\n",
    "\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        loss = loss / CONFIG['n_accumulate']\n",
    "        loss.backward()\n",
    "    \n",
    "        if (step + 1) % CONFIG['n_accumulate'] == 0:\n",
    "            optimizer.step()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "                \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        \n",
    "        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n",
    "                        LR=optimizer.param_groups[0]['lr'])\n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1661185831216,
     "user": {
      "displayName": "pink panther",
      "userId": "06900797602742482056"
     },
     "user_tz": -120
    },
    "id": "U5Ro64pIy4HN"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, dataloader, device, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, data in bar:        \n",
    "        \n",
    "        text_ids = data['text_ids'].to(device, dtype = torch.long)\n",
    "        text_mask = data['text_mask'].to(device, dtype = torch.long)\n",
    "        targets = data['target'].to(device, dtype=torch.long)\n",
    "        \n",
    "        batch_size = text_ids.size(0)\n",
    "\n",
    "        outputs = model(text_ids, text_mask)\n",
    "        # outputs = outputs.argmax(dim=1)\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        \n",
    "        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n",
    "                        LR=optimizer.param_groups[0]['lr'])   \n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1661185831216,
     "user": {
      "displayName": "pink panther",
      "userId": "06900797602742482056"
     },
     "user_tz": -120
    },
    "id": "HpH2EB0snbV0"
   },
   "outputs": [],
   "source": [
    "def run_training(model, optimizer, scheduler, device, num_epochs, fold):\n",
    "    # To automatically log gradients\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
    "    \n",
    "    start = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_epoch_loss = np.inf\n",
    "    history = defaultdict(list)\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1): \n",
    "        gc.collect()\n",
    "        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, \n",
    "                                           dataloader=train_loader, \n",
    "                                           device=CONFIG['device'], epoch=epoch)\n",
    "        \n",
    "        val_epoch_loss = valid_one_epoch(model, valid_loader, device=CONFIG['device'], \n",
    "                                         epoch=epoch)\n",
    "    \n",
    "        history['Train Loss'].append(train_epoch_loss)\n",
    "        history['Valid Loss'].append(val_epoch_loss)\n",
    "        \n",
    "       \n",
    "        \n",
    "        # deep copy the model\n",
    "        if val_epoch_loss <= best_epoch_loss:\n",
    "            print(f\"Validation Loss Improved ({best_epoch_loss} ---> {val_epoch_loss})\")\n",
    "            best_epoch_loss = val_epoch_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            PATH = f\"/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Arabert/Loss_F1-Fold-{fold}.bin\"\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            # Save a model file from the current directory\n",
    "            print(\"Model Saved\")\n",
    "            \n",
    "        print()\n",
    "    \n",
    "    end = time.time()\n",
    "    time_elapsed = end - start\n",
    "    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
    "    print(\"Best Loss: {:.4f}\".format(best_epoch_loss))\n",
    "    \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1661185831217,
     "user": {
      "displayName": "pink panther",
      "userId": "06900797602742482056"
     },
     "user_tz": -120
    },
    "id": "JxT6JHg0nogC"
   },
   "outputs": [],
   "source": [
    "def fetch_scheduler(optimizer):\n",
    "    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CONFIG['T_max'], \n",
    "                                                   eta_min=CONFIG['min_lr'])\n",
    "    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n",
    "        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=CONFIG['T_0'], \n",
    "                                                             eta_min=CONFIG['min_lr'])\n",
    "    elif CONFIG['scheduler'] == None:\n",
    "        return None\n",
    "        \n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1661185831218,
     "user": {
      "displayName": "pink panther",
      "userId": "06900797602742482056"
     },
     "user_tz": -120
    },
    "id": "x-x9oUAsnqqy"
   },
   "outputs": [],
   "source": [
    "def prepare_loaders(train,fold):\n",
    "    df_train = train[train.kfold != fold].reset_index(drop=True)\n",
    "    df_valid = train[train.kfold == fold].reset_index(drop=True)\n",
    "    # sampler = ImbalancedDatasetSampler(df_train)\n",
    "    \n",
    "    train_dataset = TrainDataset(df_train, tokenizer=tokenizer, max_length=CONFIG['max_length'])\n",
    "    valid_dataset = TrainDataset(df_valid, tokenizer=tokenizer, max_length=CONFIG['max_length'])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], \n",
    "                              num_workers=2, shuffle=True, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], \n",
    "                              num_workers=2, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-MsRHzBnZX8"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ahmed/Downloads/Imagenet_VLCQ/Nadi\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57750,
     "status": "ok",
     "timestamp": 1661185888919,
     "user": {
      "displayName": "pink panther",
      "userId": "06900797602742482056"
     },
     "user_tz": -120
    },
    "id": "ih8htg1LoRZC",
    "outputId": "d77171d2-dc31-4639-bb24-2779a02971f1"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 4997,
     "status": "ok",
     "timestamp": 1661185893900,
     "user": {
      "displayName": "pink panther",
      "userId": "06900797602742482056"
     },
     "user_tz": -120
    },
    "id": "5dHag8YTn7jE"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Dataset/NADI2022_Subtask1_TRAIN.tsv', sep='\\t', lineterminator='\\n')\n",
    "valid = pd.read_csv('/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Dataset/NADI2022_Subtask1_DEV.tsv', sep='\\t', lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1661185893900,
     "user": {
      "displayName": "pink panther",
      "userId": "06900797602742482056"
     },
     "user_tz": -120
    },
    "id": "ZLZqgBrJoyaa",
    "outputId": "7178453d-060c-41fb-bae3-eee9b999b516"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#1_id</th>\n",
       "      <th>#2_content</th>\n",
       "      <th>#3_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_15711</td>\n",
       "      <td>الطرحة في 61 النفر 10 جنيه</td>\n",
       "      <td>yemen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_2257</td>\n",
       "      <td>كله ولا يطالبوا بارض فدك الله ياخدهم الله يعجل...</td>\n",
       "      <td>lebanon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_5618</td>\n",
       "      <td>' لما اطلقتو تلك التغريدة وقتها كان في واحد سع...</td>\n",
       "      <td>algeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_7284</td>\n",
       "      <td>بجكولى بى خير</td>\n",
       "      <td>iraq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_15841</td>\n",
       "      <td>يعم القمر براحه علينا</td>\n",
       "      <td>egypt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         #1_id                                         #2_content #3_label\n",
       "0  TRAIN_15711                         الطرحة في 61 النفر 10 جنيه    yemen\n",
       "1   TRAIN_2257  كله ولا يطالبوا بارض فدك الله ياخدهم الله يعجل...  lebanon\n",
       "2   TRAIN_5618  ' لما اطلقتو تلك التغريدة وقتها كان في واحد سع...  algeria\n",
       "3   TRAIN_7284                                      بجكولى بى خير     iraq\n",
       "4  TRAIN_15841                              يعم القمر براحه علينا    egypt"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1661185893901,
     "user": {
      "displayName": "pink panther",
      "userId": "06900797602742482056"
     },
     "user_tz": -120
    },
    "id": "gB9D4twgo0bI",
    "outputId": "e2bf0ebe-0c30-44f7-eee6-33fea35b08da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20398"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1661185893901,
     "user": {
      "displayName": "pink panther",
      "userId": "06900797602742482056"
     },
     "user_tz": -120
    },
    "id": "qYdSpBt2o2HR",
    "outputId": "01b959d4-6c94-42fc-a219-602a807c456d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['#3_label'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1661185893901,
     "user": {
      "displayName": "pink panther",
      "userId": "06900797602742482056"
     },
     "user_tz": -120
    },
    "id": "LREmIaISpGJF",
    "outputId": "9da05001-6423-4a11-984b-be93eb267f3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid['#3_label'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1661185893902,
     "user": {
      "displayName": "pink panther",
      "userId": "06900797602742482056"
     },
     "user_tz": -120
    },
    "id": "alyqp1JApKHQ"
   },
   "outputs": [],
   "source": [
    "# df_train=pd.concat([train,valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1661185893903,
     "user": {
      "displayName": "pink panther",
      "userId": "06900797602742482056"
     },
     "user_tz": -120
    },
    "id": "F8zmQRzrpDra"
   },
   "outputs": [],
   "source": [
    "num_classes=train['#3_label'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1661185893903,
     "user": {
      "displayName": "pink panther",
      "userId": "06900797602742482056"
     },
     "user_tz": -120
    },
    "id": "0_HBDMdYn1Mi"
   },
   "outputs": [],
   "source": [
    "CONFIG = {\"seed\": 42,\n",
    "          \"epochs\": 5,\n",
    "          \"train_batch_size\": 8,\n",
    "          \"valid_batch_size\": 8,\n",
    "          \"max_length\": 512,\n",
    "          \"learning_rate\": 1e-5,\n",
    "          \"scheduler\": 'CosineAnnealingLR',\n",
    "          \"min_lr\": 1e-8,\n",
    "          \"T_max\": 500,\n",
    "          \"weight_decay\": 1e-8,\n",
    "          \"n_fold\": 5,\n",
    "          \"n_accumulate\": 1,\n",
    "          \"num_classes\": 18,\n",
    "          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "66678967f5d24ce581f1dd82aa801929",
      "c6b063de0c3f4bb394d98d66335bea8d",
      "6a85d8c2926e4657be4d32ce8ca3999d",
      "52b9e30db5b84ed7a2297843fdea280e",
      "686adaff2d114f6e98c96d86710359f9",
      "c60d6765620d4c7e99e1e391fa46c081",
      "e20a77bef88c414dbe3115a53e49bf8a",
      "4e84c7e6f4f841369f75bcb2c9a0ccb2",
      "0240aa85927447828d85a08e3359ab36",
      "5bba9d2f5a9b41329b5d906ccf0d3203",
      "061427642dd947efa082effd8a1bb12d",
      "d9a8a620edb14e228048262cdee40e15",
      "3aec2147921b4123b98d868ada62aeac",
      "0cf0b79543ee44379ceceaaa07893ef2",
      "fabd03f48634462899aab62dbbac2f01",
      "9a8cfb71bcbf4dfb8d4f34557b3d090d",
      "4d7b84f3957e4d3ca73853681e9b94a1",
      "a143c616978c479cb9ff7b0b688ead20",
      "b013b4c8b5ee42d8a0082ef00b6d75f7",
      "87860daa0c4a4bff81c9a0e01f608878",
      "5dcca20e3ca9419eb89694fa837f4116",
      "a74d8778f3974096b54337d3c9153bb4",
      "3542f2e5f2c94f8cba7749e0c60d23aa",
      "805c8d2929584faa90dfe4b7e69d430d",
      "d4fbeecb7719411ba2b46ce6139e0986",
      "2cbbdaab14404cf891f2be8182c181df",
      "f176a2283b9b4cfab993647172520a77",
      "51374272379e43f098b94f486888525c",
      "6cf71f3223b342768c811b3acbb7a41d",
      "1873cbe7803c401592ff163d4688896d",
      "046d372083cc4f29bb4cfbe191106230",
      "1a6cdb48f7a343eb9951019c0a59dc8b",
      "566c431d7ab14ef29bf18f4207d9a755"
     ]
    },
    "executionInfo": {
     "elapsed": 11234,
     "status": "ok",
     "timestamp": 1661185905118,
     "user": {
      "displayName": "pink panther",
      "userId": "06900797602742482056"
     },
     "user_tz": -120
    },
    "id": "89VbI4nqq17L",
    "outputId": "b7005550-53ff-4ac1-dbc6-7c511ec8eb84"
   },
   "outputs": [],
   "source": [
    "tokenizer= AutoTokenizer.from_pretrained('aubmindlab/bert-base-arabertv02')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1661185905118,
     "user": {
      "displayName": "pink panther",
      "userId": "06900797602742482056"
     },
     "user_tz": -120
    },
    "id": "E9sfjlh6qj6t"
   },
   "outputs": [],
   "source": [
    "train.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1661185905119,
     "user": {
      "displayName": "pink panther",
      "userId": "06900797602742482056"
     },
     "user_tz": -120
    },
    "id": "Dq0tSkUrntN4"
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=CONFIG['n_fold'], shuffle=True, random_state=CONFIG['seed'])\n",
    "\n",
    "for fold, ( _, val_) in enumerate(skf.split(X=train, y=train['#3_label'])):\n",
    "    train.loc[val_ , \"kfold\"] = int(fold)\n",
    "    \n",
    "train[\"kfold\"] = train[\"kfold\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1661185905122,
     "user": {
      "displayName": "pink panther",
      "userId": "06900797602742482056"
     },
     "user_tz": -120
    },
    "id": "wOMg6NEVsEkU"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "train['#3_label'] = le.fit_transform(train['#3_label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 858,
     "referenced_widgets": [
      "c729abe9f935495da2a4aa2e6593ae5d",
      "4a6e510ff989486ca60c0e669f9f110f",
      "eae04b34ab63408f9ccbe8ac83260609",
      "44c79f8c20494db0abca09b53f0b03c1",
      "423045b1aad243dcb9cb7012226b4a52",
      "5eb6685521114b28922e484fd37fbc12",
      "35aa4f753d52403c90363c9cbe134dc2",
      "9fe8f48d340c4cf5a0d98b34d3f8ee12",
      "59979f7408d1462187545cccb2dbf5ef",
      "c87e3dde20684ee29353522735f542f4",
      "91e0caa1bee14f619a5badd419e3d297",
      "156f683aa5c3479f967d9d16f53ba1b0",
      "8940171c93104cc18c38eb703c9486ce",
      "08820e3c69534d8eb3ce616a7a22e1f3",
      "9cb9875a65b444858f58f287b3c50aa4",
      "33cb04e5e15d43a2915e2b9bf6a7d8ec",
      "977402a03f1a4cc2b6134af832bcca1f",
      "9c73c414190242c78866124c197953ed",
      "51847b0b927b4b36b2c21c1db1ce1b16",
      "07345977750743229c301525bbf2f0a7",
      "e4478468a59e402c8dc21ab5bf435aa6",
      "14a17abcab664f089ac21e3a50727493"
     ]
    },
    "id": "RdoS6yH5nwWG",
    "outputId": "bedb5cb0-860a-4b8b-fe57-409b4834b117"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Fold: 0 ======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv02 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU: NVIDIA GeForce RTX 3080 Ti\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 2039/2039 [05:28<00:00,  6.21it/s, Epoch=1, LR=9.85e-6, Train_Loss=2.51]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████| 510/510 [00:25<00:00, 19.98it/s, Epoch=1, LR=9.85e-6, Valid_Loss=2.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (inf ---> 2.452370993413177)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████| 2039/2039 [05:29<00:00,  6.19it/s, Epoch=2, LR=9.41e-6, Train_Loss=2.38]\n",
      "  0%|                                                                                           | 0/510 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████| 510/510 [00:25<00:00, 19.97it/s, Epoch=2, LR=9.41e-6, Valid_Loss=2.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (2.452370993413177 ---> 2.3910700585327898)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████| 2039/2039 [05:29<00:00,  6.19it/s, Epoch=3, LR=8.71e-6, Train_Loss=2.31]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                           | 0/510 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████████| 510/510 [00:25<00:00, 19.96it/s, Epoch=3, LR=8.71e-6, Valid_Loss=2.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                          | 0/2039 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████| 2039/2039 [05:29<00:00,  6.19it/s, Epoch=4, LR=7.79e-6, Train_Loss=2.21]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████| 510/510 [00:25<00:00, 19.97it/s, Epoch=4, LR=7.79e-6, Valid_Loss=2.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (2.3910700585327898 ---> 2.3604060564847553)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/2039 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████| 2039/2039 [05:29<00:00,  6.19it/s, Epoch=5, LR=6.7e-6, Train_Loss=2.1]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                           | 0/510 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████████| 510/510 [00:25<00:00, 20.01it/s, Epoch=5, LR=6.7e-6, Valid_Loss=2.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete in 0h 29m 39s\n",
      "Best Loss: 2.3604\n",
      "\n",
      "====== Fold: 1 ======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv02 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU: NVIDIA GeForce RTX 3080 Ti\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████| 2039/2039 [05:28<00:00,  6.20it/s, Epoch=1, LR=9.85e-6, Train_Loss=2.51]\n",
      "  0%|                                                                                           | 0/510 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████| 510/510 [00:25<00:00, 20.14it/s, Epoch=1, LR=9.85e-6, Valid_Loss=2.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (inf ---> 2.4372707091125787)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/2039 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████| 2039/2039 [05:28<00:00,  6.20it/s, Epoch=2, LR=9.41e-6, Train_Loss=2.38]\n",
      "  0%|                                                                                           | 0/510 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████| 510/510 [00:25<00:00, 20.02it/s, Epoch=2, LR=9.41e-6, Valid_Loss=2.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (2.4372707091125787 ---> 2.3893124457667856)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/2039 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|██████████████████████████████████████████| 2039/2039 [05:28<00:00,  6.20it/s, Epoch=3, LR=8.71e-6, Train_Loss=2.3]\n",
      "  0%|                                                                                           | 0/510 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████████| 510/510 [00:25<00:00, 20.14it/s, Epoch=3, LR=8.71e-6, Valid_Loss=2.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/2039 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████| 2039/2039 [05:29<00:00,  6.20it/s, Epoch=4, LR=7.79e-6, Train_Loss=2.18]\n",
      "  0%|                                                                                           | 0/510 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████| 510/510 [00:25<00:00, 20.05it/s, Epoch=4, LR=7.79e-6, Valid_Loss=2.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/2039 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|██████████████████████████████████████████| 2039/2039 [05:28<00:00,  6.20it/s, Epoch=5, LR=6.7e-6, Train_Loss=2.09]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                           | 0/510 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|████████████████████████████████████████████| 510/510 [00:25<00:00, 20.12it/s, Epoch=5, LR=6.7e-6, Valid_Loss=2.38]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (2.3893124457667856 ---> 2.383756957714464)\n",
      "Model Saved\n",
      "\n",
      "Training complete in 0h 29m 35s\n",
      "Best Loss: 2.3838\n",
      "\n",
      "====== Fold: 2 ======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv02 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU: NVIDIA GeForce RTX 3080 Ti\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/2039 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|█████████████████████████████████████████| 2039/2039 [05:28<00:00,  6.20it/s, Epoch=1, LR=9.85e-6, Train_Loss=2.46]\n",
      "  0%|                                                                                           | 0/510 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████| 510/510 [00:25<00:00, 20.09it/s, Epoch=1, LR=9.85e-6, Valid_Loss=2.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (inf ---> 2.4096808408989627)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/2039 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  9%|███▌                                      | 174/2039 [00:28<05:03,  6.15it/s, Epoch=2, LR=6.16e-6, Train_Loss=2.38]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3014/1336139979.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     model, history = run_training(model, optimizer, scheduler,\n\u001b[0m\u001b[1;32m     17\u001b[0m                                   \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                   \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3014/2055893528.py\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(model, optimizer, scheduler, device, num_epochs, fold)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         train_epoch_loss = train_one_epoch(model, optimizer, scheduler, \n\u001b[0m\u001b[1;32m     15\u001b[0m                                            \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                            device=CONFIG['device'], epoch=epoch)\n",
      "\u001b[0;32m/tmp/ipykernel_3014/1895400859.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, scheduler, dataloader, device, epoch)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_accumulate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_accumulate'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for fold in range(0, CONFIG['n_fold']):\n",
    "    print(f\"====== Fold: {fold} ======\")\n",
    "\n",
    "    \n",
    "    # Create Dataloaders\n",
    "    train_loader, valid_loader = prepare_loaders(train,fold=fold)\n",
    "    \n",
    "    model = NADIModel(pretrained_path='aubmindlab/bert-base-arabertv02')\n",
    "    model.to(CONFIG['device'])\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Define Optimizer and Scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
    "    scheduler = fetch_scheduler(optimizer)\n",
    "    \n",
    "    model, history = run_training(model, optimizer, scheduler,\n",
    "                                  device=CONFIG['device'],\n",
    "                                  num_epochs=CONFIG['epochs'],\n",
    "                                  fold=fold)\n",
    "    \n",
    "    \n",
    "    del model, history, train_loader, valid_loader\n",
    "    _ = gc.collect()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4TZwIpgR2NNg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Marbert_focal_loss_with_sampler.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0240aa85927447828d85a08e3359ab36": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "046d372083cc4f29bb4cfbe191106230": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "061427642dd947efa082effd8a1bb12d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "07345977750743229c301525bbf2f0a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "08820e3c69534d8eb3ce616a7a22e1f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_51847b0b927b4b36b2c21c1db1ce1b16",
      "max": 654190940,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_07345977750743229c301525bbf2f0a7",
      "tabbable": null,
      "tooltip": null,
      "value": 654190940
     }
    },
    "0cf0b79543ee44379ceceaaa07893ef2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_b013b4c8b5ee42d8a0082ef00b6d75f7",
      "max": 1099714,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_87860daa0c4a4bff81c9a0e01f608878",
      "tabbable": null,
      "tooltip": null,
      "value": 1099714
     }
    },
    "14a17abcab664f089ac21e3a50727493": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "156f683aa5c3479f967d9d16f53ba1b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8940171c93104cc18c38eb703c9486ce",
       "IPY_MODEL_08820e3c69534d8eb3ce616a7a22e1f3",
       "IPY_MODEL_9cb9875a65b444858f58f287b3c50aa4"
      ],
      "layout": "IPY_MODEL_33cb04e5e15d43a2915e2b9bf6a7d8ec",
      "tabbable": null,
      "tooltip": null
     }
    },
    "1873cbe7803c401592ff163d4688896d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1a6cdb48f7a343eb9951019c0a59dc8b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2cbbdaab14404cf891f2be8182c181df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_1a6cdb48f7a343eb9951019c0a59dc8b",
      "placeholder": "​",
      "style": "IPY_MODEL_566c431d7ab14ef29bf18f4207d9a755",
      "tabbable": null,
      "tooltip": null,
      "value": " 112/112 [00:00&lt;00:00, 3.84kB/s]"
     }
    },
    "33cb04e5e15d43a2915e2b9bf6a7d8ec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3542f2e5f2c94f8cba7749e0c60d23aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_805c8d2929584faa90dfe4b7e69d430d",
       "IPY_MODEL_d4fbeecb7719411ba2b46ce6139e0986",
       "IPY_MODEL_2cbbdaab14404cf891f2be8182c181df"
      ],
      "layout": "IPY_MODEL_f176a2283b9b4cfab993647172520a77",
      "tabbable": null,
      "tooltip": null
     }
    },
    "35aa4f753d52403c90363c9cbe134dc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "3aec2147921b4123b98d868ada62aeac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_4d7b84f3957e4d3ca73853681e9b94a1",
      "placeholder": "​",
      "style": "IPY_MODEL_a143c616978c479cb9ff7b0b688ead20",
      "tabbable": null,
      "tooltip": null,
      "value": "Downloading vocab.txt: 100%"
     }
    },
    "423045b1aad243dcb9cb7012226b4a52": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "44c79f8c20494db0abca09b53f0b03c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_c87e3dde20684ee29353522735f542f4",
      "placeholder": "​",
      "style": "IPY_MODEL_91e0caa1bee14f619a5badd419e3d297",
      "tabbable": null,
      "tooltip": null,
      "value": " 757/757 [00:00&lt;00:00, 20.6kB/s]"
     }
    },
    "4a6e510ff989486ca60c0e669f9f110f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_5eb6685521114b28922e484fd37fbc12",
      "placeholder": "​",
      "style": "IPY_MODEL_35aa4f753d52403c90363c9cbe134dc2",
      "tabbable": null,
      "tooltip": null,
      "value": "Downloading config.json: 100%"
     }
    },
    "4d7b84f3957e4d3ca73853681e9b94a1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4e84c7e6f4f841369f75bcb2c9a0ccb2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51374272379e43f098b94f486888525c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51847b0b927b4b36b2c21c1db1ce1b16": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "52b9e30db5b84ed7a2297843fdea280e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_5bba9d2f5a9b41329b5d906ccf0d3203",
      "placeholder": "​",
      "style": "IPY_MODEL_061427642dd947efa082effd8a1bb12d",
      "tabbable": null,
      "tooltip": null,
      "value": " 439/439 [00:00&lt;00:00, 10.7kB/s]"
     }
    },
    "566c431d7ab14ef29bf18f4207d9a755": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "59979f7408d1462187545cccb2dbf5ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5bba9d2f5a9b41329b5d906ccf0d3203": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5dcca20e3ca9419eb89694fa837f4116": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5eb6685521114b28922e484fd37fbc12": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "66678967f5d24ce581f1dd82aa801929": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c6b063de0c3f4bb394d98d66335bea8d",
       "IPY_MODEL_6a85d8c2926e4657be4d32ce8ca3999d",
       "IPY_MODEL_52b9e30db5b84ed7a2297843fdea280e"
      ],
      "layout": "IPY_MODEL_686adaff2d114f6e98c96d86710359f9",
      "tabbable": null,
      "tooltip": null
     }
    },
    "686adaff2d114f6e98c96d86710359f9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6a85d8c2926e4657be4d32ce8ca3999d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_4e84c7e6f4f841369f75bcb2c9a0ccb2",
      "max": 439,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0240aa85927447828d85a08e3359ab36",
      "tabbable": null,
      "tooltip": null,
      "value": 439
     }
    },
    "6cf71f3223b342768c811b3acbb7a41d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "805c8d2929584faa90dfe4b7e69d430d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_51374272379e43f098b94f486888525c",
      "placeholder": "​",
      "style": "IPY_MODEL_6cf71f3223b342768c811b3acbb7a41d",
      "tabbable": null,
      "tooltip": null,
      "value": "Downloading special_tokens_map.json: 100%"
     }
    },
    "87860daa0c4a4bff81c9a0e01f608878": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8940171c93104cc18c38eb703c9486ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_977402a03f1a4cc2b6134af832bcca1f",
      "placeholder": "​",
      "style": "IPY_MODEL_9c73c414190242c78866124c197953ed",
      "tabbable": null,
      "tooltip": null,
      "value": "Downloading pytorch_model.bin: 100%"
     }
    },
    "91e0caa1bee14f619a5badd419e3d297": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "977402a03f1a4cc2b6134af832bcca1f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a8cfb71bcbf4dfb8d4f34557b3d090d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c73c414190242c78866124c197953ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "9cb9875a65b444858f58f287b3c50aa4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_e4478468a59e402c8dc21ab5bf435aa6",
      "placeholder": "​",
      "style": "IPY_MODEL_14a17abcab664f089ac21e3a50727493",
      "tabbable": null,
      "tooltip": null,
      "value": " 624M/624M [00:11&lt;00:00, 55.3MB/s]"
     }
    },
    "9fe8f48d340c4cf5a0d98b34d3f8ee12": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a143c616978c479cb9ff7b0b688ead20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "a74d8778f3974096b54337d3c9153bb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "b013b4c8b5ee42d8a0082ef00b6d75f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c60d6765620d4c7e99e1e391fa46c081": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c6b063de0c3f4bb394d98d66335bea8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_c60d6765620d4c7e99e1e391fa46c081",
      "placeholder": "​",
      "style": "IPY_MODEL_e20a77bef88c414dbe3115a53e49bf8a",
      "tabbable": null,
      "tooltip": null,
      "value": "Downloading tokenizer_config.json: 100%"
     }
    },
    "c729abe9f935495da2a4aa2e6593ae5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4a6e510ff989486ca60c0e669f9f110f",
       "IPY_MODEL_eae04b34ab63408f9ccbe8ac83260609",
       "IPY_MODEL_44c79f8c20494db0abca09b53f0b03c1"
      ],
      "layout": "IPY_MODEL_423045b1aad243dcb9cb7012226b4a52",
      "tabbable": null,
      "tooltip": null
     }
    },
    "c87e3dde20684ee29353522735f542f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d4fbeecb7719411ba2b46ce6139e0986": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_1873cbe7803c401592ff163d4688896d",
      "max": 112,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_046d372083cc4f29bb4cfbe191106230",
      "tabbable": null,
      "tooltip": null,
      "value": 112
     }
    },
    "d9a8a620edb14e228048262cdee40e15": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3aec2147921b4123b98d868ada62aeac",
       "IPY_MODEL_0cf0b79543ee44379ceceaaa07893ef2",
       "IPY_MODEL_fabd03f48634462899aab62dbbac2f01"
      ],
      "layout": "IPY_MODEL_9a8cfb71bcbf4dfb8d4f34557b3d090d",
      "tabbable": null,
      "tooltip": null
     }
    },
    "e20a77bef88c414dbe3115a53e49bf8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "e4478468a59e402c8dc21ab5bf435aa6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eae04b34ab63408f9ccbe8ac83260609": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_9fe8f48d340c4cf5a0d98b34d3f8ee12",
      "max": 757,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_59979f7408d1462187545cccb2dbf5ef",
      "tabbable": null,
      "tooltip": null,
      "value": 757
     }
    },
    "f176a2283b9b4cfab993647172520a77": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fabd03f48634462899aab62dbbac2f01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_5dcca20e3ca9419eb89694fa837f4116",
      "placeholder": "​",
      "style": "IPY_MODEL_a74d8778f3974096b54337d3c9153bb4",
      "tabbable": null,
      "tooltip": null,
      "value": " 1.05M/1.05M [00:01&lt;00:00, 964kB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
