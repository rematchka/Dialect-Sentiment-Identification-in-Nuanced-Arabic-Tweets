{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"qrib_model.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"35ad34494d6540f8b7c9cdc0e1bb903d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bcb94a7944e748f18a3546050d2de21b","IPY_MODEL_55a00de76bfb4754843be53d35c25727","IPY_MODEL_84840b23b9f44c558d687d30331df6a3"],"layout":"IPY_MODEL_4eb6230e3bce407ca06d3e19f255cbc4"}},"bcb94a7944e748f18a3546050d2de21b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d40cb0850e2489faae71105383922ea","placeholder":"​","style":"IPY_MODEL_70bbac81a3b542dea796b309d1985ad2","value":"Downloading config.json: 100%"}},"55a00de76bfb4754843be53d35c25727":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_281db875d43e45a3a3862d0986aa5901","max":504,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6c713156bb174d8db7a97b21315fcba6","value":504}},"84840b23b9f44c558d687d30331df6a3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8724b46a366049ada87baa41d040357e","placeholder":"​","style":"IPY_MODEL_9df7936a69724eaa950892f28260b6c3","value":" 504/504 [00:00&lt;00:00, 14.5kB/s]"}},"4eb6230e3bce407ca06d3e19f255cbc4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d40cb0850e2489faae71105383922ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70bbac81a3b542dea796b309d1985ad2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"281db875d43e45a3a3862d0986aa5901":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c713156bb174d8db7a97b21315fcba6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8724b46a366049ada87baa41d040357e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9df7936a69724eaa950892f28260b6c3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5956b00e22dd469abfc71ecf1043a327":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_18bf49aa703d4fc2b8fc85eede3ca03a","IPY_MODEL_d1f805ba4ad0452daa62946f720c5d59","IPY_MODEL_bf84597006814c0f8192455bd5b67aa3"],"layout":"IPY_MODEL_88b12fa19f544f9790e9f75ea546192b"}},"18bf49aa703d4fc2b8fc85eede3ca03a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_06af14fdac394d52985bc6b8821e340b","placeholder":"​","style":"IPY_MODEL_bc0d305e1709496cb99e89b32f6747b6","value":"Downloading vocab.txt: 100%"}},"d1f805ba4ad0452daa62946f720c5d59":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_daf4a02c21bc4663a2cbd37515d4ff6a","max":656429,"min":0,"orientation":"horizontal","style":"IPY_MODEL_efa294cd26c549459b61159fc5e44d2f","value":656429}},"bf84597006814c0f8192455bd5b67aa3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_95b3c362f28c4d7388b58cb37326d12c","placeholder":"​","style":"IPY_MODEL_d36ae5415ad0412a879d7e2c6a8d7cd8","value":" 641k/641k [00:00&lt;00:00, 4.35MB/s]"}},"88b12fa19f544f9790e9f75ea546192b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06af14fdac394d52985bc6b8821e340b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc0d305e1709496cb99e89b32f6747b6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"daf4a02c21bc4663a2cbd37515d4ff6a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"efa294cd26c549459b61159fc5e44d2f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"95b3c362f28c4d7388b58cb37326d12c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d36ae5415ad0412a879d7e2c6a8d7cd8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"250c4ae2c550489582eee82871b81e00":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ae97d4edfecd4dfeb0842a5b88266d41","IPY_MODEL_d64adafe978349a599694f471f7547a5","IPY_MODEL_81bdbe4a2a0740c78a2c763dbcbaa72c"],"layout":"IPY_MODEL_a6de7ec40d8c4705b6c960e25312e7be"}},"ae97d4edfecd4dfeb0842a5b88266d41":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_808931be6fc14a718dbe81488ccc7497","placeholder":"​","style":"IPY_MODEL_7de703fa5e3746c88a36f72170bbc737","value":"Downloading pytorch_model.bin: 100%"}},"d64adafe978349a599694f471f7547a5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2c9dcc570a148e18bef677947d5fee1","max":543488365,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3153098254c248d18a09a6118b51c326","value":543488365}},"81bdbe4a2a0740c78a2c763dbcbaa72c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5fdb205eeac4a35a490eb46dd7a308e","placeholder":"​","style":"IPY_MODEL_6f3462e43c91423890b69c50d5e98d0b","value":" 518M/518M [00:09&lt;00:00, 56.2MB/s]"}},"a6de7ec40d8c4705b6c960e25312e7be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"808931be6fc14a718dbe81488ccc7497":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7de703fa5e3746c88a36f72170bbc737":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f2c9dcc570a148e18bef677947d5fee1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3153098254c248d18a09a6118b51c326":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c5fdb205eeac4a35a490eb46dd7a308e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f3462e43c91423890b69c50d5e98d0b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["!pip install sentencepiece\n","!pip  install transformers\n","!pip install pytorch-ignite\n","!pip install datasets\n","\n"],"metadata":{"id":"vMmciFBRyRFe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661783595972,"user_tz":-120,"elapsed":28185,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}},"outputId":"16757c1b-fc5f-45b3-a85c-212d84a7287c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 5.0 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.97\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.21.2-py3-none-any.whl (4.7 MB)\n","\u001b[K     |████████████████████████████████| 4.7 MB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 11.4 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n","\u001b[K     |████████████████████████████████| 120 kB 71.0 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.21.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch-ignite\n","  Downloading pytorch_ignite-0.4.9-py3-none-any.whl (259 kB)\n","\u001b[K     |████████████████████████████████| 259 kB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.12.1+cu113)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (21.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pytorch-ignite) (3.0.9)\n","Installing collected packages: pytorch-ignite\n","Successfully installed pytorch-ignite-0.4.9\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n","\u001b[K     |████████████████████████████████| 365 kB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n","Collecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 65.3 MB/s \n","\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.9.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.12.0)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n","\u001b[K     |████████████████████████████████| 115 kB 64.1 MB/s \n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.7.1)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.8.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 61.1 MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.2.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: urllib3, xxhash, responses, multiprocess, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed datasets-2.4.0 multiprocess-0.70.13 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AtS4CD_krp3d"},"outputs":[],"source":["import argparse\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import pandas as pd\n","import random\n","import numpy as np\n","import torch.nn as nn\n","import torch\n","from transformers import AutoModel\n","import torch.nn.functional as F\n","from sklearn.metrics import f1_score, accuracy_score, classification_report\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","# Any results you write to the current directory are saved as output.\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the \"../input/\" directory.\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","from transformers import BertTokenizer,BertModel\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader,Dataset\n","from torch.nn.utils.rnn import pack_padded_sequence\n","from torch.optim import AdamW\n","from tqdm import tqdm\n","from argparse import ArgumentParser\n","from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n","from ignite.metrics import Accuracy, Loss\n","from ignite.engine.engine import Engine, State, Events\n","from ignite.handlers import EarlyStopping\n","from ignite.contrib.handlers import TensorboardLogger, ProgressBar\n","from ignite.utils import convert_tensor\n","from torch.optim.lr_scheduler import ExponentialLR\n","import warnings  \n","warnings.filterwarnings('ignore')\n","import gc\n","import copy\n","import time\n","import random\n","import string\n","\n","# For data manipulation\n","import numpy as np\n","import pandas as pd\n","\n","# Pytorch Imports\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Utils\n","from tqdm import tqdm\n","from collections import defaultdict\n","\n","# Sklearn Imports\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import StratifiedKFold, KFold\n","from transformers import AutoTokenizer, AutoModel, AdamW\n","import random\n","import os\n","from urllib import request\n","import numpy as np\n","from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n","\n","from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, BertTokenizer\n","from transformers.data.processors import SingleSentenceClassificationProcessor\n","from transformers import Trainer , TrainingArguments\n","from transformers.trainer_utils import EvaluationStrategy\n","from transformers.data.processors.utils import InputFeatures\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader"]},{"cell_type":"markdown","source":["# Preprocessing"],"metadata":{"id":"ARjNBDtgwfVJ"}},{"cell_type":"code","source":["class TrainDataset(Dataset):\n","    def __init__(self, df, tokenizer, max_length):\n","        self.df = df\n","        self.max_len = max_length\n","        self.tokenizer = tokenizer\n","        self.text = df['#2_content'].values\n","        self.label=df['#3_label'].values\n","        \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, index):\n","        text = self.text[index]\n","        # summary = self.summary[index]\n","        inputs_text = self.tokenizer.encode_plus(\n","                                text,\n","                                truncation=True,\n","                                add_special_tokens=True,\n","                                max_length=self.max_len,\n","                                padding='max_length'\n","                            )\n","        \n","                            \n","        target = self.label[index]\n","        \n","        text_ids = inputs_text['input_ids']\n","        text_mask = inputs_text['attention_mask']\n","        \n","       \n","        \n","        \n","        return {\n","            \n","            'text_ids': torch.tensor(text_ids, dtype=torch.long),\n","            'text_mask': torch.tensor(text_mask, dtype=torch.long),\n","            'target': torch.tensor(target, dtype=torch.float)\n","        }"],"metadata":{"id":"lLRNRjusoELI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import pandas as pd\n","# from sklearn.model_selection import train_test_split\n","# from torch.utils.data import Dataset, DataLoader\n","# import Dataset\n","# import os\n","# import numpy as np\n","# from emoji import UNICODE_EMOJI\n","# import TweetNormalizer\n","# import re\n","# import text_normalization\n","\n","\n","# dic = {\n","#       \"egypt\": 'المصرية',\n","# \t  \"nile\": 'المصرية',\n","# \t  \"msa\": \"اللغة العربية الفصحى\",\n","# \t  \"magreb\": \"المغربية\",\n","# \t  \"gulf\": \"الخليجية\",\n","# \t  \"levant\": \"الشامية\"\n","# }\n","\n","# def is_emoji(s):\n","#     return s in UNICODE_EMOJI\n","\n","# # add space near your emoji\n","# def add_space(text):\n","#     return ''.join(' ' + char if is_emoji(char) else char for char in text).strip()\n","\n","# def preprocess(text, lang='ar'):\n","#     if lang == 'ar':\n","#         sent = add_space(text)\n","#         sent = re.sub(r'(?:@[\\w_]+)', \"user\", sent)\n","#         sent = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', \"url\", sent)\n","#         sent = sent.replace('_', ' ')\n","#         sent = sent.replace('#', ' ')\n","#     else:\n","#         sent = add_space(text)\n","#         sent = re.sub(r'(?:@[\\w_]+)', \"@user\", sent)\n","#         sent = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', \"http\", sent)\n","#         sent = sent.replace('_', ' ')\n","#         sent = sent.replace('#', ' ')\n","\n","#     return sent\n","\n","# def prepare_text(df, col='tweet'):\n","#     if col == 'tweet':\n","#         df['dialect'] = df['dialect'].map(dic)   \n","#     for i in range(df.shape[0]):\n","#         df.loc[i, col] = df.loc[i, 'dialect'] + ' [SEP] ' + df.loc[i, col]\n","\n","\n","#     return df\n","\n","# def augment_data(df_train,text_col,label_col):\n","#     df_aug = pd.DataFrame(columns=[text_col, label_col)\n","#     dic_dup = {1: 3,\n","#                0: 1\n","#                }\n","#     for i in range(df_train.shape[0]):\n","#         current = df_train.iloc[i]\n","#         text = current[text_col]\n","#         label_cat = current[label_col]\n","\n","#         aug_ratio = dic_dup[label_cat]\n","#         for k in range(aug_ratio):\n","#             tokens = text.split(' ')\n","#             l = len(tokens)\n","#             n = int(0.1 * l)\n","#             indices = np.random.choice(l, n, replace=False)\n","#             for j in range(len(indices)):\n","#                 tokens[indices[j]] = '[MASK]'\n","#             new_text = ' '.join(tokens)\n","#             entry = {text_col: new_text, label_col: label_cat}\n","#             df_aug = df_aug.append(entry, ignore_index=True)\n","#     df_aug.drop_duplicates(subset=[text_col], keep='first', inplace=True)\n","#     df = pd.concat([df_train,df_aug])\n","#     return df\n"],"metadata":{"id":"0ZOanFa2wz3L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# '''\n","# Created by: Mohamed Salem Elhady  \n","# Email: mohamed.elaraby@alumni.ubc.ca\n","# Text Normalization: V1 \n","# '''\n","# import sys\n","# import re\n","# import emojis\n","# from emoji import UNICODE_EMOJI\n","# #sys.setdefaultencoding('utf-8')\n","# ##########################Clean Text Data #######################################\n","# ########################Global Variable Declaration##############################\n","# list_seeds = ['سبحان الله', 'الله أكبر', 'اللهم', 'بسم الله', 'يا رب', 'العضيم', 'سبحان', 'يارب', 'قران', 'quran',\n","#               'حديث', 'hadith', 'صلاه_الفجر', '﴾', 'ﷺ', 'صحيح البخاري', 'صحيح مسلم', 'يآرب', 'سورة']\n","# MaxWordPerTweet=7\n","# #################################################################################\n","# def is_emoji(s):\n","#     return s in UNICODE_EMOJI\n","\n","# # add space near your emoji\n","# def add_space(text):\n","#     return ''.join(' ' + char if is_emoji(char) else char for char in text).strip()\n","\n","# def clean(sent):\n","#     \"\"\"clean data from any English char, emoticons, underscore, and repeated > 2\n","#     str -> str\"\"\"\n","#     p1 = re.compile('\\W')\n","#     p2 = re.compile('\\s+')\n","#     sent = re.sub(r\"http\\S+\", \"\", sent)\n","#     sent = ReplaceThreeOrMore(sent)\n","#     sent = remove_unicode_diac(sent)\n","#     sent = sent.replace('_', ' ')\n","#     sent = re.sub(r'[A-Za-z0-9]', r'', sent)\n","#     sent = re.sub(p1, ' ', sent)\n","#     sent = re.sub(p2, ' ', sent)\n","#     return sent\n","\n","# def tokenize_emojis(tweet):\n","#     return list(emojis.get(tweet))\n","\n","# def replace_emoji(sent):\n","#     emoji_pattern = re.compile(\"[\"\n","#                                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","#                                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","#                                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","#                                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","#                                \"]+\", flags=re.UNICODE)\n","#     return emoji_pattern.sub(r'[MASK]', sent)\n","\n","# def preprocess(tweet):\n","#     tweet = add_space(tweet)\n","#     emos = tokenize_emojis(tweet)\n","#     sent = remove_unicode_diac(tweet)\n","#     sent = re.sub(r'(?:@[\\w_]+)', \"user\", sent)\n","#     sent = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', \"url\", sent)\n","#     sent = sent.replace('_', ' ')\n","#     sent = sent.replace('#', ' ')\n","#     if len(emos) > 0:\n","#         sent = sent + ' [SEP] '  + ' '.join(emos)\n","#     #    #sent = sent + ' [SEP] ' + clean_unicode(tweet) + ' [SEP] ' + ' '.join(emos)\n","\n","#     #else:\n","#     #    sent = sent + ' [SEP] ' + clean_unicode(tweet)\n","#     return sent\n","\n","# def preprocess_last(tweet, k=0):\n","#     emos = tokenize_emojis(tweet)\n","#     sent = remove_unicode_diac(tweet)\n","#     sent = re.sub(r'(?:@[\\w_]+)', \"\", sent)\n","#     sent = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', \"\", sent)\n","#     sent = sent.replace('_', ' ')\n","#     sent = sent.replace('#', ' ')\n","#     if k == 0:\n","#         sent = sent\n","#     elif k ==1 :\n","#         sent = sent + ' [SEP] ' + ' '.join(emos)\n","#     elif k==2 :\n","#         sent = sent + ' [SEP] ' + clean_unicode(tweet) + ' [SEP] ' + ' '.join(emos)\n","#     elif k == 3:\n","#         sent = sent + ' [SEP] ' + clean_unicode(tweet)\n","#     else:\n","#         sent = replace_emoji(sent)\n","#         sent = sent + ' [SEP] ' + clean_unicode(tweet) + ' [SEP] ' + ' '.join(emos)\n","\n","#     return sent\n","\n","\n","# def normalize(sent):\n","#     \"\"\"clean data from any English char, emoticons, underscore, and repeated > 2\n","#     str -> str\"\"\"\n","#     sent = re.sub(r'(?:@[\\w_]+)', \"user\", sent)\n","#     sent = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', \"url\", sent)\n","#     #sent = re.sub(r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", \"hashtag\", sent)\n","#     sent = ReplaceThreeOrMore(sent)\n","#     sent = remove_unicode_diac(sent)\n","#     sent = sent.replace('_', ' ')\n","#     return sent\n","\n","# def ReplaceThreeOrMore(s):\n","#     # pattern to look for three or more repetitions of any character, including\n","#     # newlines.\n","#     pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL)\n","#     return pattern.sub(r\"\\1\\1\", s)\n","# def norm_alif(text):\n","#     text = text.replace(u\"\\u0625\", u\"\\u0627\")  # HAMZA below, with LETTER ALEF\n","#     #text = text.replace(u\"\\u0621\", u\"\\u0627\")  # HAMZA, with LETTER ALEF\n","#     text = text.replace(u\"\\u0622\", u\"\\u0627\")  # ALEF WITH MADDA ABOVE, with LETTER ALEF\n","#     text = text.replace(u\"\\u0623\", u\"\\u0627\")  # ALEF WITH HAMZA ABOVE, with LETTER ALEF\n","#     return text\n","# def remove_unicode_diac(text):\n","#     \"\"\"Takes Arabic in utf-8 and returns same text without diac\"\"\"\n","#     # Replace diacritics with nothing\n","#     text = text.replace(u\"\\u064B\", \"\")  # fatHatayn\n","#     text = text.replace(u\"\\u064C\", \"\")  # Dammatayn\n","#     text = text.replace(u\"\\u064D\", \"\")  # kasratayn\n","#     text = text.replace(u\"\\u064E\", \"\")  # fatHa\n","#     text = text.replace(u\"\\u064F\", \"\")  # Damma\n","#     text = text.replace(u\"\\u0650\", \"\")  # kasra\n","#     text = text.replace(u\"\\u0651\", \"\")  # shaddah\n","#     text = text.replace(u\"\\u0652\", \"\")  # sukuun\n","#     text = text.replace(u\"\\u0670\", \"`\")  # dagger 'alif\n","#     return text\n","# def norm_taa(text):\n","#     text=text.replace(u\"\\u0629\", u\"\\u0647\") # taa' marbuuTa, with haa'\n","#     #text=text.replace(u\"\\u064A\", u\"\\u0649\") # yaa' with 'alif maqSuura\n","#     return text\n","# def norm_yaa(text):\n","#     if len(text)!=0:\n","#         if text[-1] == u\"\\u064A\":\n","#             text = text[:-1] + text[-1].replace(u\"\\u064A\", u\"\\u0649\")  # yaa' with 'alif maqSuura\n","#     return text\n","\n","# def NormForWord2Vec(text):\n","#     text=norm_taa(text)\n","#     text=norm_yaa(text)\n","#     text=norm_alif(text)\n","#     return text\n","\n","# def remove_nonunicode2(Tweet):\n","#     ## defining set of unicode ##\n","#     #u\"\"\n","#     #Tweet=Tweet.decode(\"utf-8\")\n","#     UniLex={ ## This is list of all arabic unicode characters in addition to space (to separate words)\n","#             u\"\\u0622\",\n","#             u\"\\u0626\",\n","#             u\"\\u0628\",\n","#             u\"\\u062a\",\n","#             u\"\\u062c\",\n","#             u\"\\u06af\",\n","#             u\"\\u062e\",\n","#             u\"\\u0630\",\n","#             u\"\\u0632\",\n","#             u\"\\u0634\",\n","#             u\"\\u0636\",\n","#             u\"\\u0638\",\n","#             u\"\\u063a\",\n","#             u\"\\u0640\",\n","#             u\"\\u0642\",\n","#             u\"\\u0644\",\n","#             u\"\\u0646\",\n","#             u\"\\u0648\",\n","#             u\"\\u064a\",\n","#             u\"\\u0670\",\n","#             u\"\\u067e\",\n","#             u\"\\u0686\",\n","#             u\"\\u0621\",\n","#             u\"\\u0623\",\n","#             u\"\\u0625\",\n","#             u\"\\u06a4\",\n","#             u\"\\u0627\",\n","#             u\"\\u0629\",\n","#             u\"\\u062b\",\n","#             u\"\\u062d\",\n","#             u\"\\u062f\",\n","#             u\"\\u0631\",\n","#             u\"\\u0633\",\n","#             u\"\\u0635\",\n","#             u\"\\u0637\",\n","#             u\"\\u0639\",\n","#             u\"\\u0641\",\n","#             u\"\\u0643\",\n","#             u\"\\u0645\",\n","#             u\"\\u0647\",\n","#             u\"\\u0649\",\n","#             u\"\\u0671\",\n","#             ' ',\n","#             '\\n'\n","#           }\n","#     fin_tweet=\"\"\n","#     for c in Tweet:\n","#         if c in UniLex:\n","#            fin_tweet=fin_tweet+c\n","#     return fin_tweet\n","\n","# ###### Heuristics Calculations ######\n","# def diac_counter(text):\n","#     #text=text.decode(\"utf-8\")\n","#     diac = [u\"\\u064B\",u\"\\u064C\", u\"\\u064D\", u\"\\u064E\", u\"\\u064F\", u\"\\u0650\", u\"\\u0651\", u\"\\u0652\", u\"\\u0670\"]\n","#     diac_count=0\n","#     for d in diac:\n","#         diac_count+=text.count(d)\n","# #         if d in text:\n","# #             print(d)\n","# #             diac_count+=1\n","#     return diac_count\n","# def check_seed(list_seeds, text):\n","#     \"\"\n","#     for word in list_seeds:\n","#         text = text.lower()\n","#         if word.decode(\"utf-8\") in text:\n","#             return True\n","#     return False\n","# def EnglishCount(text):\n","#     printable = ['e', 'a', 'o', 't', 'i']\n","#     count = 0\n","#     for ch in printable:\n","#         count += text.count(ch.lower())\n","#     return count\n","# ########################################\n","\n","\n","\n","# def eliminate_single_char_words(Tweet):\n","#     parts = Tweet.split(\" \")\n","#     cleaned_line_parts = []\n","#     for P in parts:\n","#         if len(P) != 1:\n","#             cleaned_line_parts.append(P)\n","#     cleaned_line = ' '.join(cleaned_line_parts)\n","#     return cleaned_line\n","# def clean_unicode(Tweet):\n","#     tweet=normalize(Tweet.strip(\"\\n\"))\n","#     if len(tweet) !=0:\n","#         sentence = []\n","#         for word in tweet.split(\" \"):\n","#             word = remove_unicode_diac(word)\n","#             word = norm_alif(word)\n","#             word = norm_taa(word)\n","#             word = norm_yaa(word)\n","#             word = normalize(word)\n","#             sentence.append(word)\n","#         tweet = ' '.join(sentence)\n","#         tweet =remove_nonunicode2(tweet)\n","#         tweet =eliminate_single_char_words(tweet)\n","#     return tweet\n","\n","# def clean_unicode2(Tweet):\n","#     KeepUniOnly(Tweet)\n","#     tweet=normalize(Tweet.strip(\"\\n\"))\n","#     if len(tweet) !=0:\n","#         sentence = []\n","#         for word in tweet.split(\" \"):\n","#             word = remove_unicode_diac(word)\n","#             word = normalize(word)\n","#             sentence.append(word)\n","#         tweet = ' '.join(sentence)\n","#         tweet =remove_nonunicode2(tweet)\n","#         tweet =eliminate_single_char_words(tweet)\n","#     return tweet\n","\n","# def NormCorpusFinal(Tweet):\n","#     tweet=KeepUniOnly(Tweet)\n","#     tweet=NormForWord2Vec(tweet)\n","#     return tweet\n","\n","# def KeepUniOnly(Tweet):## this one is without normalization\n","#     tweet=Tweet.replace(\"# \",\" \")\n","#     tweet=tweet.replace(\"#\",\" \")\n","#     tweet=tweet.replace(\"_\",\" \")\n","#     tweet=tweet.replace(u\"\\u0657\",\" \")\n","#     tweet=tweet.replace(\"\\n\",\" \")\n","#     tweet=remove_nonunicode2(tweet)\n","#     tweet=eliminate_single_char_words(tweet)\n","#     tweet=ReplaceThreeOrMore(tweet)\n","#     return tweet\n","\n","# def get_charset(rawtext):\n","#     chars = sorted(list(set(rawtext)))\n","#     return chars\n","\n","# def DialectChecker(text):\n","#     ##Based on Hueristics done by Hassan\n","#     if (diac_counter(text)>5 or check_seed(list_seeds,text) or EnglishCount(text)>4 or \"<URL>\"  in text\n","#         or text.count('#') >2 or '\"'  in text or text.count('@') or \"\\\"RT\" in text or len(text.split(\" \")) <7):\n","#         return False\n","#     else:\n","#         return True\n","\n","# ###############################################################\n","# '''\n","# Fread=open(\"Egypt_portion.txt\",'r')\n","# Fwriter=open(\"Egypt_portion_norm.txt\",'w')\n","# for line in Fread:\n","#     cleaned_line=clean_unicode_for_w2v(line)\n","#     Fwriter.write(str(cleaned_line))\n","# Fwriter.close()\n","# '''"],"metadata":{"id":"QcOa9agTwhIg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Losses"],"metadata":{"id":"iVcFbe1Bvcqh"}},{"cell_type":"code","source":["class F1_Loss(nn.Module):\n","    '''Calculate F1 score. Can work with gpu tensors\n","    \n","    The original implmentation is written by Michal Haltuf on Kaggle.\n","    \n","    Returns\n","    -------\n","    torch.Tensor\n","        `ndim` == 1. epsilon <= val <= 1\n","    \n","    Reference\n","    ---------\n","    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n","    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n","    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\n","    - http://www.ryanzhang.info/python/writing-your-own-loss-function-module-for-pytorch/\n","    '''\n","    def __init__(self, epsilon=1e-7):\n","        super().__init__()\n","        self.epsilon = epsilon\n","        \n","    def forward(self, y_pred, y_true,):\n","        # assert y_pred.ndim == 2\n","        # assert y_true.ndim == 1\n","        # print(y_pred.shape)\n","        # print(y_true.shape)\n","        # y_pred[y_pred<0.5]=0\n","        # y_pred[y_pred>=0.5]=0\n","\n","\n","        \n","        y_true_one_hot = F.one_hot(y_true.to(torch.int64), 18).to(torch.float32)\n","        # y_pred_one_hot = F.one_hot(y_pred.to(torch.int64), 2).to(torch.float32)\n","        \n","        tp = (y_true_one_hot * y_pred).sum(dim=0).to(torch.float32)\n","        tn = ((1 - y_true_one_hot) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n","        fp = ((1 - y_true_one_hot) * y_pred).sum(dim=0).to(torch.float32)\n","        fn = (y_true_one_hot * (1 - y_pred)).sum(dim=0).to(torch.float32)\n","\n","        precision = tp / (tp + fp + self.epsilon)\n","        recall = tp / (tp + fn + self.epsilon)\n","\n","        f1 = 2* (precision*recall) / (precision + recall + self.epsilon)\n","        f1 = f1.clamp(min=self.epsilon, max=1-self.epsilon)\n","        f1=f1.detach()\n","        # print(f1.shape)\n","        # y_pred=y_pred.reshape((y_pred.shape[0], 1))\n","        # y_true=y_true.reshape((y_true.shape[0], 1))\n","\n","        # p1=y_true*(math.log(sigmoid(y_pred)))*(1-f1)[1]\n","        # p0=(1-y_true)*math.log(1-sigmoid(y_pred))*(1-f1)[0]\n","\n","\n","        # y_true_one_hot = F.one_hot(y_true.to(torch.int64), 2)\n","        # print(y_pred)\n","        # print(y_true_one_hot)\n","        CE =torch.nn.CrossEntropyLoss(weight=( 1 - f1))(y_pred, y_true_one_hot)\n","        # loss = ( 1 - f1)  * CE\n","        return  CE.mean()"],"metadata":{"id":"RGw6mbs0uIlN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Recall_Loss(nn.Module):\n","    '''Calculate Recall score. Can work with gpu tensors\n","    \n","    The original implmentation is written by Michal Haltuf on Kaggle.\n","    \n","    Returns\n","    -------\n","    torch.Tensor\n","        `ndim` == 1. epsilon <= val <= 1\n","    \n","    Reference\n","    ---------\n","    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n","    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n","    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\n","    - http://www.ryanzhang.info/python/writing-your-own-loss-function-module-for-pytorch/\n","    '''\n","    def __init__(self, epsilon=1e-7):\n","        super().__init__()\n","        self.epsilon = epsilon\n","        \n","    def forward(self, y_pred, y_true,):\n","        # assert y_pred.ndim == 2\n","        # assert y_true.ndim == 1\n","        # print(y_pred.shape)\n","        # print(y_true.shape)\n","        # y_pred[y_pred<0.5]=0\n","        # y_pred[y_pred>=0.5]=0\n","\n","\n","        \n","        y_true_one_hot = F.one_hot(y_true.to(torch.int64), 18).to(torch.float32)\n","        # y_pred_one_hot = F.one_hot(y_pred.to(torch.int64), 2).to(torch.float32)\n","        \n","        tp = (y_true_one_hot * y_pred).sum(dim=0).to(torch.float32)\n","        tn = ((1 - y_true_one_hot) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n","        fp = ((1 - y_true_one_hot) * y_pred).sum(dim=0).to(torch.float32)\n","        fn = (y_true_one_hot * (1 - y_pred)).sum(dim=0).to(torch.float32)\n","\n","        precision = tp / (tp + fp + self.epsilon)\n","        recall = tp / (tp + fn + self.epsilon)\n","\n","        # f1 = 2* (precision*recall) / (precision + recall + self.epsilon)\n","        # f1 = f1.clamp(min=self.epsilon, max=1-self.epsilon)\n","        # f1=f1.detach()\n","        # print(f1.shape)\n","        # y_pred=y_pred.reshape((y_pred.shape[0], 1))\n","        # y_true=y_true.reshape((y_true.shape[0], 1))\n","\n","        # p1=y_true*(math.log(sigmoid(y_pred)))*(1-f1)[1]\n","        # p0=(1-y_true)*math.log(1-sigmoid(y_pred))*(1-f1)[0]\n","\n","\n","        # y_true_one_hot = F.one_hot(y_true.to(torch.int64), 2)\n","        # print(y_pred)\n","        # print(y_true_one_hot)\n","        recall=recall.detach()\n","        CE =torch.nn.CrossEntropyLoss(weight=( 1 - recall))(y_pred, y_true_one_hot)\n","        # loss = ( 1 - f1)  * CE\n","        return  CE.mean()"],"metadata":{"id":"Pdd02bGivpr5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Based on https://github.com/AbdelkaderMH/iSarcasmEval/blob/8f28f24ebfb641415a604329ed859506ae687148/focal_loss.py\n","class BinaryFocalLoss(nn.Module):\n","    \"\"\"\n","    This is a implementation of Focal Loss with smooth label cross entropy supported which is proposed in\n","    'Focal Loss for Dense Object Detection. (https://arxiv.org/abs/1708.02002)'\n","        Focal_Loss= -1*alpha*(1-pt)*log(pt)\n","    :param alpha: (tensor) 3D or 4D the scalar factor for this criterion\n","    :param gamma: (float,double) gamma > 0 reduces the relative loss for well-classified examples (p>0.5) putting more\n","                    focus on hard misclassified example\n","    :param reduction: `none`|`mean`|`sum`\n","    :param **kwargs\n","        balance_index: (int) balance class index, should be specific when alpha is float\n","    \"\"\"\n","\n","    def __init__(self, alpha=3, gamma=2, ignore_index=None, reduction='mean', **kwargs):\n","        super(BinaryFocalLoss, self).__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","        self.smooth = 1e-6  # set '1e-4' when train with FP16\n","        self.ignore_index = ignore_index\n","        self.reduction = reduction\n","\n","        assert self.reduction in ['none', 'mean', 'sum']\n","\n","        # if self.alpha is None:\n","        #     self.alpha = torch.ones(2)\n","        # elif isinstance(self.alpha, (list, np.ndarray)):\n","        #     self.alpha = np.asarray(self.alpha)\n","        #     self.alpha = np.reshape(self.alpha, (2))\n","        #     assert self.alpha.shape[0] == 2, \\\n","        #         'the `alpha` shape is not match the number of class'\n","        # elif isinstance(self.alpha, (float, int)):\n","        #     self.alpha = np.asarray([self.alpha, 1.0 - self.alpha], dtype=np.float).view(2)\n","\n","        # else:\n","        #     raise TypeError('{} not supported'.format(type(self.alpha)))\n","\n","    def forward(self, output, target):\n","        prob = torch.sigmoid(output)\n","        prob = torch.clamp(prob, self.smooth, 1.0 - self.smooth)\n","\n","        valid_mask = None\n","        if self.ignore_index is not None:\n","            valid_mask = (target != self.ignore_index).float()\n","\n","        pos_mask = (target == 1).float()\n","        neg_mask = (target == 0).float()\n","        if valid_mask is not None:\n","            pos_mask = pos_mask * valid_mask\n","            neg_mask = neg_mask * valid_mask\n","\n","        pos_weight = (pos_mask * torch.pow(1 - prob, self.gamma)).detach()\n","        pos_loss = -pos_weight * torch.log(prob)  # / (torch.sum(pos_weight) + 1e-4)\n","\n","        neg_weight = (neg_mask * torch.pow(prob, self.gamma)).detach()\n","        neg_loss = -self.alpha * neg_weight * F.logsigmoid(-output)  # / (torch.sum(neg_weight) + 1e-4)\n","        loss = pos_loss + neg_loss\n","        loss = loss.mean()\n","        return loss\n","\n","\n","class FocalLoss_Ori(nn.Module):\n","    \"\"\"\n","    This is a implementation of Focal Loss with smooth label cross entropy supported which is proposed in\n","    'Focal Loss for Dense Object Detection. (https://arxiv.org/abs/1708.02002)'\n","    Focal_Loss= -1*alpha*((1-pt)**gamma)*log(pt)\n","    Args:\n","        num_class: number of classes\n","        alpha: class balance factor\n","        gamma:\n","        ignore_index:\n","        reduction:\n","    \"\"\"\n","\n","    def __init__(self, num_class, alpha=None, gamma=2, ignore_index=None, reduction='mean'):\n","        super(FocalLoss_Ori, self).__init__()\n","        self.num_class = num_class\n","        self.gamma = gamma\n","        self.reduction = reduction\n","        self.smooth = 1e-4\n","        self.ignore_index = ignore_index\n","        self.alpha = alpha\n","        if alpha is None:\n","            self.alpha = torch.ones(num_class, )\n","        elif isinstance(alpha, (int, float)):\n","            self.alpha = torch.as_tensor([alpha] * num_class)\n","        elif isinstance(alpha, (list, np.ndarray)):\n","            self.alpha = torch.as_tensor(alpha)\n","        if self.alpha.shape[0] != num_class:\n","            raise RuntimeError('the length not equal to number of class')\n","\n","        # if isinstance(self.alpha, (list, tuple, np.ndarray)):\n","        #     assert len(self.alpha) == self.num_class\n","        #     self.alpha = torch.Tensor(list(self.alpha))\n","        # elif isinstance(self.alpha, (float, int)):\n","        #     assert 0 < self.alpha < 1.0, 'alpha should be in `(0,1)`)'\n","        #     assert balance_index > -1\n","        #     alpha = torch.ones((self.num_class))\n","        #     alpha *= 1 - self.alpha\n","        #     alpha[balance_index] = self.alpha\n","        #     self.alpha = alpha\n","        # elif isinstance(self.alpha, torch.Tensor):\n","        #     self.alpha = self.alpha\n","        # else:\n","        #     raise TypeError('Not support alpha type, expect `int|float|list|tuple|torch.Tensor`')\n","\n","    def forward(self, logit, target):\n","        # assert isinstance(self.alpha,torch.Tensor)\\\n","        N, C = logit.shape[:2]\n","        alpha = self.alpha.to(logit.device)\n","        prob = F.softmax(logit, dim=1)\n","        if prob.dim() > 2:\n","            # N,C,d1,d2 -> N,C,m (m=d1*d2*...)\n","            prob = prob.view(N, C, -1)\n","            prob = prob.transpose(1, 2).contiguous()  # [N,C,d1*d2..] -> [N,d1*d2..,C]\n","            prob = prob.view(-1, prob.size(-1))  # [N,d1*d2..,C]-> [N*d1*d2..,C]\n","        ori_shp = target.shape\n","        target = target.view(-1, 1)  # [N,d1,d2,...]->[N*d1*d2*...,1]\n","        valid_mask = None\n","        if self.ignore_index is not None:\n","            valid_mask = target != self.ignore_index\n","            target = target * valid_mask\n","\n","        # ----------memory saving way--------\n","        prob = prob.gather(1, target).view(-1) + self.smooth  # avoid nan\n","        logpt = torch.log(prob)\n","        # alpha_class = alpha.gather(0, target.view(-1))\n","        alpha_class = alpha[target.squeeze().long()]\n","        class_weight = -alpha_class * torch.pow(torch.sub(1.0, prob), self.gamma)\n","        loss = class_weight * logpt\n","        if valid_mask is not None:\n","            loss = loss * valid_mask.squeeze()\n","\n","        if self.reduction == 'mean':\n","            loss = loss.mean()\n","            if valid_mask is not None:\n","                loss = loss.sum() / valid_mask.sum()\n","        elif self.reduction == 'none':\n","            loss = loss.view(ori_shp)\n","        return loss\n","\n","\n","\n","def weighted_binary_cross_entropy(input, targets, pos_weight, weight=None, size_average=True, reduce=True):\n","    \"\"\"\n","    Args:\n","        sigmoid_x: predicted probability of size [N,C], N sample and C Class. Eg. Must be in range of [0,1], i.e. Output from Sigmoid.\n","        targets: true value, one-hot-like vector of size [N,C]\n","        pos_weight: Weight for postive sample\n","    \"\"\"\n","    sigmoid_x = torch.sigmoid(input)\n","    if not (targets.size() == sigmoid_x.size()):\n","        raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(targets.size(), sigmoid_x.size()))\n","\n","    loss = -pos_weight* targets * sigmoid_x.log() - (1-targets)*(1-sigmoid_x).log()\n","\n","    if weight is not None:\n","        loss = loss * weight\n","\n","    if not reduce:\n","        return loss\n","    elif size_average:\n","        return loss.mean()\n","    else:\n","        return loss.sum()\n","\n","class WeightedBCELoss(nn.Module):\n","    def __init__(self, pos_weight= 1, weight=None, PosWeightIsDynamic= True, WeightIsDynamic= False, size_average=True, reduce=True):\n","        \"\"\"\n","        Args:\n","            pos_weight = Weight for postive samples. Size [1,C]\n","            weight = Weight for Each class. Size [1,C]\n","            PosWeightIsDynamic: If True, the pos_weight is computed on each batch. If pos_weight is None, then it remains None.\n","            WeightIsDynamic: If True, the weight is computed on each batch. If weight is None, then it remains None.\n","        \"\"\"\n","        super().__init__()\n","\n","        #self.register_buffer('weight', weight)\n","        #self.register_buffer('pos_weight', pos_weight)\n","        self.size_average = size_average\n","        self.reduce = reduce\n","        self.PosWeightIsDynamic = PosWeightIsDynamic\n","\n","    def forward(self, input, target):\n","        # pos_weight = Variable(self.pos_weight) if not isinstance(self.pos_weight, Variable) else self.pos_weight\n","        if self.PosWeightIsDynamic:\n","            positive_counts = target.sum(dim=0)\n","            nBatch = len(target)\n","            self.pos_weight = (nBatch - positive_counts)/(positive_counts +1e-5)\n","\n","\n","        return weighted_binary_cross_entropy(input, target,\n","                                                self.pos_weight,\n","                                                weight=None,\n","                                                size_average=self.size_average,\n","                                                reduce=self.reduce)\n","\n","\n","class WeightedCELoss(nn.Module):\n","    def __init__(self, size_average=True, reduce=True):\n","        \"\"\"\n","        Args:\n","            pos_weight = Weight for postive samples. Size [1,C]\n","            weight = Weight for Each class. Size [1,C]\n","            PosWeightIsDynamic: If True, the pos_weight is computed on each batch. If pos_weight is None, then it remains None.\n","            WeightIsDynamic: If True, the weight is computed on each batch. If weight is None, then it remains None.\n","        \"\"\"\n","        super().__init__()\n","\n","        self.size_average = size_average\n","        self.reduce = reduce\n","\n","    def forward(self, input, target):\n","        positive_counts = target.sum(dim=0)\n","        nBatch = len(target)\n","        pos_weight = (nBatch - positive_counts)/(positive_counts +1e-5)\n","        neg_count = nBatch - positive_counts\n","        neg_weight = (nBatch - neg_count)/(neg_count +1e-5)\n","\n","        weight = torch.tensor([neg_weight, pos_weight], device=target.device)\n","  \n","\n","\n","        return F.cross_entropy(input, target, weight=weight)\n","\n","\n","\n","class FLMultiLoss(nn.Module):\n","    def __init__(self, gamma= 2):\n","        \"\"\"\n","        Args:\n","            pos_weight = Weight for postive samples. Size [1,C]\n","            weight = Weight for Each class. Size [1,C]\n","            PosWeightIsDynamic: If True, the pos_weight is computed on each batch. If pos_weight is None, then it remains None.\n","            WeightIsDynamic: If True, the weight is computed on each batch. If weight is None, then it remains None.\n","        \"\"\"\n","        super().__init__()\n","\n","        self.gamma = gamma\n","    def forward(self, input, target):\n","\n","\n","        return focal_binary_cross_entropy(input, target, gamma=2)\n","\n","def EntropyLoss(input_):\n","    mask = input_.ge(0.000001)\n","    mask_out = torch.masked_select(input_, mask)\n","    entropy = -(torch.sum(mask_out * torch.log(mask_out)))\n","    return entropy / float(input_.size(0))\n","\n","def focal_binary_cross_entropy(logits, targets, gamma=2):\n","    num_label = targets.shape[1]\n","    l = logits.reshape(-1)\n","    t = targets.reshape(-1)\n","    p = torch.sigmoid(l)\n","    p = torch.where(t >= 0.5, p, 1-p)\n","    logp = - torch.log(torch.clamp(p, 1e-4, 1-1e-4))\n","    loss = logp*((1-p)**gamma)\n","    loss = num_label*loss.mean()\n","    return loss"],"metadata":{"id":"zKy23cw1wHB9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing import Optional, Sequence\n","\n","import torch\n","from torch import Tensor\n","from torch import nn\n","from torch.nn import functional as F\n","\n","\n","class FocalLoss(nn.Module):\n","    \"\"\" Focal Loss, as described in https://arxiv.org/abs/1708.02002.\n","    It is essentially an enhancement to cross entropy loss and is\n","    useful for classification tasks when there is a large class imbalance.\n","    x is expected to contain raw, unnormalized scores for each class.\n","    y is expected to contain class labels.\n","    Shape:\n","        - x: (batch_size, C) or (batch_size, C, d1, d2, ..., dK), K > 0.\n","        - y: (batch_size,) or (batch_size, d1, d2, ..., dK), K > 0.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 alpha: Optional[Tensor] = None,\n","                 gamma: float = 0.,\n","                 reduction: str = 'mean',\n","                 ignore_index: int = -100):\n","        \"\"\"Constructor.\n","        Args:\n","            alpha (Tensor, optional): Weights for each class. Defaults to None.\n","            gamma (float, optional): A constant, as described in the paper.\n","                Defaults to 0.\n","            reduction (str, optional): 'mean', 'sum' or 'none'.\n","                Defaults to 'mean'.\n","            ignore_index (int, optional): class label to ignore.\n","                Defaults to -100.\n","        \"\"\"\n","        if reduction not in ('mean', 'sum', 'none'):\n","            raise ValueError(\n","                'Reduction must be one of: \"mean\", \"sum\", \"none\".')\n","\n","        super().__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","        self.ignore_index = ignore_index\n","        self.reduction = reduction\n","\n","        self.nll_loss = nn.NLLLoss(\n","            weight=alpha, reduction='none', ignore_index=ignore_index)\n","\n","    def __repr__(self):\n","        arg_keys = ['alpha', 'gamma', 'ignore_index', 'reduction']\n","        arg_vals = [self.__dict__[k] for k in arg_keys]\n","        arg_strs = [f'{k}={v}' for k, v in zip(arg_keys, arg_vals)]\n","        arg_str = ', '.join(arg_strs)\n","        return f'{type(self).__name__}({arg_str})'\n","\n","    def forward(self, x: Tensor, y: Tensor) -> Tensor:\n","        if x.ndim > 2:\n","            # (N, C, d1, d2, ..., dK) --> (N * d1 * ... * dK, C)\n","            c = x.shape[1]\n","            x = x.permute(0, *range(2, x.ndim), 1).reshape(-1, c)\n","            # (N, d1, d2, ..., dK) --> (N * d1 * ... * dK,)\n","            y = y.view(-1)\n","\n","        unignored_mask = y != self.ignore_index\n","        y = y[unignored_mask]\n","        if len(y) == 0:\n","            return torch.tensor(0.)\n","        x = x[unignored_mask]\n","\n","        # compute weighted cross entropy term: -alpha * log(pt)\n","        # (alpha is already part of self.nll_loss)\n","        log_p = F.log_softmax(x, dim=-1)\n","        ce = self.nll_loss(log_p, y)\n","\n","        # get true class column from each row\n","        all_rows = torch.arange(len(x))\n","        log_pt = log_p[all_rows, y]\n","\n","        # compute focal term: (1 - pt)^gamma\n","        pt = log_pt.exp()\n","        focal_term = (1 - pt)**self.gamma\n","\n","        # the full loss: -alpha * ((1 - pt)^gamma) * log(pt)\n","        loss = focal_term * ce\n","\n","        if self.reduction == 'mean':\n","            loss = loss.mean()\n","        elif self.reduction == 'sum':\n","            loss = loss.sum()\n","\n","        return loss\n","\n","\n","def focal_loss(alpha: Optional[Sequence] = None,\n","               gamma: float = 0.,\n","               reduction: str = 'mean',\n","               ignore_index: int = -100,\n","               device='cuda',\n","               dtype=torch.float32) -> FocalLoss:\n","    \"\"\"Factory function for FocalLoss.\n","    Args:\n","        alpha (Sequence, optional): Weights for each class. Will be converted\n","            to a Tensor if not None. Defaults to None.\n","        gamma (float, optional): A constant, as described in the paper.\n","            Defaults to 0.\n","        reduction (str, optional): 'mean', 'sum' or 'none'.\n","            Defaults to 'mean'.\n","        ignore_index (int, optional): class label to ignore.\n","            Defaults to -100.\n","        device (str, optional): Device to move alpha to. Defaults to 'cpu'.\n","        dtype (torch.dtype, optional): dtype to cast alpha to.\n","            Defaults to torch.float32.\n","    Returns:\n","        A FocalLoss object\n","    \"\"\"\n","    if alpha is not None:\n","        if not isinstance(alpha, Tensor):\n","            alpha = torch.tensor(alpha)\n","        alpha = alpha.to(device=device, dtype=dtype)\n","\n","    fl = FocalLoss(\n","        alpha=alpha,\n","        gamma=gamma,\n","        reduction=reduction,\n","        ignore_index=ignore_index)\n","    return fl"],"metadata":{"id":"dG5x6Ck0nOh_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Models"],"metadata":{"id":"fvByMHwKuI4d"}},{"cell_type":"code","source":["import torch.nn as nn\n","import torch\n","from transformers import AutoModel\n","import torch.nn.functional as F\n","\n","def init_weights(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv2d') != -1 or classname.find('ConvTranspose2d') != -1:\n","        nn.init.kaiming_uniform_(m.weight)\n","        nn.init.zeros_(m.bias)\n","    elif classname.find('BatchNorm') != -1:\n","        nn.init.normal_(m.weight, 1.0, 0.02)\n","        nn.init.zeros_(m.bias)\n","    elif classname.find('Linear') != -1:\n","        nn.init.xavier_normal_(m.weight)\n","        if m.bias is not None:\n","            nn.init.zeros_(m.bias)\n","\n","\n","class AttentionWithContext(nn.Module):\n","    def __init__(self, hidden_dim):\n","        super(AttentionWithContext, self).__init__()\n","\n","        self.attn = nn.Linear(hidden_dim, hidden_dim)\n","        self.contx = nn.Linear(hidden_dim, 1, bias=False)\n","        #self.apply(init_weights)\n","    def forward(self, inp):\n","        u = torch.tanh_(self.attn(inp))\n","        a = F.softmax(self.contx(u))\n","        s = (a * inp).sum(1)\n","        return s\n","\n","\n","class TransformerLayer(nn.Module):\n","    def __init__(self,\n","                 pretrained_path='aubmindlab/bert-base-arabert'):\n","        super(TransformerLayer, self).__init__()\n","\n","        \n","        self.transformer = AutoModel.from_pretrained(pretrained_path, output_hidden_states=True)\n","\n","\n","    def forward(self, input_ids=None, attention_mask=None):\n","        outputs = self.transformer(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask\n","            , output_hidden_states=True\n","        )\n","        #(output_last_layer, pooled_cls, (output_layers))\n","        #output[0] (8, seqlen=64, 768) cls [8, 768] ( 12 (8, seqlen=64, 768))\n","\n","        return outputs\n","\n","    def output_num(self):\n","        return self.transformer.config.hidden_size\n","\n","class ATTClassifier(nn.Module):\n","    def __init__(self, in_feature, class_num=1, dropout_prob=0.2):\n","        super(ATTClassifier, self).__init__()\n","        self.attention = AttentionWithContext(in_feature)\n","\n","        self.Classifier = nn.Sequential(\n","            nn.Linear(2 * in_feature, 512),\n","            nn.Dropout(dropout_prob),\n","            nn.ReLU(),\n","            nn.Linear(512, class_num)\n","        )\n","\n","        self.apply(init_weights)\n","\n","    def forward(self, x):\n","        att = self.attention(x[0]) #(X[0] (bs, seqlenght, embedD) att = \\sum_i alpha_i x[0][i]\n","\n","        xx = torch.cat([att, x[1]], 1)\n","\n","        out = self.Classifier(xx)\n","        return out\n","class Attention(nn.Module):\n","    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n","        super(Attention, self).__init__(**kwargs)\n","        \n","        self.supports_masking = True\n","\n","        self.bias = bias\n","        self.feature_dim = feature_dim\n","        self.step_dim = step_dim\n","        self.features_dim = 0\n","        \n","        weight = torch.zeros(feature_dim, 1)\n","        nn.init.kaiming_uniform_(weight)\n","        self.weight = nn.Parameter(weight)\n","        \n","        if bias:\n","            self.b = nn.Parameter(torch.zeros(step_dim))\n","        \n","    def forward(self, x, mask=None):\n","        feature_dim = self.feature_dim \n","        step_dim = self.step_dim\n","\n","        eij = torch.mm(\n","            x.contiguous().view(-1, feature_dim), \n","            self.weight\n","        ).view(-1, step_dim)\n","        \n","        if self.bias:\n","            eij = eij + self.b\n","            \n","        eij = torch.tanh(eij)\n","        a = torch.exp(eij)\n","        \n","        if mask is not None:\n","            a = a * mask\n","\n","        a = a / (torch.sum(a, 1, keepdim=True) + 1e-10)\n","\n","        weighted_input = x * torch.unsqueeze(a, -1)\n","        return torch.sum(weighted_input, 1)\n","\n","class WeightedLayerPooling(nn.Module):\n","    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights=None):\n","        super(WeightedLayerPooling, self).__init__()\n","        self.layer_start = layer_start\n","        self.num_hidden_layers = num_hidden_layers\n","        self.layer_weights = layer_weights if layer_weights is not None \\\n","            else nn.Parameter(\n","            torch.tensor([1] * (num_hidden_layers + 1 - layer_start), dtype=torch.float)\n","        )\n","\n","    def forward(self, all_hidden_states):\n","        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n","        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n","        weighted_average = (weight_factor * all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n","        return weighted_average\n","class NADIModel(nn.Module):\n","  def __init__(self, pretrained_path='aubmindlab/bert-base-arabert',in_feature=768, class_num=18, dropout_prob=0.2):\n","        super(NADIModel, self).__init__()\n","        self.base_model=TransformerLayer(pretrained_path).to('cuda')\n","        self.config = AutoConfig.from_pretrained(pretrained_path)\n","        self.weighted_pooler = WeightedLayerPooling(num_hidden_layers=self.config.num_hidden_layers, layer_start=4)\n","        self.att=Attention(768, 512).to('cuda')\n","        self.dropout1 = nn.Dropout(0.1)\n","        self.dropout2 = nn.Dropout(0.2)\n","        self.dropout3 = nn.Dropout(0.3)\n","        self.dropout4 = nn.Dropout(0.4)\n","        self.dropout5 = nn.Dropout(0.5)\n","        self.classifier=nn.Linear(768, class_num)\n","  def forward(self, ids,mask):\n","    output=self.base_model(ids,mask)\n","    all_hidden_states = torch.stack(output.hidden_states)\n","    weighted_pooling_embeddings = (self.weighted_pooler(all_hidden_states)) # For WeightedLayerPooling\n","    # weighted_pooling_embeddings = weighted_pooling_embeddings[:, 0]\n","    # print(weighted_pooling_embeddings.shape)\n","    output=self.att(weighted_pooling_embeddings)\n","    logits1 = self.classifier(self.dropout1(output))\n","    logits2 = self.classifier(self.dropout2(output))\n","    logits3 = self.classifier(self.dropout3(output))\n","    logits4 = self.classifier(self.dropout4(output))\n","    logits5 = self.classifier(self.dropout5(output))\n","\n","    logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\n","    # output=self.classifier(output)\n","    # output=torch.softmax(output, dim=1)\n","    return logits\n"," "],"metadata":{"id":"FW53GnvRuJ6J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train, Validation"],"metadata":{"id":"SYQWLGzVyqt_"}},{"cell_type":"code","source":["class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n","    \"\"\"\n","    Samples elements randomly from a given list of indices for imbalanced dataset\n","    Arguments:\n","        indices (list, optional): a list of indices\n","        num_samples (int, optional): number of samples to draw\n","    \"\"\"\n","\n","    def __init__(self, dataset, indices=None, num_samples=None):\n","        # if indices is not provided,\n","        # all elements in the dataset will be considered\n","        self.indices = list(range(len(dataset['#3_label']))) \\\n","            if indices is None else indices\n","\n","        # if num_samples is not provided,\n","        # draw `len(indices)` samples in each iteration\n","        self.num_samples = len(self.indices) \\\n","            if num_samples is None else num_samples\n","\n","        # distribution of classes in the dataset\n","        label_to_count = {}\n","        for idx in self.indices:\n","            label = self._get_label(dataset, idx)\n","            if label in label_to_count:\n","                label_to_count[label] += 1\n","            else:\n","                label_to_count[label] = 1\n","\n","        # weight for each sample\n","        weights = [1.0 / label_to_count[self._get_label(dataset, idx)] for idx in self.indices]\n","        self.weights = torch.DoubleTensor(weights)\n","\n","    def _get_label(self, dataset, id_):\n","        return dataset['#3_label'][id_]\n","\n","    def __iter__(self):\n","        return (self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True))\n","\n","    def __len__(self):\n","        return self.num_samples"],"metadata":{"id":"pjYrf5IerGh7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def criterion(outputs1,  targets):\n","\n","    criterion = F1_Loss()\n","    criterion_2 = FocalLoss()\n","    loss = 0.5* criterion_2(outputs1, targets) + 0.5*criterion(outputs1, targets)\n","    return loss"],"metadata":{"id":"ZWP3677hqMr1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n","    model.train()\n","    \n","    dataset_size = 0\n","    running_loss = 0.0\n","    \n","    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n","    for step, data in bar:\n","        \n","        text_ids = data['text_ids'].to(device, dtype = torch.long)\n","        text_mask = data['text_mask'].to(device, dtype = torch.long)\n","        targets = data['target'].to(device, dtype=torch.long)\n","        \n","        batch_size = text_ids.size(0)\n","        # print(targets)\n","\n","        outputs = model(text_ids, text_mask)\n","        # print(outputs.shape)\n","        \n","        # print(outputs.shape)\n","\n","        \n","        loss = criterion(outputs, targets)\n","        loss = loss / CONFIG['n_accumulate']\n","        loss.backward()\n","    \n","        if (step + 1) % CONFIG['n_accumulate'] == 0:\n","            optimizer.step()\n","\n","            # zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            if scheduler is not None:\n","                scheduler.step()\n","                \n","        running_loss += (loss.item() * batch_size)\n","        dataset_size += batch_size\n","        \n","        epoch_loss = running_loss / dataset_size\n","        \n","        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n","                        LR=optimizer.param_groups[0]['lr'])\n","    gc.collect()\n","    \n","    return epoch_loss"],"metadata":{"id":"ltIB44KZyxPt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@torch.no_grad()\n","def valid_one_epoch(model, dataloader, device, epoch):\n","    model.eval()\n","    \n","    dataset_size = 0\n","    running_loss = 0.0\n","    \n","    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n","    for step, data in bar:        \n","        \n","        text_ids = data['text_ids'].to(device, dtype = torch.long)\n","        text_mask = data['text_mask'].to(device, dtype = torch.long)\n","        targets = data['target'].to(device, dtype=torch.long)\n","        \n","        batch_size = text_ids.size(0)\n","\n","        outputs = model(text_ids, text_mask)\n","        # outputs = outputs.argmax(dim=1)\n","        \n","        loss = criterion(outputs, targets)\n","        \n","        running_loss += (loss.item() * batch_size)\n","        dataset_size += batch_size\n","        \n","        epoch_loss = running_loss / dataset_size\n","        \n","        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n","                        LR=optimizer.param_groups[0]['lr'])   \n","    \n","    gc.collect()\n","    \n","    return epoch_loss"],"metadata":{"id":"U5Ro64pIy4HN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def run_training(model, optimizer, scheduler, device, num_epochs, fold,train_loader, valid_loader):\n","    # To automatically log gradients\n","    \n","    if torch.cuda.is_available():\n","        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n","    \n","    start = time.time()\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_epoch_loss = np.inf\n","    history = defaultdict(list)\n","    \n","    for epoch in range(1, num_epochs + 1): \n","        gc.collect()\n","        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, \n","                                           dataloader=train_loader, \n","                                           device=CONFIG['device'], epoch=epoch)\n","        \n","        val_epoch_loss = valid_one_epoch(model, valid_loader, device=CONFIG['device'], \n","                                         epoch=epoch)\n","    \n","        history['Train Loss'].append(train_epoch_loss)\n","        history['Valid Loss'].append(val_epoch_loss)\n","        \n","       \n","        \n","        # deep copy the model\n","        if val_epoch_loss <= best_epoch_loss:\n","            print(f\"Validation Loss Improved ({best_epoch_loss} ---> {val_epoch_loss})\")\n","            best_epoch_loss = val_epoch_loss\n","            best_model_wts = copy.deepcopy(model.state_dict())\n","            PATH = f\"/content/drive/MyDrive/wanlp/NADI/Saved_Models/qrib/Loss-Fold-{fold}.bin\"\n","            torch.save(model.state_dict(), PATH)\n","            # Save a model file from the current directory\n","            print(\"Model Saved\")\n","            \n","        print()\n","    \n","    end = time.time()\n","    time_elapsed = end - start\n","    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n","    print(\"Best Loss: {:.4f}\".format(best_epoch_loss))\n","    \n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    \n","    return model, history"],"metadata":{"id":"HpH2EB0snbV0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def fetch_scheduler(optimizer):\n","    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n","        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CONFIG['T_max'], \n","                                                   eta_min=CONFIG['min_lr'])\n","    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n","        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=CONFIG['T_0'], \n","                                                             eta_min=CONFIG['min_lr'])\n","    elif CONFIG['scheduler'] == None:\n","        return None\n","        \n","    return scheduler"],"metadata":{"id":"JxT6JHg0nogC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def prepare_loaders(train,fold):\n","    df_train = train[train.kfold != fold].reset_index(drop=True)\n","    df_valid = train[train.kfold == fold].reset_index(drop=True)\n","    # print(len(df_valid))\n","    sampler = ImbalancedDatasetSampler(df_train)\n","    \n","    train_dataset = TrainDataset(df_train, tokenizer=tokenizer, max_length=CONFIG['max_length'])\n","    valid_dataset = TrainDataset(df_valid, tokenizer=tokenizer, max_length=CONFIG['max_length'])\n","\n","    train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], \n","                              num_workers=2, sampler=sampler, pin_memory=True, drop_last=True)\n","    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], \n","                              num_workers=2, shuffle=False, pin_memory=True)\n","    \n","    return train_loader, valid_loader"],"metadata":{"id":"x-x9oUAsnqqy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train"],"metadata":{"id":"T-MsRHzBnZX8"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ih8htg1LoRZC","executionInfo":{"status":"ok","timestamp":1661783630248,"user_tz":-120,"elapsed":27335,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}},"outputId":"46d7104e-1a34-4fd1-b459-7a0cbc31442c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# ,jmnbv"],"metadata":{"id":"yeq4g6QBasWl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train = pd.read_csv('/content/drive/MyDrive/wanlp/Datasets/NADI2022-Train/NADI2022-Train/Subtask1/NADI2022_Subtask1_TRAIN.tsv', sep='\\t', lineterminator='\\n')\n","valid = pd.read_csv('/content/drive/MyDrive/wanlp/Datasets/NADI2022-Train/NADI2022-Train/Subtask1/NADI2022_Subtask1_DEV.tsv', sep='\\t', lineterminator='\\n')"],"metadata":{"id":"5dHag8YTn7jE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"ZLZqgBrJoyaa","executionInfo":{"status":"ok","timestamp":1661783631924,"user_tz":-120,"elapsed":47,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}},"outputId":"d730fc89-b25a-4b7f-b9fd-2c91ebd8c4e6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["         #1_id                                         #2_content #3_label\n","0  TRAIN_15711                         الطرحة في 61 النفر 10 جنيه    yemen\n","1   TRAIN_2257  كله ولا يطالبوا بارض فدك الله ياخدهم الله يعجل...  lebanon\n","2   TRAIN_5618  ' لما اطلقتو تلك التغريدة وقتها كان في واحد سع...  algeria\n","3   TRAIN_7284                                      بجكولى بى خير     iraq\n","4  TRAIN_15841                              يعم القمر براحه علينا    egypt"],"text/html":["\n","  <div id=\"df-c92e9213-e886-4ddd-af34-de3c46689f91\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>#1_id</th>\n","      <th>#2_content</th>\n","      <th>#3_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>TRAIN_15711</td>\n","      <td>الطرحة في 61 النفر 10 جنيه</td>\n","      <td>yemen</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>TRAIN_2257</td>\n","      <td>كله ولا يطالبوا بارض فدك الله ياخدهم الله يعجل...</td>\n","      <td>lebanon</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>TRAIN_5618</td>\n","      <td>' لما اطلقتو تلك التغريدة وقتها كان في واحد سع...</td>\n","      <td>algeria</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>TRAIN_7284</td>\n","      <td>بجكولى بى خير</td>\n","      <td>iraq</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>TRAIN_15841</td>\n","      <td>يعم القمر براحه علينا</td>\n","      <td>egypt</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c92e9213-e886-4ddd-af34-de3c46689f91')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c92e9213-e886-4ddd-af34-de3c46689f91 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c92e9213-e886-4ddd-af34-de3c46689f91');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["len(train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gB9D4twgo0bI","executionInfo":{"status":"ok","timestamp":1661783631924,"user_tz":-120,"elapsed":43,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}},"outputId":"009fe781-b6a3-4311-f6e9-1c70cdbceded"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["20398"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["train['#3_label'].nunique()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qYdSpBt2o2HR","executionInfo":{"status":"ok","timestamp":1661783631927,"user_tz":-120,"elapsed":37,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}},"outputId":"b7572edf-2f92-4b21-a1a0-55da414ce83d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["18"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["valid['#3_label'].nunique()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LREmIaISpGJF","executionInfo":{"status":"ok","timestamp":1661783631928,"user_tz":-120,"elapsed":35,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}},"outputId":"1b0413d6-baff-409c-c97f-468020d0b892"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["18"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["# df_train=pd.concat([train,valid])\n","df_train=train.copy()"],"metadata":{"id":"alyqp1JApKHQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_classes=df_train['#3_label'].nunique()"],"metadata":{"id":"F8zmQRzrpDra"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CONFIG = {\"seed\": 42,\n","          \"epochs\": 5,\n","          \"train_batch_size\": 8,\n","          \"valid_batch_size\": 64,\n","          \"max_length\": 512,\n","          \"learning_rate\": 2e-5,\n","          \"scheduler\": 'CosineAnnealingLR',\n","          \"min_lr\": 1e-8,\n","          \"T_max\": 500,\n","          \"weight_decay\": 1e-8,\n","          \"n_fold\": 5,\n","          \"n_accumulate\": 1,\n","          \"num_classes\": 18,\n","          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n","          }"],"metadata":{"id":"0_HBDMdYn1Mi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer= AutoTokenizer.from_pretrained('qarib/bert-base-qarib_far_9920k')\n"],"metadata":{"id":"89VbI4nqq17L","executionInfo":{"status":"ok","timestamp":1661783633491,"user_tz":-120,"elapsed":1239,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}},"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["35ad34494d6540f8b7c9cdc0e1bb903d","bcb94a7944e748f18a3546050d2de21b","55a00de76bfb4754843be53d35c25727","84840b23b9f44c558d687d30331df6a3","4eb6230e3bce407ca06d3e19f255cbc4","1d40cb0850e2489faae71105383922ea","70bbac81a3b542dea796b309d1985ad2","281db875d43e45a3a3862d0986aa5901","6c713156bb174d8db7a97b21315fcba6","8724b46a366049ada87baa41d040357e","9df7936a69724eaa950892f28260b6c3","5956b00e22dd469abfc71ecf1043a327","18bf49aa703d4fc2b8fc85eede3ca03a","d1f805ba4ad0452daa62946f720c5d59","bf84597006814c0f8192455bd5b67aa3","88b12fa19f544f9790e9f75ea546192b","06af14fdac394d52985bc6b8821e340b","bc0d305e1709496cb99e89b32f6747b6","daf4a02c21bc4663a2cbd37515d4ff6a","efa294cd26c549459b61159fc5e44d2f","95b3c362f28c4d7388b58cb37326d12c","d36ae5415ad0412a879d7e2c6a8d7cd8"]},"outputId":"a918aa7a-d331-482f-a2d1-092d6e293ee0"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading config.json:   0%|          | 0.00/504 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35ad34494d6540f8b7c9cdc0e1bb903d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading vocab.txt:   0%|          | 0.00/641k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5956b00e22dd469abfc71ecf1043a327"}},"metadata":{}}]},{"cell_type":"code","source":["df_train.reset_index(inplace=True)"],"metadata":{"id":"E9sfjlh6qj6t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["skf = StratifiedKFold(n_splits=CONFIG['n_fold'], shuffle=True, random_state=CONFIG['seed'])\n","\n","for fold, ( _, val_) in enumerate(skf.split(X=df_train, y=df_train['#3_label'])):\n","    df_train.loc[val_ , \"kfold\"] = int(fold)\n","    \n","df_train[\"kfold\"] = df_train[\"kfold\"].astype(int)"],"metadata":{"id":"Dq0tSkUrntN4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train['kfold'].values"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0_0l7FW06mgP","executionInfo":{"status":"ok","timestamp":1661783633932,"user_tz":-120,"elapsed":28,"user":{"displayName":"Reem Elsayed","userId":"13751898496823085405"}},"outputId":"62bbbf9b-4c67-4dbc-93f7-834c0968feb0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([4, 2, 3, ..., 0, 1, 2])"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder\n","\n","le = LabelEncoder()\n","df_train['#3_label'] = le.fit_transform(df_train['#3_label'].values)"],"metadata":{"id":"wOMg6NEVsEkU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for fold in range(3, CONFIG['n_fold']):\n","    print(f\"====== Fold: {fold} ======\")\n","\n","    \n","    # Create Dataloaders\n","    train_loader, valid_loader = prepare_loaders(df_train,fold=fold)\n","    \n","    model = NADIModel('qarib/bert-base-qarib_far_9920k')\n","    model.to(CONFIG['device'])\n","    torch.cuda.empty_cache()\n","    \n","    # Define Optimizer and Scheduler\n","    optimizer = AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n","    scheduler = fetch_scheduler(optimizer)\n","    \n","    model, history = run_training(model, optimizer, scheduler,\n","                                  device=CONFIG['device'],\n","                                  num_epochs=CONFIG['epochs'],\n","                                  fold=fold,train_loader=train_loader, valid_loader=valid_loader )\n","    \n","    \n","    del model, history, train_loader, valid_loader\n","    _ = gc.collect()\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":762,"referenced_widgets":["250c4ae2c550489582eee82871b81e00","ae97d4edfecd4dfeb0842a5b88266d41","d64adafe978349a599694f471f7547a5","81bdbe4a2a0740c78a2c763dbcbaa72c","a6de7ec40d8c4705b6c960e25312e7be","808931be6fc14a718dbe81488ccc7497","7de703fa5e3746c88a36f72170bbc737","f2c9dcc570a148e18bef677947d5fee1","3153098254c248d18a09a6118b51c326","c5fdb205eeac4a35a490eb46dd7a308e","6f3462e43c91423890b69c50d5e98d0b"]},"id":"RdoS6yH5nwWG","outputId":"59cc552e-b813-42b7-9f73-6ee09801c9c8"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["====== Fold: 2 ======\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"250c4ae2c550489582eee82871b81e00","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/518M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at qarib/bert-base-qarib_far_9920k were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["[INFO] Using GPU: Tesla T4\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 2039/2039 [26:46<00:00,  1.27it/s, Epoch=1, LR=1.97e-5, Train_Loss=1.64]\n","100%|██████████| 64/64 [02:07<00:00,  1.99s/it, Epoch=1, LR=1.97e-5, Valid_Loss=1.58]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Validation Loss Improved (inf ---> 1.582331174027686)\n","Model Saved\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 2039/2039 [26:48<00:00,  1.27it/s, Epoch=2, LR=1.88e-5, Train_Loss=0.949]\n","100%|██████████| 64/64 [02:07<00:00,  1.99s/it, Epoch=2, LR=1.88e-5, Valid_Loss=1.62]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 2039/2039 [26:46<00:00,  1.27it/s, Epoch=3, LR=1.74e-5, Train_Loss=0.606]\n","100%|██████████| 64/64 [02:07<00:00,  1.99s/it, Epoch=3, LR=1.74e-5, Valid_Loss=1.61]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 2039/2039 [26:46<00:00,  1.27it/s, Epoch=4, LR=1.56e-5, Train_Loss=0.431]\n","100%|██████████| 64/64 [02:07<00:00,  1.99s/it, Epoch=4, LR=1.56e-5, Valid_Loss=1.72]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 2039/2039 [26:47<00:00,  1.27it/s, Epoch=5, LR=1.34e-5, Train_Loss=0.304]\n","100%|██████████| 64/64 [02:07<00:00,  1.99s/it, Epoch=5, LR=1.34e-5, Valid_Loss=1.75]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","Training complete in 2h 24m 38s\n","Best Loss: 1.5823\n","\n","====== Fold: 3 ======\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at qarib/bert-base-qarib_far_9920k were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["[INFO] Using GPU: Tesla T4\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2039/2039 [26:51<00:00,  1.27it/s, Epoch=1, LR=1.97e-5, Train_Loss=1.65]\n","100%|██████████| 64/64 [02:07<00:00,  1.99s/it, Epoch=1, LR=1.97e-5, Valid_Loss=1.63]\n"]},{"output_type":"stream","name":"stdout","text":["Validation Loss Improved (inf ---> 1.63316723370441)\n","Model Saved\n","\n"]},{"output_type":"stream","name":"stderr","text":[" 74%|███████▍  | 1518/2039 [20:00<06:52,  1.26it/s, Epoch=2, LR=6.66e-7, Train_Loss=1.01]"]}]}]}